<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jzq6520.github.io</id>
    <title>chuck</title>
    <updated>2019-11-12T08:48:48.948Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jzq6520.github.io"/>
    <link rel="self" href="https://jzq6520.github.io/atom.xml"/>
    <subtitle>天地不仁以万物为刍狗</subtitle>
    <logo>https://jzq6520.github.io/images/avatar.png</logo>
    <icon>https://jzq6520.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, chuck</rights>
    <entry>
        <title type="html"><![CDATA[pytorch--自定义loss]]></title>
        <id>https://jzq6520.github.io/post/pytorch-zi-ding-yi-loss</id>
        <link href="https://jzq6520.github.io/post/pytorch-zi-ding-yi-loss">
        </link>
        <updated>2019-11-12T06:04:25.000Z</updated>
        <content type="html"><![CDATA[<ol>
<li>负log loss；</li>
<li>binary crossentropy；</li>
<li>focal loss；</li>
</ol>
<p>网上找到的loss写的都普遍复杂，我自己稍微写的逻辑简单一点。</p>
<pre><code class="language-python">if inputs.is_cuda and not self.alpha.is_cuda:
            self.alpha = self.alpha.cuda()
</code></pre>
<h2 id="focal-loss">focal loss</h2>
<p>focal loss仔细实践起来可以分为两种情况，一种是二分类（<strong>sigmoid激活</strong>）的时候，还有一种情况就是多分类（<strong>softmax激活</strong>）的时候。</p>
<h3 id="二分类focal-loss">二分类focal loss</h3>
<figure data-type="image" tabindex="1"><img src="https://jzq6520.github.io/post-images/1573545890626.png" alt=""></figure>
<pre><code class="language-python">class FocalLoss(nn.Module):
    &quot;&quot;&quot; -[alpha*y*(1-p)^gamma*log(p)+(1-alpha)(1-y)*p^gamma*log(1-p)] loss&quot;&quot;&quot;

    def __init__(self, gamma, alpha=None , onehot=False):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.onehot = onehot


    def forward(self, inputs, targets):
        &quot;&quot;&quot;

        :param input: onehot
        :param target: 默认是onehot以后
        :return:
        &quot;&quot;&quot;
        N = inputs.size(0)
        C = inputs.size(1)
        inputs = torch.clamp(inputs, min=0.001, max=1.0)  ##将一个张量中的数值限制在一个范围内，如限制在[0.1,1.0]范围内，可以避免一些运算错误，如预测结果q中元素可能为0
        if inputs.is_cuda and not self.alpha.is_cuda:
            self.alpha = self.alpha.cuda()    

         if not self.onehot:
            class_mask = inputs.data.new(N, C).fill_(0)
            class_mask = Variable(class_mask)
            ids = targets.view(-1, 1)
            class_mask.scatter_(1, ids.data, 1.)
            targets = class_mask

        pos_sample_loss_matrix = -targets * (torch.pow((1 - inputs), self.gamma)) * inputs.log()  ## 正样本的loss
        # mean_pos_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()

        neg_sample_loss_matrix = -(targets == 0).float() * (torch.pow((inputs), self.gamma)) * (1 - inputs).log()  ## 负样本的loss
        # mean_neg_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()
        if self.alpha:
            return (self.alpha * pos_sample_loss_matrix + (1 - self.alpha) * neg_sample_loss_matrix).sum() / (N * C)
        else:
            return (pos_sample_loss_matrix + neg_sample_loss_matrix).sum() / (N * C)


</code></pre>
<h2 id="多分类focal-loss">多分类focal loss</h2>
<figure data-type="image" tabindex="2"><img src="https://jzq6520.github.io/post-images/1573546010700.png" alt=""></figure>
<pre><code class="language-python">class FocalLoss(nn.Module):
    &quot;&quot;&quot; -[y*(1-p)^gamma*log(p) loss
        softmax激活输入的foacl loss。
    &quot;&quot;&quot;

    def __init__(self, gamma, onehot=False):
        super(FocalLoss, self).__init__()

        self.gamma = gamma
        self.onehot = onehot


    def forward(self, inputs, targets):
        &quot;&quot;&quot;

        :param input: onehot
        :param target: 默认是onehot以后
        :return:
        &quot;&quot;&quot;

        inputs = torch.clamp(inputs, min=0.001, max=1.0)  ##将一个张量中的数值限制在一个范围内，如限制在[0.1,1.0]范围内，可以避免一些运算错误，如预测结果q中元素可能为0

         if not self.onehot:
            N = inputs.size(0)
            C = inputs.size(1)
            class_mask = inputs.data.new(N, C).fill_(0)
            class_mask = Variable(class_mask)
            ids = targets.view(-1, 1)
            class_mask.scatter_(1, ids.data, 1.)
            targets = class_mask

        pos_sample_loss_matrix = -targets * (torch.pow((1 - inputs), self.gamma)) * inputs.log()  ## 正样本的loss
        # mean_pos_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()

        ## 默认输出均值
        ## 这里不能直接求mean，
        # 因为整个矩阵还是原来的输入大小的，
        # 求loss应该是除以label中有目标的总数。
        
        return pos_sample_loss_matrix / targets.sum()
        


</code></pre>
<h2 id="代码">代码</h2>
<ul>
<li>NegtiveLogLoss</li>
<li>BinaryCrossEntropy</li>
</ul>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch.autograd import Variable

class NegtiveLogLoss(nn.Module):
   &quot;&quot;&quot; -log(p) loss&quot;&quot;&quot;

   def __init__(self, onehot=False):
       super(NegtiveLogLoss, self).__init__()
       self.onehot = onehot

   def forward(self, inputs, targets):
       &quot;&quot;&quot;

       :param input: onehot
       :param target: 默认是onehot以后
       :return:
       &quot;&quot;&quot;

       inputs = torch.clamp(inputs, min=0.001, max=1.0)  ## 将一个张量中的数值限制在一个范围内，如限制在[0.1,1.0]范围内，可以避免一些运算错误，如预测结果q中元素可能为0

       if not self.onehot:
           N = inputs.size(0)
           C = inputs.size(1)
           class_mask = inputs.data.new(N, C).fill_(0)
           class_mask = Variable(class_mask)
           ids = targets.view(-1, 1)
           class_mask.scatter_(1, ids.data, 1.)
           targets = class_mask

       loss_matrix = -targets * inputs.log()  ## 对预测的矩阵里面的每个元素做log，
       ## 然后乘以one hot的label，也就是说获得1位置的值了。
       ## 这时候还是个矩阵，还没有计算均值
       ## 默认输出均值
       return loss_matrix.sum() / targets.sum()  ## 这里不能直接求mean，
       # 因为整个矩阵还是原来的输入大小的，
       # 求loss应该是除以label中有目标的总数。



class BinaryCrossEntropy(nn.Module):
   &quot;&quot;&quot; -(ylog(p)+(1-y)log(1-p) loss&quot;&quot;&quot;

   def __init__(self, alpha=None, onehot=False):
       super(BinaryCrossEntropy, self).__init__()
       self.alpha = alpha
       self.onehot = onehot

   def forward(self, inputs, targets):
       &quot;&quot;&quot;

       :param input: onehot
       :param target: 默认是onehot以后
       :return:
       &quot;&quot;&quot;
       N = inputs.size(0)
       C = inputs.size(1)
       inputs = torch.clamp(inputs, min=0.001, max=1.0)  ##将一个张量中的数值限制在一个范围内，如限制在[0.1,1.0]范围内，可以避免一些运算错误，如预测结果q中元素可能为0
       
       if not self.onehot:

           class_mask = inputs.data.new(N, C).fill_(0)
           class_mask = Variable(class_mask)
           ids = targets.view(-1, 1)
           class_mask.scatter_(1, ids.data, 1.)
           targets = class_mask

       pos_sample_loss_matrix = -targets * inputs.log()  ## 正样本的loss
       # mean_pos_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()
      
       neg_sample_loss_matrix = -(targets == 0).float() * (1 - inputs).log()  ## 负样本的loss
       # mean_neg_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()

       if self.alpha:
           return (self.alpha*pos_sample_loss_matrix + (1-self.alpha)*neg_sample_loss_matrix).sum() / (N * C)
       else:
           return (pos_sample_loss_matrix + neg_sample_loss_matrix).sum() / (N * C)  



</code></pre>
<h2 id="引用">引用</h2>
<ul>
<li>http://kodgv.xyz/2019/04/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/FocalLoss%E9%92%88%E5%AF%B9%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE/</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深度学习--新的激活函数Mish]]></title>
        <id>https://jzq6520.github.io/post/shen-du-xue-xi-xin-de-ji-huo-han-shu-mish</id>
        <link href="https://jzq6520.github.io/post/shen-du-xue-xi-xin-de-ji-huo-han-shu-mish">
        </link>
        <updated>2019-11-11T09:42:23.000Z</updated>
        <content type="html"><![CDATA[<pre><code>import torch
import torch.nn.functional as F
from torch import nn

class Mish(nn.Module):
    '''
    Applies the mish function element-wise:
    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))
    Shape:
        - Input: (N, *) where * means, any number of additional
          dimensions
        - Output: (N, *), same shape as the input
    Examples:
        &gt;&gt;&gt; m = Mish()
        &gt;&gt;&gt; input = torch.randn(2)
        &gt;&gt;&gt; output = m(input)
    '''
    def __init__(self):
        '''
        Init method.
        '''
        super(Mish, self).__init__()

    def forward(self, input):
        '''
        Forward pass of the function.
        '''
        return input * torch.tanh(F.softplus(input))

model = models.resnet50(pretrained=True, progress=True)
print(&quot;acitve&quot;, model.relu)
model.relu = Mish()  ## 在pytorch的resnet50里面这样替换一下就可以了
print(&quot;acitve&quot;, model.relu)

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[pytorch--Deep Learning with PyTorch 简单记录]]></title>
        <id>https://jzq6520.github.io/post/pytorch-deep-learning-with-pytorch-jian-dan-ji-lu</id>
        <link href="https://jzq6520.github.io/post/pytorch-deep-learning-with-pytorch-jian-dan-ji-lu">
        </link>
        <updated>2019-11-11T06:18:36.000Z</updated>
        <content type="html"><![CDATA[<h2 id="底层细节">底层细节</h2>
<ul>
<li>A tensor containing only one element is called a scalar.</li>
<li>一维tensor：</li>
</ul>
<pre><code>temp = torch.FloatTensor([23,24,24.5,26,27.2,23.0])
</code></pre>
<ul>
<li>tensor 数据类型：<code>torch.FloatTensor</code>等</li>
<li>数据类型转换, Tensor后加long(), int(), double(),float(),byte()等函数就能将Tensor进行类型转换；</li>
<li>Torch provides a utility function called from_numpy(), which converts a numpy array into a torch tensor.</li>
</ul>
<pre><code>boston_tensor = torch.from_numpy(boston.data)
</code></pre>
<ul>
<li>tensor to numpy,and slice</li>
</ul>
<pre><code>plt.imshow(panda_tensor[:,:,0].numpy())
</code></pre>
<ul>
<li>4d tensor,先用numpy reshape</li>
</ul>
<pre><code>cat_imgs = np.array([np.array(Image.open(cat).resize((224,224))) for cat in cats[:64]]) 
cat_imgs = cat_imgs.reshape(-1,224,224,3) 
cat_tensors = torch.from_numpy(cat_imgs)
</code></pre>
<ul>
<li>算术运算，addition, subtraction, multiplication, dot product, and matrix multiplication.All of these operations can be either performed on the** CPU or the GPU**.</li>
</ul>
<pre><code>加法：d = torch.add(a,b) ， c = a+b
数值乘法：a.mul(b) ， a*b
矩阵相乘：a.matmul(b)
</code></pre>
<ul>
<li>把tensor从cpu拷贝到gpu，PyTorch provides a simple function called cuda() to copy a tensor on the CPU to the GPU.</li>
</ul>
<pre><code>a = torch.rand(10000,10000) b = torch.rand(10000,10000)
a.matmul(b)
Time taken: 3.23 s

#Move the tensors to GPU a = a.cuda() b = b.cuda()
a.matmul(b)
Time taken: 11.2 µs
</code></pre>
<ul>
<li>require_grad参数表示是否是可学习的，也就是是否梯度下降。</li>
<li>在loss上调用<code>backward</code>函数计算梯度。calculate the gradients by calling the <code>backward</code> function on the final loss variable。</li>
<li></li>
</ul>
<pre><code class="language-python">  def loss_fn(y,y_pred): 
    loss = (y_pred-y).pow(2).sum() 
    for param in [w,b]: ##这里是训练w和b，
        if not param.grad is None: ## 如果参数的梯度是None，即不计算梯度的，那么就梯度设置为0.
            param.grad.data.zero_()  ## remove any previously calculated gradients by calling the grad.data.zero_() operation.
    loss.backward()  ## 调用了这句以后就会自动计算梯度，梯度计算出来就存在param.grad.data里面。
    return loss.data[0]

def optimize(learning_rate): 
    w.data -= learning_rate * w.grad.data 
    b.data -= learning_rate * b.grad.data
</code></pre>
<ul>
<li>Dataset是一个数据结构,只需要继承并实例化三个方法</li>
</ul>
<pre><code>from torch.utils.data import Dataset 
class DogsAndCatsDataset(Dataset): 
    def __init__(self,):
        pass 
    def __len__(self):
        pass 
    def __getitem__(self,idx): 
        pass
</code></pre>
<ul>
<li>DataLoader其实是一个generator，但是可以进行<strong>多线程读取</strong>，可以指定batchsize。</li>
</ul>
<pre><code>dataloader = DataLoader(dogsdset,batch_size=32,num_workers=2) 
for imgs , labels in dataloader: 
	#Apply your DL on the dataset. 
	pass
</code></pre>
<h2 id="layer">layer</h2>
<p>higher-level functionalities are called <strong>layers</strong> across the deep learning frameworks。</p>
<p><img src="https://jzq6520.github.io/post-images/1573460839202.png" alt=""><br>
Summarizing the previous diagram, any deep learning training involves getting data, building an architecture that in general is getting a bunch of layers together, evaluating the accuracy of the model using a loss function, and then optimizing the algorithm by optimizing the weights of our network.</p>
<ul>
<li>全连接层<pre><code class="language-python">myLayer = Linear(in_features=10,out_features=5,bias=True)
</code></pre>
<ul>
<li>使用：<code>myLayer(input)</code></li>
<li>访问参数：
<ul>
<li>weight: <code>myLayer.weight</code></li>
<li>bias: <code>myLayer.bias</code></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cython(2)--编译和运行]]></title>
        <id>https://jzq6520.github.io/post/cython2-bian-yi-he-yun-xing</id>
        <link href="https://jzq6520.github.io/post/cython2-bian-yi-he-yun-xing">
        </link>
        <updated>2019-11-06T03:34:57.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>和c、c++一样，Cython代码运行前也是需要编译的；</li>
<li>Cython是python的<strong>超集</strong>，所以python编译器是不能直接import和运行Cython的；</li>
<li>这个编译步骤可以是<strong>显式</strong>的，也可以是<strong>隐式</strong>的，即可以<strong>自动运行</strong>，不需要用户参与(让Cython感觉很像Python)，也可以由终端用户在需要更多控制时临时运行，两种都有它的作用；</li>
<li><strong>自动编译</strong>Cython的一个很好的特性是，它使使用Cython感觉像是在使用纯Python工作；</li>
<li>几种操作（<strong>但是没必要知道所有的编译方法，所有按需学习</strong>）：
<ul>
<li>Cython代码可以从IPython解释器编译并交互式地运行。</li>
<li>可以在导入时自动编译。</li>
<li>它可以通过Python的distutils等构建工具单独编译。</li>
<li>它可以集成到标准构建系统中，如make、CMake或SCons。</li>
</ul>
</li>
</ul>
<h2 id="1-cython编译流程pipeline">1 Cython编译流程（pipeline）</h2>
<p>该管道的工作是将Cython代码转换为Python<strong>扩展模块</strong>（extension module），Python解释器可以导入并使用该模块。</p>
<p><strong>该管道包括两个阶段</strong>：</p>
<ol>
<li>第一阶段由cython编译器处理，它将cython源代码转换为优化的、与<strong>平台无关</strong>的C或c++。</li>
<li>第二阶段使用标准的C或c++编译器将生成的C或c++源代码<strong>编译成共享库</strong>（shared library）。生成的<strong>共享库</strong>是<strong>平台相关</strong>的。共享库在unix系统上是<code>.so</code>后缀。</li>
</ol>
<p>我们将这个<strong>编译后的模块称为扩展模块（extension module）</strong>，它可以<strong>像用纯Python编写一样导入和使用</strong>。</p>
<p>cython编译器是一个源代码到源代码的编译器，生成代码经过了高度优化。由Cython生成的C代码比典型的手工编写的C代码要快。<strong>所以我们可以看到生成的c文件里面的代码我们都是看不太懂的，而且很长很复杂</strong>。Cython生成的C代码也具有高度可移植性，可以同时支持所有常见的C编译器和许多Python版本。</p>
<p>编译的时候确保两个工具安装了：</p>
<ol>
<li>c或c++编译器； gcc</li>
<li>cython编译器。anaconda自带或 <code>pip install cython -i https://pypi.tuna.tsinghua.edu.cn/simple/</code>安装。</li>
</ol>
<h3 id="安装cython">安装cython</h3>
<ul>
<li>anaconda自带；</li>
<li><code>pip install cython -i https://pypi.tuna.tsinghua.edu.cn/simple/</code>安装。</li>
</ul>
<h2 id="2-编译方式1-using-distutils-with-cythonize标准方式">2 编译方式1 Using distutils with cythonize（标准方式）</h2>
<p>distutils包是用于构建、打包和分发Python项目的，python再带。</p>
<h3 id="21-一个pyx文件内">2.1 一个pyx文件内</h3>
<p>步骤：</p>
<ol>
<li><code>cythonize</code>将Cython编译成c/c++语言；</li>
<li>然后，<code>distutils</code>（distributing utils）将c/c++语言编译成可以执行代码os,即扩展模块。</li>
</ol>
<p>代码：</p>
<pre><code class="language-python"># setup.py
# cythonize在.pyx源文件上调用cython编译器，setup将生成的C或c++代码编译成Python扩展模块。
from distutils.core import setup, Extension
from Cython.Build import cythonize

# setup(ext_modules=cythonize('fib.pyx')) #这样写的话其实没有显示知道编译以后扩展包的名字
setup(ext_modules = cythonize(Extension(name='warp_fib', sources=[&quot;fib.pyx&quot;])) #指定扩展包名
</code></pre>
<p>然后命令行执行编译即可：</p>
<pre><code class="language-c">// build_ext参数是一个命令，指示distutils构建扩展对象或cythonize调用创建的对象。可选的——inplace标志指示distutils将每个扩展模块放在其各自的Cython.pyx源文件旁边。
$ python setup.py build_ext --inplace
</code></pre>
<h4 id="编译生成的文件">编译生成的文件</h4>
<p><img src="https://jzq6520.github.io/post-images/1573020753272.png" alt=""><br>
这里面 <code>.c .o .so</code>这几个文件都是编译生成的文件。</p>
<h3 id="22-包含c语言文件和pyx文件">2.2 包含c语言文件和pyx文件</h3>
<pre><code class="language-python">from distutils.core import setup, Extension from Cython.Build import cythonize

# First create an Extension object with the appropriate name and sources. 
ext = Extension(name=&quot;wrap_fib&quot;, sources=[&quot;cfib.c&quot;, &quot;wrap_fib.pyx&quot;])

# Use cythonize on the extension object. 
setup(ext_modules=cythonize(ext))
</code></pre>
<h3 id="23-包含预先编译好的动态库">2.3 包含预先编译好的动态库</h3>
<pre><code class="language-python">from distutils.core import setup, Extension 
from Cython.Build import cythonize

ext = Extension(name=&quot;wrap_fib&quot;, sources=[&quot;wrap_fib.pyx&quot;], library_dirs=[&quot;/path/to/libfib.so&quot;], libraries=[&quot;fib&quot;])

setup(ext_modules=cythonize(ext))
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cython(1)--基本概念]]></title>
        <id>https://jzq6520.github.io/post/cython1</id>
        <link href="https://jzq6520.github.io/post/cython1">
        </link>
        <updated>2019-11-06T02:19:26.000Z</updated>
        <content type="html"><![CDATA[<p>以下全来自于对《Cython：A Guide for Python Programmers》一书的学习。<br>
cython最大的作用就是用来加速python代码。</p>
<p>cython是什么：</p>
<ul>
<li>Cython是融合python和c/c++的一种编程语言；</li>
<li>cython编译器可以将Cython代码编程成c/c++。然后编译后可以被python当成扩展包进行调用。</li>
</ul>
<p>Cython的美妙之处在于:它将Python的表达性和动态性与C的裸机性能（bare-metal performance）结合在一起，<strong>同时仍然感觉像Python</strong>。</p>
<h2 id="1-特点">1. 特点</h2>
<ul>
<li>Cython代码里面可以写python代码，也就是说Cython可以理解python代码。</li>
<li>需要用cdef声明变量</li>
</ul>
<h2 id="2-速度">2. 速度</h2>
<p>总结：</p>
<ol>
<li>能用：循环密集、算术密集</li>
<li>不能用：I/O密集</li>
</ol>
<p>然后看分析：</p>
<p>速度快，可以看一下对比数据。<br>
<img src="https://jzq6520.github.io/post-images/1573008098399.png" alt=""></p>
<ul>
<li>函数调用：<code>fib(0)</code>可以是看做函数调用的时间对比，<code>fib(0)</code>运行时主要消耗在<strong>调用相应语言中的函数</strong>所需的时间上;<strong>运行函数体的时间相对较短</strong>。从表1-1中可以看到，Cython生成的代码比调用Python函数快一个数量级，比手工编写的快两倍多.</li>
<li>循环：python中的循环相对c语言来说是非常慢的。加速循环Python代码的一个可靠的方法是找到方法将Python for和while循环移动到<strong>已编译的代码中</strong>，可以通过<strong>调用内置函数</strong>，也可以使用类似<strong>Cython</strong>的东西来完成转换。</li>
<li>数学操作：python在做算术运算的时候是不知道数据类型的，所以还需要去查找，但是c和cyhton是显示声明数据类型的。</li>
<li>堆栈与堆分配：在C级，动态Python对象完全是堆分配的。Python煞费苦心地智能地管理内存，使用内存池并内化常用的整数和字符串。但事实仍然是，创建和销毁对象——任何对象，甚至标量——都会增加处理动态分配内存和Python内存子系统的开销。因为Python浮动对象是不可变的，所以使用Python浮动的操作涉及到创建和销毁堆分配的对象。Cython版本的fib声明所有变量都是堆栈分配的C double。通常，堆栈分配比堆分配快得多。此外，C浮点数是可变的，这意味着for循环体在分配和内存使用方面更有效。</li>
</ul>
<p>但是，值得注意的是，<strong>并不是所有Python代码在使用Cython编译时都能看到巨大的性能改进</strong>。前面的fib示例是有意对CPU进行限制的，这意味着所有的运行时都是在CPU寄存器内操作几个变量，几乎不需要移动数据。相反,如果这个函数内存约束(例如,添加两个大数组的元素),I / O绑定(例如,从磁盘读取大型文件),或网络绑定(例如,从一个FTP服务器下载文件),Python之间的性能差异,C, Cython可能显著降低(内存受限操作)或完全消失(<strong>I/O密集型</strong>或network密集型操作)。</p>
<p>当提高Python的性能是我们的目标时，帕累托原则就对我们有利:我们可以预期，<strong>一个程序大约80%的运行时间是由20%的代码造成的</strong>。这个原则的一个推论是，如果不进行分析，很难找到那20%。但是没有理由不分析Python代码，因为它的内置分析工具非常简单。<strong>在我们使用Cython改善性能之前，获取分析数据是第一步</strong>。</p>
<ul>
<li>也就是说，<strong>如果我们通过分析确定程序中的瓶颈是由于I/O或网络限制造成的，那么我们就不能指望Cython在性能上有显著的改进</strong>。</li>
</ul>
<h2 id="用cython包装wrappingc语言">用cython包装（wrapping）c语言</h2>
<p>python调用c语言的方法。<br>
下面给出的只是其中一种方式，是吧c语言分来开存放，还有一种方式，是写在一个文件里。</p>
<p>c代码：</p>
<pre><code class="language-c">// cfib.c
double cfib(int n) { 
    int i; 
    double a=0.0, b=1.0, tmp; 
    for (i=0; i&lt;n; ++i) {
            tmp = a; a = a + b; b = tmp; 
        } 
    return a; 
}
</code></pre>
<pre><code class="language-c">// cfib.h
double cfib(int n);
</code></pre>
<p>cython代码：</p>
<pre><code class="language-python"># wrap_fib.pyx
cdef extern from &quot;cfib.h&quot;: 
    double cfib(int n)

def fib(n): 
&quot;&quot;&quot;Returns the nth Fibonacci number.&quot;&quot;&quot; 
    return cfib(n)
</code></pre>
<p>最后经过编译就可以在python里面调用了<code>from wrap_fib import fib</code>。cython代码里面需要用定义一个包装（wrapping）函数来返回c定义的函数。</p>
<p>并且，因为Cython语言理解Python，并且可以访问Python的标准库，所以我们可以利用Python的所有强大功能和灵活性。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-RetinaNet和Focal Loss]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-retinanet-he-focal-loss</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-retinanet-he-focal-loss">
        </link>
        <updated>2019-10-31T07:34:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-focal-loss">1 Focal Loss</h2>
<p>文章的题目是Focal Loss for Dense Object Detection，密集目标检测。主要还是解决那些和目标没有交集的anchor的分类，这些anchor是最简单的负样本，但是他们数量巨多。</p>
<ul>
<li><strong>样本不平衡：对于RPN来说大部分anchor都是负样本，对于yolo来说大部分格子都是负样本</strong>。</li>
<li>两阶段目标检测训练的最后的分类部分通常都是稀疏的，什么意思呢，<em>ﬁrst stage generates a sparse set of candidate object locations, and the second stage classiﬁes each candidate location as one of the foreground classes or as background using a convolutional neural net- work</em>.（第一阶级会产生一些建议区域,然后第二阶段对这些区域进行分类，这样对第二阶段来说需要分类的区域已经大大减少了，<strong>所以说是稀疏的</strong>)，见 Fast R-CNN。</li>
<li>而单阶段的目标检测训练通常是密集的，什么意思呢，就是需要对所有可能区域进行分类，见YOLO。举个例子：以输入为800X800,五层金字塔尺度分别为100X100,50X50,25X25,12X12,6X6,<strong>算一下anchor数量，119745</strong>， <strong>10几万anchor覆盖在整张图像中，密密麻麻</strong>，让你知道什么是密集检测，哈哈，插句话，<strong>10几万anchor中绝大多数都是负样本</strong>，无用的计算太多，这也是现在anchor free的一个出发点。【2】</li>
<li>单阶段检测（高密度检测）训练遇到的主要问题是<strong>样本不平衡</strong>，因为通常正样本总是小于负样本的，例如yolo是根据<strong>目标中心</strong>是不是落在格子里，落在格子里那么这个格子就是正样本，否则这个格子就是负样本，<strong>那么大部分格子肯定是没有目标的，也就是说正负样本是非常不平衡的</strong>。而Fast R-CNN也是，region proposal很多，但是里面<strong>大部分区域（anchor）应该是负样本，少部分（anchor）是正样本</strong>，所以Fast R-CNN训练分类的时候是通过(稀疏)采样。因为anchor是要分类的，所以大部分anchor是负样本。</li>
<li>在这篇文章之前一般都是双阶段的效果比较好（proposal-driven mechanism）。</li>
<li>所以有没有办法让单阶段的效果变好，毕竟单阶段的速度快。</li>
<li><strong>文章就抓住了训练时候正负样本既不平衡的这个主要矛盾</strong>来提高模型精度。最后达到了和FPN， mask R-CNN这样的精度。</li>
<li>最好的模型是ResNet-101- FPN(当然是文章提出的时候，现在backbone都有很多优秀的了）， 5fps。</li>
</ul>
<h3 id="11-之前目标检测中解决样本不平衡的方法">1.1 之前目标检测中解决样本不平衡的方法</h3>
<p>利用第一阶段<strong>过滤到大部分假阳（背景）</strong>，然后第二阶段再进行下面的工作：</p>
<ol>
<li>对需要分类的区域进行采样，见Fast R-CNN。</li>
<li>在线难例挖掘 online hard example mining (OHEM)</li>
</ol>
<p>相比之下，一个单级目标检测必须处理一组更大的候选对象位置，这些对象位置是通过图像定期采样的。实际上，这常常相当于枚举约100k个位置，这些位置密集地覆盖了空间位置、尺度和纵横比。虽然也可以采用类似的抽样启发法，但它们的效率很低，因为训练过程仍然由容易分类的背景示例所主导。这种低效率是对象检测中的一个经典问题，通常通过引导或硬示例挖掘等技术来解决。</p>
<p>这种不平衡导致了两个问题:(1)训练效率低，因为大多数区域都是<strong>简单的负样本</strong>，这种简单的负样本对训练作用不大;(2)整体而言，很容易被这些<strong>容易的但是很多</strong>的负样本影响，<strong>导致模型退化</strong>。</p>
<h3 id="12-focal-loss">1.2 Focal loss</h3>
<ul>
<li><strong>Huber损失（类似smooth L1）</strong>，该函数通过降低具有较大错误的示例(硬示例)的损失的权重来<strong>减少异常值的贡献</strong>，因为在误差大的时候损失的梯度是不变的，不变很重要 这样就不容易被误差值所影响，其实也就是差值一个是不进行平方，一个进行平方计算，平方计算就会增长很快，而在误差小的时候梯度慢慢减少。</li>
<li>相比之下，<strong>Focal loss</strong>的重点并不是处理异常值，而是通过减权<strong>简单的例子</strong>来解决阶级不平衡，这样即使简单的例子的数量很大，它们<strong>对总损失的贡献也很小</strong>。<strong>这点非常重要，是减少简单例子的影响，所以选择这个loss的适合一定要思考你的负样本是不是所谓的简单的负样本</strong>。</li>
</ul>
<p>下面来看一下cross entropy的损失函数曲线（其实就是-log(p)）：<br>
<img src="https://jzq6520.github.io/post-images/1572512564723.png" alt=""><br>
cross entropy可以看着只计算这个神经元负责的当前正样本的loss，假设有两个神经元分别预测目标和背景，那么对于两个神经元来说这两个都是正样本（因为第一个负责预测目标，第二负责预测背景呀，所以第一个神经元的正样本就是目标，第二个神经元的正样本就是背景，都是要预测概率为1的）因为是<strong>onehot</strong>。</p>
<p>这里有个假设，假设有个正样本，我们预测他为正样本的概率是<code>0.6~1</code>之间，我们就算这个样本是简单的，<strong>也就说能预测对的就算是简单的</strong>。</p>
<p><strong>所以我们观察上图可以发现，在我们定义的简单样本的区间内，其实普通的cross entropy的loss还是继续下降的（也就是说还是有一点的loss的），因为cross entropy总是希望概率可以很大很确信</strong>。所以可以看出focal loss在这个区间内的loss是很小的，而预测不对的话损失是很大的。<strong>这就是Focal loss核心思想</strong>。划重点。</p>
<p>因为我们希望在目标检测中那么多的<strong>负样本不要共享很多loss，预测的差不多得了，能大于0.6就好了，知道你是就行了，超过0.6，loss就少贡献一点</strong>，但是因为正负都是同等对待的，所以正样本预测的概率数值也会小一点。</p>
<h4 id="121-普通的加权方式">1.2.1 普通的加权方式</h4>
<figure data-type="image" tabindex="1"><img src="https://jzq6520.github.io/post-images/1572514035161.png" alt=""></figure>
<ul>
<li>虽然alpha参数可以平衡正负样本，但是还无法区分简单和困难的例子，我们希望减少简单例子的权重而关注（Focus）那些难的样本。</li>
<li>α ∈ [0, 1] for class 1（正样本） and 1−α for class −1（负样本）.</li>
<li>因为RetinaNet里面是直接预测k个类，而不是先预测是不是目标。所以这里的加权应该是直接在是目标的类别加权α，然后负样本加权1−α。</li>
<li>下图可以看出<strong>只加α</strong>和<strong>加了focal与α</strong>的α的值是不一样的，与我们直观理解一样只加α的时候正样本的权重应该要高一点，而加了focal以后反而正样本的权重要小一点好：<br>
<img src="https://jzq6520.github.io/post-images/1572601464394.png" alt=""></li>
</ul>
<h4 id="122-focal-loss">1.2.2 focal loss</h4>
<p>所以focal loss增加了一个调制的因子<code>(1 − pt )^γ</code>，当γ大于0时就是focal loss，等于0的适合是普通的交叉熵loss。<br>
<img src="https://jzq6520.github.io/post-images/1572577100834.png" alt=""></p>
<p>有两点：</p>
<ol>
<li>可以发现当pt趋向于0的时候，<code>(1 − pt )^γ</code>因子就接近1，loss的值是非常大的，当pt趋向于1的时候（或者说是子啊0.6到1的时候）<code>(1 − pt )^γ</code>这个因子就会解决与0，那么loss贡献就会非常的小，这个和γ参数也有关，γ越大那么预测概率解决于0的时候的loss越小。</li>
<li><em>we found γ = 2 to work best in our experiments</em>，作者实验下来 γ = 2的时候最好。</li>
</ol>
<p>实际使用中作者还加了alpha权重：<br>
<img src="https://jzq6520.github.io/post-images/1572577872330.png" alt=""></p>
<h2 id="2-retinanet">2 RetinaNet</h2>
<p><strong>单阶段（one-stage）的目标检测，使用了Focal Loss解决分类的正负样本不平衡问题</strong>。</p>
<p>为什么叫这个名字，因为是一个密集目标检测（像视网膜一样密集），这里的密集意思是一次性在所有区域里面检测模型，而不是像两阶段一样从建议区域里面检测。</p>
<p>retinanet只是使用了Focal Loss的一个网络，其他和这个loss无关，采样的都是以前的一些结构，然后很好的组合成一块。</p>
<p>总的结构是这样的：<br>
<img src="https://jzq6520.github.io/post-images/1572589354355.png" alt=""></p>
<p>分别说一下RetinaNet里面用的这4个东西的设置：</p>
<ol>
<li>FPN</li>
<li>Anchor</li>
<li>打label</li>
<li>分类分支</li>
<li>回归分支</li>
</ol>
<h3 id="21-fpn">2.1 FPN</h3>
<ul>
<li>右边特征金字塔层（level）<strong>从p3到p7</strong>用来检测，FPN原文中只是用了p3到p5.<em>We construct a pyramid with levels P3 through P7 , where <code>l</code> indicates pyramid level (P <code>l</code> has resolution 2^<code>l</code>(2的l次方) lower than the input)</em>.</li>
<li>金字塔层channel为256。</li>
<li>P6是在p5上用stride为2的3x3卷积。P7也是这样，然后都有relu。, P 6 is obtained via a 3×3 stride-2 conv on C 5 , and P 7 is computed by applying ReLU followed by a 3×3 stride-2 conv on P 6 .</li>
</ul>
<h3 id="22-anchor">2.2 anchor</h3>
<ul>
<li>P3, P4, P5,P6,P7 的基础面积分别是32，64，128，256，512，然后实际面积是基础面积乘以三个倍数，得到三种面积，倍数分别是<code>2^0=1，2^(1/3)=1.26，2^(2/3)=1.58</code>；然后每种面积都有三种宽高比1：1， 1：2，2：1，所以每个金字塔层总共有9个anchor。</li>
</ul>
<h3 id="23-打label">2.3 打label</h3>
<p><strong>记住正负样本是对anchor打的</strong>。</p>
<p>这里有个特例就是<strong>iou=0的时候</strong>，iou等于0说明anchor的框是完全和目标没有交集的，这个时候就<strong>没有box相对坐标可以回归了</strong>。</p>
<p>这里打label的阈值和RPN中有点不同，这里是与目标框iou大于等于0.5的anchor算是正样本，小于0.4的anchor算是负样本，<code>[0.4，5）</code>iou之间不计算loss.</p>
<p><strong>这里来区分一下简单负样本，难负样本</strong>：</p>
<ul>
<li><strong>简单负样本</strong>：那些和目标框没有交集的anchor，也就是IOU为0的anchor。</li>
<li><strong>难负样本</strong>：和目标框有交集的，然后iou大于0小于0.4的，当然在这个范围内越接近0.4越难了，因为很接近目标啦，但还是负样本。</li>
</ul>
<h3 id="24-分类分支">2.4 分类分支</h3>
<ul>
<li>不同金字塔层的分类分支是参数共享的。</li>
<li>分类分支和回归分支没有共同的conv，这个和RPN不一样。</li>
<li>分支的conv很多，加上最后一个预测的有5个3x3conv，最后预测的时候也是3x3conv。</li>
<li>最后预测的适合是sigmoid激活，而不是softmax。这样好像对Focal loss的使用好，因为可以想象focalloss应用范围都是概率要0到1之间的，softmax是加起来才是1 那可能本身概率就会小一点。</li>
<li>每个conv层的channel都是256，最后输出channel是k个类别乘以A个anchor。</li>
</ul>
<h3 id="25-回归分支">2.5 回归分支</h3>
<ul>
<li>4乘A个channel。</li>
<li>回归离anchor最近的目标，<em>regressing the offset from each anchor box to a nearby ground-truth object</em>。<strong>一个目标可能分配给多个anchor，但是一个anchor只会分配一个目标</strong>。</li>
<li>其他设置和分类分支一样，channel，参数共享，conv什么的都一样</li>
</ul>
<p><em>那么iou为0的时候要回归坐标吗？负样本要回归坐标吗？</em>：</p>
<ol>
<li>regressing the offset from each anchor box to a nearby ground-truth object, <strong>if one exists</strong>.即如果存在的话要回归，<strong>具体要不要得看代码了</strong>。<strong>但是RPN里面是只计算正样本的回归损失，见RPN博客中的损失函数</strong>。</li>
</ol>
<h3 id="26-推理阶段-inference阶段-也就是预测阶段">2.6 推理阶段 Inference阶段 也就是预测阶段</h3>
<p>在推理阶段，由于预测出来的款也很多，作者对金字塔每层特征图都使用0.05（为什么这么低？不是0.5吗？因为后面还进行排序然后选前1000）的置信度阈值进行筛选，然后取置信度前1000的候选框（不足1000更好） ，接下来收集所有层的候选框，进行NMS,阈值0.5。</p>
<h3 id="25-训练阶段">2.5 训练阶段</h3>
<ul>
<li>γ = 2这是最好，是通过【0.5~5】测出来的。</li>
<li>再次强调<strong>使用所有anchor训练</strong>的，<em>We emphasize that when training RetinaNet, the focal loss is applied to all <strong>∼ 100k anchors</strong> in each sampled image</em>。以输入为800X800,五层金字塔尺度分别为100X100,50X50,25X25,12X12,6X6,<strong>算一下anchor数量，119745个</strong>。</li>
<li><strong>所有的loss相加哦，然后除以分配的ground-truth box的anchor（有交集）数量</strong>，因为<strong>简单负样本</strong>贡献loss少，几乎为0，<strong>难负样本还是贡献loss的</strong>，所以加入计算，因为负样本也是和目标框有交集的。<em>The total focal loss of an image is computed as the sum of the focal loss over all ∼ 100k anchors, normalized by the number of anchors assigned to a ground-truth box.</em></li>
<li>因为去除掉iou为0的anchor以后，还剩下一部分有和目标交集的难负样本呀，所以这部分的负样本的数量还是大于正样本的，所以文章中还对loss加了个权重，来平衡这部分的loss。最后我们注意,α,重量分配到罕见的类。<em>Finally we note that α, the weight assigned to the rare class, also has a stable range.</em>(for γ = 2, α = 0.25 works best)，这两个参数需要相互结合调整。</li>
<li>α ∈ [0, 1] for class 1 and 1−α for class −1.</li>
</ul>
<h4 id="251-参数初始化">2.5.1 参数初始化</h4>
<p>所有新conv层RetinaNet子网中除了最后一个conv外都初始化为bias=0。最后conv层用下面的公式算出来的值初始化，文章中设置为π = .01：<br>
<img src="https://jzq6520.github.io/post-images/1572600783017.png" alt=""><br>
作用：<strong>这样初始化可以防止大量的背景锚点在训练的第一次迭代中产生较大的、不稳定的损失值</strong>。</p>
<h3 id="网络训练细节">网络训练细节</h3>
<p>RetinaNet is trained with stochastic gradient descent (SGD). We use synchronized <strong>SGD</strong> over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise speciﬁed, all models are <strong>trained for 90k iterations</strong> with an <strong>initial learning rate of 0.01</strong>, which is then <strong>divided by 10 at 60k and again at 80k iterations</strong>. We use horizontal image ﬂipping as the only form of data augmentation unless otherwise noted. <strong>Weight decay of 0.0001 and momentum of 0.9 are used</strong>. The training loss is the sum the focal loss and the standard smooth L 1 loss used for box regression [10]. <strong>Training time</strong> ranges between <strong>10 and 35 hours</strong> for the models in Table 1e.</p>
<h2 id="结果">结果</h2>
<p>信息量确实很大，文章很好，看不动了，好累，直接贴实验结果吧。<br>
<img src="https://jzq6520.github.io/post-images/1572601681760.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572601690019.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572601695560.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572601732469.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572601737614.png" alt=""></p>
<h2 id="引用">引用</h2>
<ul>
<li>各种loss：http://baijiahao.baidu.com/s?id=1603857666277651546&amp;wfr=spider&amp;for=pc</li>
<li>fpn anchor：https://www.cnblogs.com/wzyuan/p/10847478.html</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读特征金字塔网络 (FPN)]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-te-zheng-jin-zi-ta-wang-luo-fpn</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-te-zheng-jin-zi-ta-wang-luo-fpn">
        </link>
        <updated>2019-10-31T02:39:07.000Z</updated>
        <content type="html"><![CDATA[<p>对于网络的卷积特征的几个重要理解：</p>
<ul>
<li>但由于深度不同，导致了不同层较大的<strong>语义差异</strong>。高分辨率图的低层特征损害了其对目标识别的表征能力。</li>
<li>语义差异：语义差异就说对目标的类别的识别的能力的差异，低层特征主要是一些边缘，形状等交基础的特征，对整个物体的类别识别帮助不大，而高层特征可能是更加具体的特征，已经很能反映整个是什么物体了，所以底层特征语义较差。</li>
<li>利用卷积神经网络特征层次结构的金字塔形状，同时构建一个在所有尺度上都具有<strong>强大语义</strong>的特征金字塔，如果不看skip连接的话FPN相当于是在网络后面接了另外的很多层卷积，所以属于高层特征。</li>
<li>所以有了另外一路<strong>上采样的卷积层</strong>以后，就具有了在<strong>每个尺度上都有高语义的特征图</strong>。即依赖于一个通过<strong>自顶向下（即上采样路径）路径和横向连接</strong>将<strong>低分辨率语义强的特征</strong>与<strong>高分辨率语义弱的特征</strong>相结合的架构。所以这也是最后特征图的channel都是一样的原因吧，这里更加注重语义特征。</li>
<li>FPN文章中是用FPN和Faster R-CNN结合。</li>
</ul>
<h2 id="结构实现细节">结构实现细节</h2>
<ul>
<li>上采样使用最近邻插值</li>
<li>只使用c2,c3,c4,c5尺度的特征图。</li>
<li>中间的skip连接是用1x1卷积把channel数降到256.</li>
<li>fpn是完全对称的<br>
<img src="https://jzq6520.github.io/post-images/1572492067196.png" alt=""></li>
<li><strong>使用同样的channel的原因</strong>：因为金字塔的所有层次都使用<strong>共享的分类器/回归器</strong>，就像传统的特征图金字塔一样，我们修正了所有特征图的特征维数。Because all levels of the pyramid use shared classiﬁers/regressors as in a traditional featurized image pyramid, we ﬁx the feature dimension (numbers of channels, denoted as d) in all the feature maps. We set d = 256 in this pa- per and thus all <strong>extra convolutional</strong> layers have 256-channel outputs.There are no non-linearities in these <strong>extra layers</strong>, which we have empirically found to have minor impacts.<strong>在这些额外的层中不存在非线性，我们根据经验发现它们的影响很小</strong>。</li>
</ul>
<h2 id="基于特征金字塔fpn的rpn">基于特征金字塔(FPN)的RPN</h2>
<p>虽然fpn和rpn很相似，但是这两个p意思不一样，一个是pyramid（金字塔），一个是proposal（建议）。</p>
<ul>
<li><strong>RPN参数共享</strong>：<strong>head</strong>：文章中把RPN的那个操作的模块叫做<strong>头部</strong>，即一个3x3的卷积，然后加两个1x1的卷积进行分类和回归。</li>
<li>FPN中的RPN head（头部）是<strong>参数共享</strong>的。</li>
<li><strong>每个特征图负责检查一种尺度</strong>：<strong>assign anchors of a single scale to each level</strong>.并且每层只有一个尺度的anchor，即一个层上没有多尺度，每层只负责一种大小的目标检测，这里说的大小是指物体的面积，用面积衡量大小，但是物体的宽高比是不一样的。因为RPN已经是在不同尺度的特征图上做了，所以不需要再在一个特征图上做不同尺度的anchor了。</li>
<li>anchor设置：对于在faster R-CNN上用的anchor，<strong>在高分辨率特征图上检测小目标，在低分辨率上检测大目标</strong>。面积设置，特征图<strong>从大到小</strong>p2,p3,p4,p5分别的anchor面积是32^2 , 64^2 , 128^2 , 256^2，且每个面积有三个宽高比：{1:2, 1:1, 2:1} 。</li>
<li>实验对比共享rpn头和不共享，共享的效果比较好。</li>
</ul>
<h2 id="打label">打label</h2>
<ul>
<li>正样本：iou大于0.7的所有anchor或和ground-truth iou最早的anchor（因为可能存在所以anchor都和ground-truth iou小于0.7，那么这个是后就取iou最大的anchor了），<em>positive label if it has the highest IoU for a given ground- truth box or an IoU over 0.7 with any ground-truth box</em>。</li>
<li>负样本：所有iou小于0.3的anchor，a negative label if it has IoU lower than 0.3 for all ground-truth boxes。</li>
<li>ground-truth没明确分配到哪个尺度的特征图，只要anchor分配好就可以了。原文：<strong>Note that scales of ground-truth boxes are not explicitly used to assign them to the levels of the pyramid; instead, ground-truth boxes are associated with anchors, which have been assigned to pyramid levels. As such, we introduce no extra rules in addition to those</strong>。因为iou是两个物体面积差不多大才是大的，一个大物体和一个小物体全部覆盖，那iou也是低的。所以物体会根据iou大小自动分配的。</li>
<li></li>
</ul>
<h2 id="fpn的roi-pooling">FPN的ROI pooling</h2>
<p>那么FPN的roi pooling怎么做呢，因为有这么多个的特征图。</p>
<p>首先我们要搞清楚一个概念，<strong>RPN的作用是确定出原图上的目标ROI区域</strong>（而非特征图上的区域），这时候我们再将原图上的ROI坐标映射到特征图上，然后把特征图上的roi区域拿过来进行分类和区域的回归矫正。</p>
<p>理解了这一点，那就清楚了，RPN确定的只是在原图上的roi区域，所以RPN做完以后，anchor就没用了，这时候就根据roi来确定我要在哪一层选择这个区域的特征来进行分类和回归。</p>
<p>虽然我们直观上理解是哪个特征图检测出了这个目标，就由哪个特征图所roi pooling，<strong>其实不是的</strong>，文章中是对roi进行重新分配了，<strong>所以RPN做完以后预测出的候选区域就和特征图没有半毛钱关系了，后面就等着再分配了</strong>。</p>
<p>那么分配规则是怎么样的呢，文章中是按照下面这个公式来确定分配给哪一层，那个类似于中括号的是<strong>取整</strong>，应该是向上取整，如果是5到4.1就是再第5层特征层做。。<br>
<img src="https://jzq6520.github.io/post-images/1572504129264.png" alt=""></p>
<p>首先，先通俗理解一下，前面最开始分配anchor的时候就知道了，深层特征图检测大目标，浅层的检测小目标，所以这里也是一样，<strong>大目标（即大ROI）分配给低分辨率（小特征图）的特征图即P5，小目标（小ROI）分配给到的高分辨率（大特征图）的特征图</strong>。</p>
<p>如果我们最小的特征图是P5，那么k0初始化为5，然后这里的224应该不要这样写好，应该写成输入图像的大小，如果输入是448那这里就是448了，意思就是如果roi是图像的一半，那么应该分配给P4特征图做roi pooling。以此类推。</p>
<p>其实这个公式算出来就是，缩小一半（2倍）就是到倒数第二层，缩小4倍就是到再下一层，缩小8倍就是再下一层。那么1到2倍就是再最后一层了。因为以2为底，log二分1就是-1，log四分之一就是-2。</p>
<p>然后做roi pooling都是7x7，最后堆叠到一个batchsize里面（这个和Fast R-CNN是一样的，见之前的博客）。然后预测分类和回归的适合连两个全连接。<em>1024-d fully-connected (fc) layers (each followed by ReLU) before the ﬁnal classiﬁcation and bounding box regression layers</em>.</p>
<h2 id="参数细节">参数细节</h2>
<h3 id="region-proposal-with-rpn">Region Proposal with RPN</h3>
<p>All architectures in Table 1 are trained end-to-end. The input image is resized such that its shorter side has 800 pixels. We adopt synchronized SGD training on 8 GPUs. A mini-batch involves 2 images per GPU and 256 anchors per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the ﬁrst 30k mini-batches and 0.002 for the next 10k. For all RPN experiments (including baselines), we include the anchor boxes that are outside the image for training, which is unlike [29] where these anchor boxes are ignored. Other implementation details are as in [29]. Training RPN with FPN on 8 GPUs takes about 8 hours on COCO.</p>
<h3 id="object-detection-with-fastfaster-r-cnn">Object Detection with Fast/Faster R-CNN</h3>
<p>The input image is resized such that its shorter side has 800 pixels. Synchronized SGD is used to train the model on 8 GPUs. Each mini-batch in- volves 2 image per GPU and 512 RoIs per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the ﬁrst 60k mini-batches and 0.002 for the next 20k. We use 2000 RoIs per image for training and 1000 for testing. Training Fast R-CNN with FPN takes about 10 hours on the COCO dataset.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读YOLO目标检测系列 v2]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-yolo-mu-biao-jian-ce-xi-lie-v2</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-yolo-mu-biao-jian-ce-xi-lie-v2">
        </link>
        <updated>2019-10-30T02:59:08.000Z</updated>
        <content type="html"><![CDATA[<p>哎，看下来 yolo v1 v2总感觉作者写文章一般般，写的不清晰。</p>
<h2 id="anchor尺寸设置">anchor尺寸设置</h2>
<p>不像Faster R-CNN里面一样使用手动设置anchor的尺寸，这里使用k-means来进行聚类，例如我们要五种类型的anchor，那么就聚为5类，那么聚类中心的尺寸就是我们的anchor尺寸。</p>
<p>但是这里的聚类距离选择的不是欧式距离，而是<code>1-IOU(box, centroid)</code>作为<strong>距离度量函数</strong>。这里说明一下怎么聚类，因为一个anchor是有两个值（RPN中提到的），一个是像素个数，还有一个长宽比，所以这里应该<strong>将长宽的大小作为聚类特征</strong>（因为长宽大小就决定了有多少像素和长宽比了），论文中说在做目标检测的时候主要是想要一个较高的IOU，但是k-means在聚类的时候是计算一个较近的距离，所以这里是1减去IOU（因为iou越大，1减去iou就越小了）。所以这里用<code>1-IOU</code>衡量，那怎么计算iou呢，我们可以假设box的中心点是<code>（0，0）</code>，因为是对现在所有的目标的box进行聚类找出我们的anchor的尺寸，所以我们的数据就是所有目标的box的长宽，然后假设中心点是<code>（0，0）</code>，这样我们就可以计算两个box之间的iou了。</p>
<p>其实RPN中是想要多种尺寸的anchor，也就是不同长宽的box，但不是说一定要像RPN中说的一样要指定几种面积，然后每种面积有几种长宽比。所以YOLO V2中就使用聚类来得到不同长宽的anchor（box）。</p>
<h2 id="label制作">label制作</h2>
<p>置信度：应该也是一个1和0，落在网格里面就是算有目标，这个和RPN不一样，RPN是通过iou计算正负样本的，这里是看是不是落在网格里面，如果不是这个区别那么就变成了普通的RPN了。原文：The network predicts 5 coordinates for each bounding box, t x , t y , t w , t h , and t o 。to就是置信度。</p>
<p>但是，需要注意的是，每个cell中实际上有5个anchor，并不是每个anchor的会预测这个物体，我们只会选择一个长宽和这个bbox最匹配的anchor来负责预测这个物体。那么什么叫长宽最为匹配？这个实际上就是将anchor移动到图像的右上角，bbox也移动到图像的左上角，然后去计算它们的iou，iou最大的其中一个anchor负责预测这个物体。这点和RPN中不一样，RPN只需要iou符合大于某个阈值就可以了。感觉yolo复杂多了。</p>
<p>和RPN还有个不同的是，RPN的是目标和不是目标是两个神经元预测，也就是softmax，这里是一个预测 sigmoid。</p>
<p>在RPN 里面中心点的是回归一个相对于宽高的偏移量，即：<br>
<img src="https://jzq6520.github.io/post-images/1572412895262.png" alt=""><br>
原文中符号写错了：<br>
<img src="https://jzq6520.github.io/post-images/1572414868247.png" alt=""><br>
这样回归其实是有缺点的，这个公式回归出来的中心点是不受约束的，它的<strong>活动范围是全图</strong>，可以想象其实我们回归出t以后然后计算出的<strong>x,y的值可以是图像中的任意位置</strong>，<strong>在随机初始化的情况下，模型需要很长时间才能稳定地预测出合理的偏移量</strong>。</p>
<p>所以yolo v2就把关键点回归约束在网格之内，是相对于网格左上角的一个偏移量，这样归一化以后的偏移量就是在0到1之间。<strong>这样回归会更加稳定</strong>。<br>
即<code>tx = bx - cx, ty = by - cy</code>，</p>
<p>距离如下图所示，那个<strong>对t做变换的参数不用管他</strong>，只是为了说明最好是加sigmoid的激活的， σ是sigmoid函数，也就是为了约束到0到1之间。实际上做数据的时候就是转换成t就可以了。<br>
<img src="https://jzq6520.github.io/post-images/1572413565039.png" alt=""><br>
宽高和RPN一样：<br>
<img src="https://jzq6520.github.io/post-images/1572413551539.png" alt=""></p>
<h2 id="输入图像的大小">输入图像的大小</h2>
<p>为了使得目标占很大，或者是中心点在图的中间的目标在预测的适合更加准确，所以最好让输出的特征图的像素是个奇数，也就是说把图像隔成奇数个网格，这样就不会像偶数格一样把图像中心分成两部分了，大家可以想象一下3格和4格的区别，4格就把图的中心分割成一半了，如果目标中心点正好在中心区域的周围，那么偏移一点误差也很大了。</p>
<h2 id="其他一些加入的改进">其他一些加入的改进</h2>
<ol>
<li>加入了一个不同层特征的融合。concat连接</li>
<li>多尺度训练，Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: {320, 352, ..., 608}. Thus the smallest option is 320 × 320 and the largest is 608 × 608. We resize the network to that dimension and continue training.</li>
</ol>
<h2 id="网络">网络</h2>
<p><strong>主干网络使用了darknet-19，因为文章说vgg16虽然效果精度高，但是计算量大</strong>。darknet-19是使用1x1来减少计算量，就是bottleneck。网络如下：<br>
<img src="https://jzq6520.github.io/post-images/1572415768870.png" alt=""></p>
<p>网上的图：<br>
<img src="https://jzq6520.github.io/post-images/1572417862443.jpg" alt=""><br>
这是在imagenet上进行预训练的结构，训练检测的时候去掉最后一个卷积层，然后增加一个3x3的卷积层和一个1x1的卷积层。原文：<em>We modify this network for detection by removing the last convolutional layer and instead adding on three 3 × 3 convolutional layers with 1024 ﬁlters each followed by a ﬁnal 1 × 1 convolutional layer with the number of outputs we need for detection。</em></p>
<p>并且，还增加了一个从最后的3×3×512层到第二个到最后一个卷积层的透传层，这样我们的模型可以使用细粒度的特征。原文（看的不是很清晰）：We also add a passthrough layer from the ﬁnal 3 × 3 × 512 layer to the second to last convolutional layer so that our model can use ﬁne grain features.</p>
<p>anchor的个数也是通过实验选择出来的。最后选择了5性价比比较高，越多效果越好。<br>
<img src="https://jzq6520.github.io/post-images/1572416170928.png" alt=""></p>
<h3 id="网络的预测输出">网络的预测输出</h3>
<p>其实每个anchor都是预测：4个（坐标值）+1个（置信度）+ n个类别：<br>
<img src="https://jzq6520.github.io/post-images/1572419086667.png" alt=""></p>
<p>对比YOLO1的输出张量，YOLO2的主要变化就是会输出5个先验框，且每个先验框都会尝试预测一个对象。输出的 <code>13*13*5*25</code> 张量中，25维向量包含 20个对象的分类概率+4个边框坐标+1个边框置信度。</p>
<h2 id="损失函数">损失函数</h2>
<p>？？？？？？what？<br>
文章中居然没说用什么loss？？？， 难道用的是smooth L1，还是yolo中的loss？？<br>
<strong>应该是结合吧，既然预测宽和高用的是RPN的，预测中心点用的是yolo，预测有没有目标也是yolo的。那么应该就是结合一下了，宽高用smooth L1， 中心点用均方差，有没有目标（置信度）用均方差</strong>以上只是猜测，具体看代码。</p>
<p>重温yolo v2 - stone的文章 - 知乎https://zhuanlan.zhihu.com/p/40659490 中的图：<br>
<img src="https://jzq6520.github.io/post-images/1572420200314.jpg" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572420615339.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572420619180.png" alt=""></p>
<p><strong>卧槽 差点被坑了，文章中是把网络输出换算成x，y，w，h之后才进行计算loss的</strong>。<strong>不是直接回归t。RPN中是回归t的</strong>。</p>
<h2 id="训练参数学习率">训练参数，学习率</h2>
<ul>
<li>学习率：train the network for 160 epochs with a starting learning rate of 10−3 , dividing it by 10 at 60 and 90 epochs.</li>
<li>sgd：We use a weight decay of 0.0005 and momentum of 0.9.</li>
<li>数据增强：We use a similar data augmentation to YOLO and SSD with random crops, color shifting, etc</li>
</ul>
<h2 id="实验结果">实验结果</h2>
<p>分辨率越高效果越好：<br>
<img src="https://jzq6520.github.io/post-images/1572414966868.png" alt=""></p>
<figure data-type="image" tabindex="1"><img src="https://jzq6520.github.io/post-images/1572421391035.png" alt=""></figure>
<h2 id="引用">引用</h2>
<ul>
<li>YOLOv2 / YOLO9000 深入理解 - X-猪的文章 - 知乎https://zhuanlan.zhihu.com/p/47575929</li>
<li><strong>重温yolo v2</strong> - stone的文章 - 知乎https://zhuanlan.zhihu.com/p/40659490</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读YOLO目标检测系列 v1]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-yolo-mu-biao-jian-ce-xi-lie-v1</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-yolo-mu-biao-jian-ce-xi-lie-v1">
        </link>
        <updated>2019-10-29T07:46:45.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>将对象检测定义为一个<strong>回归问题</strong>，回归到空间分离的边界框和相关的类概率。</li>
<li>与最先进的检测系统相比，YOLO会产生更多的定位错误，但不太可能预测背景上的误报<em>less likely to predict false positives on background</em>（假阳少）</li>
<li>都看做一个回归问题，所以不需要复杂的pipeline。</li>
<li>titan x gpu实现每秒150帧。</li>
<li>yolo是看<strong>整张图片</strong>进行预测的，相对来说区域建议网络第二阶段是是看roi。</li>
<li>yolo的背景的假阳少，因为他可以看到更大的信息（现在Faster R-CNN的第一个RPN模块其实也是看整个上下文的，这里对比的只是Fast R-CNN）。原文：<em>Fast R-CNN, a top detection method， mistakes back-ground patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.</em></li>
</ul>
<h2 id="如何检测">如何检测</h2>
<ol>
<li>将图片分成SxS个格子；</li>
<li>如果目标落在哪个格子上，哪个格子就负责整个目标。简单的说就是做数据的时候，这个格子的class label是1，在yolo里面是使用置信度来计算的。</li>
</ol>
<p>有些博客不懂乱说，例如知乎上的一篇文章，上来就说最后一层的featuremap对应的就是前面分好的网格里面的格子的特征，可见对感受野的概念不是很理解，其实这里的想法和Faster R-CNN里面的概念比较像，anchor其实也是在原图上滑动的呀，只是说这里没有anchor，而是这里的featuremap上的像素对应的是一个格子，也不能说是格子，因为格子的作用和anchor也是非常类似的，<strong>作用都是来进行分配任务，教网络怎么预测，网络学的是什么，那么预测的也是什么了</strong>。只是说这里是根据中心点是不是落在网络里面来选择正样本，而RPN里面是根据和anchor的IOU的大小来规定哪个是正样本哪个是负样本。</p>
<p>只是这里说的比较明确，说是把原图分割成最后一层feature map大小的个数，feature map是多少个像素，那么就分成几个格子。</p>
<p>只是说<strong>Faster R-CNN里面好像没有明确提到anchor的中心点在原图的哪个地方</strong>。其实这也是我们规定哪个点就是哪个点，<strong>但是按照感受野的概念，感受野中心的区域的权重往往会高一点</strong>，所以anchor的中心点可以设置成格子的中心。所以anchor的中心点在原图的坐标经过换算一下也是很简单的。应该就是<code>(x*stride + stride/2, y*stride + stride/2)</code>,这里的x和y表示在feature map上的坐标，计算出来的表示原图上的坐标，这也是我自己推测的（<strong>代码还没看过</strong>）。</p>
<h2 id="定义label">定义label</h2>
<p>每个格子预测<strong>坐标（x,y,w,h）+ 置信度（是否有目标，有、没有）+ 类别（class）</strong>：<br>
* <strong>置信度=Pr(Object)+ IOU</strong>，这是论文里面说的，iou论文里面是说预测的和真实的。但是没有预测哪里来的iou，<strong>所以做数据的是置信度就是有目标是1，没有目标是0</strong>，不用多想这里的置信度就是有没有目标，就是0和1。<br>
* 坐标：宽和高是<strong>相对于图像</strong>大小的，中心点是<strong>相对于格子</strong>的的。<br>
* 类别：是哪个类别</p>
<p>置信度和坐标是绑定在一起的，类别是独立的。例如一个格子检测两个目标，那么就有两组坐标和对应的置信度，但是类别可以是有20个。<em>但是这样有个问题，就是如果两个框都是有目标，那么怎么知道哪个框是哪个类别的目标呢？</em> 是不是这里的意思就是一个格子就只能预测一个类别，但是可以有两个包围框预测，最后应该是看哪个包围框更准（置信度高，即是目标的概率高）就选哪个？确实和我想的一样，<strong>两个包围框都是预测一个类别的（见引用）</strong>。如图所示：<br>
<img src="https://jzq6520.github.io/post-images/1572353048785.png" alt=""></p>
<p>Fast R-CNN最后只是有一个类别和坐标，但是他的类别里面是包含背景的，yolo的类别里面是不包含背景。</p>
<p>下面这幅图终于看懂了，Class probability map确实只是class，但这里只是表示这个格子预测的目标是哪个类（就是目标中心对于的框的目标），下面这个图片的Class probability map画的应该是预测出来的，那么做数据的时候是怎么做的。。。如果没有目标那么类别是什么？即使不计算loss那也总要填一个值吧。这里好像又没有背景这个类别。<br>
这个应该是预测时候的示意图。<br>
<img src="https://jzq6520.github.io/post-images/1572338771015.png" alt=""></p>
<h2 id="训练的时候">训练的时候</h2>
<ul>
<li>训练的时候只预测一个目标，选择iou高的。原文：<em>YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth.</em></li>
</ul>
<h2 id="损失函数">损失函数</h2>
<p>通常没有目标的区域比较多，而有目标的区域少，所以<strong>坐标的回归</strong>在loss里面占的比重很少，所以为了<strong>减少这种不平衡</strong>，将<strong>非目标的置信度回归的权重减少</strong>。<br>
<img src="https://jzq6520.github.io/post-images/1572341526648.png" alt=""></p>
<figure data-type="image" tabindex="1"><img src="https://jzq6520.github.io/post-images/1572341047772.png" alt=""></figure>
<p>这个长的像1的系数表示有目标的适合是1，没有的适合是0。那头上写的是noobj的时候就反一下。这里大写的C表示confidence置信度。p那个表示类别。计算不是目标的置信度的适合权重设置为0.5 。</p>
<p>根号的作用：<br>
<img src="https://jzq6520.github.io/post-images/1572352978309.png" alt=""></p>
<h2 id="缺点">缺点</h2>
<ul>
<li>不能同时检测两个中心点在同一个网格的物体。</li>
<li>对于不常见的长宽比的物体效果较差</li>
<li>对于多尺度效果较差</li>
</ul>
<h2 id="引用">引用</h2>
<ul>
<li>图解YOLO - 晓雷的文章 - 知乎 https://zhuanlan.zhihu.com/p/24916786</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读Fast R-CNN的ROI pooling]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-fast-r-cnn-ji-roi-pooling</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-fast-r-cnn-ji-roi-pooling">
        </link>
        <updated>2019-10-29T03:59:07.000Z</updated>
        <content type="html"><![CDATA[<p>Fast R-CNN主要是使用了一个ROI Pooling操作来对候选区域进行分类和包围框的微调。<br>
这里主要说明一下ROI pooling是怎么做的，主要包括：</p>
<ol>
<li>原图上的ROI坐标如何映射到feature map上？</li>
<li>ROI Pooling是如何做？</li>
<li>ROI Pooling的梯度反向传播是怎么做的？</li>
<li>那么多的ROI Pooling做完以后是怎么进入到全连接层进行训练的？</li>
<li>正负样本怎么制作？</li>
<li>loss怎么计算？</li>
<li>回归的数值是什么？</li>
</ol>
<p>以上就是我这篇文章看下来的疑问或文章中的重点，然后查找资料和阅读相关论文来寻找答案。<br>
下面一个个来说明。</p>
<h2 id="1-原图上的roi坐标如何映射到feature-map上">1 原图上的ROI坐标如何映射到feature map上？</h2>
<p>这个问题相对简单，可以这样理解，最后一层的feature map是网络多次下采样得到的，所以只要对原图上的坐标进行相同倍数的缩放即可。vgg16最后一个特征图是经过4次下采样，即缩小16倍得到的，所以坐标<code>x' = [x/下采样倍数] + 1</code> 中括号表示向下取整，vgg16的下采样倍数就是16。这里为什么要+1还不清楚。</p>
<h2 id="2-roi-pooling是如何做">2 ROI Pooling是如何做？</h2>
<p>因为物体有大有小，所以物体的ROI映射到特征图上以后也是有大有小 尺寸不一样，所以如果想输入到全连接层进行分类，那么就必须要同样的神经元个数，因为全连接的输入大小是固定的。</p>
<p>所以这里使用了一个max pooling方式将任意大小的roi对应的featuremap（这里的对应的roi featuremap只是最后输出的featuremap中切下的一小块）弄到一个尺度，比如文章中就是都弄到了7x7 。</p>
<p>其实很简单，就是将roi feature map隔成7x7的格子，然后每个格子里取最大的数（max pooling) 。如下图所示，图中的黑框就是原图中的roi在featuremap中对应的特征区域：<br>
<img src="https://jzq6520.github.io/post-images/1572328585352.png" alt=""></p>
<h2 id="3-roi-pooling的梯度反向传播是怎么做的">3 ROI Pooling的梯度反向传播是怎么做的？</h2>
<p>这个梯度计算在文章中的公式写的很难看懂，但是看了其他人的<a href="https://zhuanlan.zhihu.com/p/30368989">博客</a>解释就比较清晰了，其实还是很简单的。</p>
<p>首先，了解一下max pooling是怎么反向传播的，其实就是max的区域有梯度，为1。<br>
<img src="https://jzq6520.github.io/post-images/1572328987720.jpg" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572329086778.png" alt=""><br>
有了上面的了解，再来看ROI pooling，其中不一样的是<strong>特征图中的一个像素</strong>可能是多个感兴趣区域的重合部分。例如：<br>
<img src="https://jzq6520.github.io/post-images/1572329164506.jpg" alt=""></p>
<p>那么这个个像素的位置的梯度就累积就可以了，比如假设3个roi pooling都用到了这个像素，那么梯度就是3了。</p>
<h2 id="4-那么多的roi-pooling做完以后是怎么进入到全连接层进行训练的">4 那么多的ROI Pooling做完以后是怎么进入到全连接层进行训练的？</h2>
<p>这一点细节其实文章中没提到，但是我想文章中是说到了每次选择2张图片，然后每张图片中选择64个roi进行训练，那么可以看出最后预测的全连接层的batch size是128，其实可以把roi pooling去掉，那么后面的网络其实就是一个输入一个7x7个神经元个数的全连接神经网络，batchsize为128。</p>
<p>所以这里是<strong>对选择的128个区域做ROI Pooling，然后把128个特征作为batchsize的形式传到网络中训练的</strong>。可以看下图：<br>
<img src="https://jzq6520.github.io/post-images/1572329514254.png" alt=""></p>
<h2 id="5-正负样本怎么制作">5 正负样本怎么制作？</h2>
<p>说到训练肯定是要去看怎么做数据，和选择正负样本的。</p>
<ul>
<li>正样本：IOU 大于等于0.5；</li>
<li>负样本：IOU在<code>[0.1, 0.5)</code>区间。</li>
</ul>
<p>正样本25%左右。</p>
<h2 id="6-loss怎么计算">6 loss怎么计算？</h2>
<p>loss也包括两部，分类+坐标回归。</p>
<ul>
<li>分类采用log loss，也就是交叉熵；</li>
<li>坐标回归采用smooth L1，当回归的数值非常大的时候，如果用L2loss就要非常小心，要防止梯度爆炸，因为抛物线两边是梯度越来越大的，L1是不变的。</li>
</ul>
<h3 id="smooth-l1">smooth L1</h3>
<p><img src="https://jzq6520.github.io/post-images/1572330028401.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572330081867.png" alt=""></p>
<h2 id="7回归的数值是什么">7回归的数值是什么？</h2>
<p>现在检测的话都是回归一个相对坐标。<br>
这里的话<strong>参照边框</strong>就是<strong>proposal的框</strong>了，而不是anchor的框。</p>
<p>回归的label是这么制作的：<br>
<img src="https://jzq6520.github.io/post-images/1572330297928.png" alt=""></p>
<p><strong>我们实际回归的就是这个t，然后再反过来求一下坐标和宽高。R-CNN论文里面写的有点复杂，其实就是回归一t就对了</strong>。</p>
<h2 id="引用">引用</h2>
<ol>
<li>roi pooling反向传播和roi在featuremap上的位置：https://zhuanlan.zhihu.com/p/30368989</li>
<li>roi pooling的操作细节：https://deepsense.ai/region-of-interest-pooling-explained/</li>
<li>R-CNN</li>
</ol>
]]></content>
    </entry>
</feed>