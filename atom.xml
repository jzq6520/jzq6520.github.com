<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jzq6520.github.io</id>
    <title>chuck</title>
    <updated>2019-10-29T07:40:32.296Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jzq6520.github.io"/>
    <link rel="self" href="https://jzq6520.github.io/atom.xml"/>
    <subtitle>天地不仁以万物为刍狗</subtitle>
    <logo>https://jzq6520.github.io/images/avatar.png</logo>
    <icon>https://jzq6520.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, chuck</rights>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读Fast R-CNN的ROI pooling]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-fast-r-cnn-ji-roi-pooling</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-fast-r-cnn-ji-roi-pooling">
        </link>
        <updated>2019-10-29T03:59:07.000Z</updated>
        <content type="html"><![CDATA[<p>Fast R-CNN主要是使用了一个ROI Pooling操作来对候选区域进行分类和包围框的微调。
这里主要说明一下ROI pooling是怎么做的，主要包括：</p>
<ol>
<li>原图上的ROI坐标如何映射到feature map上？</li>
<li>ROI Pooling是如何做？</li>
<li>ROI Pooling的梯度反向传播是怎么做的？</li>
<li>那么多的ROI Pooling做完以后是怎么进入到全连接层进行训练的？</li>
<li>正负样本怎么制作？</li>
<li>loss怎么计算？</li>
<li>回归的数值是什么？</li>
</ol>
<p>以上就是我这篇文章看下来的疑问或文章中的重点，然后查找资料和阅读相关论文来寻找答案。
下面一个个来说明。</p>
<h2 id="1-原图上的roi坐标如何映射到feature-map上">1 原图上的ROI坐标如何映射到feature map上？</h2>
<p>这个问题相对简单，可以这样理解，最后一层的feature map是网络多次下采样得到的，所以只要对原图上的坐标进行相同倍数的缩放即可。vgg16最后一个特征图是经过4次下采样，即缩小16倍得到的，所以坐标<code>x' = [x/下采样倍数] + 1</code> 中括号表示向下取整，vgg16的下采样倍数就是16。这里为什么要+1还不清楚。</p>
<h2 id="2-roi-pooling是如何做">2 ROI Pooling是如何做？</h2>
<p>因为物体有大有小，所以物体的ROI映射到特征图上以后也是有大有小 尺寸不一样，所以如果想输入到全连接层进行分类，那么就必须要同样的神经元个数，因为全连接的输入大小是固定的。</p>
<p>所以这里使用了一个max pooling方式将任意大小的roi对应的featuremap（这里的对应的roi featuremap只是最后输出的featuremap中切下的一小块）弄到一个尺度，比如文章中就是都弄到了7x7 。</p>
<p>其实很简单，就是将roi feature map隔成7x7的格子，然后每个格子里取最大的数（max pooling) 。如下图所示，图中的黑框就是原图中的roi在featuremap中对应的特征区域：
<img src="https://jzq6520.github.io/post-images/1572328585352.png" alt=""></p>
<h2 id="3-roi-pooling的梯度反向传播是怎么做的">3 ROI Pooling的梯度反向传播是怎么做的？</h2>
<p>这个梯度计算在文章中的公式写的很难看懂，但是看了其他人的<a href="https://zhuanlan.zhihu.com/p/30368989">博客</a>解释就比较清晰了，其实还是很简单的。</p>
<p>首先，了解一下max pooling是怎么反向传播的，其实就是max的区域有梯度，为1。
<img src="https://jzq6520.github.io/post-images/1572328987720.jpg" alt="">
<img src="https://jzq6520.github.io/post-images/1572329086778.png" alt="">
有了上面的了解，再来看ROI pooling，其中不一样的是<strong>特征图中的一个像素</strong>可能是多个感兴趣区域的重合部分。例如：
<img src="https://jzq6520.github.io/post-images/1572329164506.jpg" alt=""></p>
<p>那么这个个像素的位置的梯度就累积就可以了，比如假设3个roi pooling都用到了这个像素，那么梯度就是3了。</p>
<h2 id="4-那么多的roi-pooling做完以后是怎么进入到全连接层进行训练的">4 那么多的ROI Pooling做完以后是怎么进入到全连接层进行训练的？</h2>
<p>这一点细节其实文章中没提到，但是我想文章中是说到了每次选择2张图片，然后每张图片中选择64个roi进行训练，那么可以看出最后预测的全连接层的batch size是128，其实可以把roi pooling去掉，那么后面的网络其实就是一个输入一个7x7个神经元个数的全连接神经网络，batchsize为128。</p>
<p>所以这里是<strong>对选择的128个区域做ROI Pooling，然后把128个特征作为batchsize的形式传到网络中训练的</strong>。可以看下图：
<img src="https://jzq6520.github.io/post-images/1572329514254.png" alt=""></p>
<h2 id="5-正负样本怎么制作">5 正负样本怎么制作？</h2>
<p>说到训练肯定是要去看怎么做数据，和选择正负样本的。</p>
<ul>
<li>正样本：IOU 大于等于0.5；</li>
<li>负样本：IOU在<code>[0.1, 0.5)</code>区间。</li>
</ul>
<p>正样本25%左右。</p>
<h2 id="6-loss怎么计算">6 loss怎么计算？</h2>
<p>loss也包括两部，分类+坐标回归。</p>
<ul>
<li>分类采用log loss，也就是交叉熵；</li>
<li>坐标回归采用smooth L1，当回归的数值非常大的时候，如果用L2loss就要非常小心，要防止梯度爆炸，因为抛物线两边是梯度越来越大的，L1是不变的。</li>
</ul>
<h3 id="smooth-l1">smooth L1</h3>
<p><img src="https://jzq6520.github.io/post-images/1572330028401.png" alt="">
<img src="https://jzq6520.github.io/post-images/1572330081867.png" alt=""></p>
<h2 id="7回归的数值是什么">7回归的数值是什么？</h2>
<p>现在检测的话都是回归一个相对坐标。
这里的话<strong>参照边框</strong>就是<strong>proposal的框</strong>了，而不是anchor的框。</p>
<p>回归的label是这么制作的：
<img src="https://jzq6520.github.io/post-images/1572330297928.png" alt=""></p>
<p><strong>我们实际回归的就是这个t，然后再反过来求一下坐标和宽高。R-CNN论文里面写的有点复杂，其实就是回归一t就对了</strong>。</p>
<h2 id="引用">引用</h2>
<ol>
<li>roi pooling反向传播和roi在featuremap上的位置：https://zhuanlan.zhihu.com/p/30368989</li>
<li>roi pooling的操作细节：https://deepsense.ai/region-of-interest-pooling-explained/</li>
<li>R-CNN</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读Faster R-CNN即region proposal network(RPN)区域建议网络]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-faster-r-cnn</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-faster-r-cnn">
        </link>
        <updated>2019-10-28T02:35:49.000Z</updated>
        <content type="html"><![CDATA[<p>Faster R-CNN主要是讲区域建议网络，而ROI pooling部分还是在Fast R-CNN里面介绍的。</p>
<h2 id="rpn-region-proposal-network">RPN region proposal network</h2>
<ul>
<li>利用网络产生region proposal（区域建立，候选区域），在网络的最后一层增加几个卷积层在一个<strong>固定的网格</strong>中<strong>回归包围框和目标分数</strong>来构建RPN。</li>
<li>交替训练region proposal和object detection。region proposal和object detection的基础特征是共享的，所以效率非常高。</li>
<li>RPN使用了anchor，所以不需要利用图像金字塔或者特征金字塔来进行多尺度的目标检测，使用了anchor以后只需要在一张特征图上进行检测出不同大小的目标。如果使用图像金字塔就需要缩放，利用特征金字塔就需要在不同尺度的特征图上检测对应大小的目标（FPN其实也证明了特征金字塔是有效的，但是一直特征图上检测多个尺度的目标还是很必要的），如下图c所示就是anchor机制，图a是图像金字塔，b是特征金字塔处理的效果。</li>
<li><img src="https://jzq6520.github.io/post-images/1572231445375.png" alt=""></li>
<li>RPN的<strong>输入</strong>是任意形状的图像，<strong>输出</strong>是一个长方形的目标建议区域和目标置信度。从这里可以看到其实<strong>单单一个RPN就可以看做是一个完整的end-to-end网络</strong>了。只是说后面的二次分类是共用之前backbone的特征图的。</li>
<li>文章中的 top一般是指网络的深层。down是指浅层。计算感受野从输入到输出的计算公式是 ：
<ul>
<li>rf = 上一层rf + (kernel size - 1)* stride * dilated rate， 这个是递推法，这里的 stride是之前的所以stride累乘得到。</li>
<li>这里有个重点就是有stride的时候下一次才会乘stride，例如做max pooling的时候是不立即计算stride的，而是下一步加上stride计算。</li>
<li>还有个是stride是累乘的。</li>
<li>参考链接：
<ul>
<li>http://zike.io/posts/calculate-receptive-field-for-vgg-16/</li>
<li>https://blog.csdn.net/Fanniexia/article/details/102512267</li>
</ul>
</li>
</ul>
</li>
<li>为了产生region proposal，用一个<strong>小的网络进行卷积</strong>，文章中使用了<strong>一个3x3的卷积层作为小的网络</strong>如下图所示，然后文章中计算出来做了3x3的卷积以后的感受野是228，因为vgg16最后一个特征图（不算最后的maxpooling）的感受野是196，根据上面给出的公式，那最后<strong>region proposal的感受野就是</strong> <code>196+(3-1)*16=228</code> 。这里的16是之前做了4次pooling，stride累乘就是16。
<img src="https://jzq6520.github.io/post-images/1572242265100.png" alt=""></li>
</ul>
<h3 id="anchor">anchor</h3>
<ul>
<li><strong>anchor</strong>
<ul>
<li>其实这一块<strong>不用想的特别复杂</strong>，他虽然画了一个anchor再上面，这里<strong>画出来</strong>只是表示最终anchor的特征是在这里提取的，<strong>重点不是在这个anchor上</strong>，anchor只是一个虚拟的大小的宽，用来给坐标回归提供一个相对位置的，或者<strong>说是一个参考的坐标系（类似于这个作用）</strong>，但是其实做数据还是按照规定在固定的网格中做数据就可以了，对应的<strong>坐标和置信度，主要的还是这两个分支</strong>。</li>
<li>anchor的个数限制了feature map上每个点可以预测几个region proposal。这个其实就像平时分类一样，我可以最后输出4个神经元来预测四个类别，那我也可以输出8个神经元来预测8个类别。一个道理，这里假设规定了k个anchor，那么我们就预测k个框就好了。</li>
<li>这里的feature map上的点可以怎么理解呢，就是说每个feature map的点是预测k组（这里的组是指坐标+置信度）目标，就像<strong>平时我们是把特征图拉直或者是做过global avg pooling，因为分类是最终分出几个类别，是根据整体的信息来的</strong> ，这里只是不再进行降维了，而是每个点我出来预测一个东西。这样其实有个好处就是，<strong>特征图上的每个点对应的感受野其实是原图上的一块区域的</strong>，而且通常来说感受野是<strong>输入中越靠感受野中间的元素对特征的贡献越大</strong>的，如下图所示，出自论文understanding-the-effective-receptive-field-in-deep-convolutional-neural-networks。所以这样来预测其实是符合计算的。
<img src="https://jzq6520.github.io/post-images/1572243550140.png" alt=""></li>
<li>我靠之前怎么想不到，<strong>这个其实就是和分割差不多啊</strong>，分割的输出也是不进行降维的，也是<strong>一个像素其实对应一块区域的感受野的</strong>。</li>
</ul>
</li>
<li>文章中老是说滑动窗口（sliding windows on the feature map）<strong>太令人误会</strong>了，其实就是<strong>加了一个卷积</strong>层然后再进行预测宽和置信度。</li>
<li>The design of multiscale anchors is a key component for sharing features <strong>without extra cost for addressing scales</strong>.</li>
</ul>
<h3 id="参数量">参数量</h3>
<ul>
<li>region proposal小网络（文章中说小网络，其<strong>实就是1个分支，也就是1个卷积层</strong>）提到了Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and <strong>512-d</strong> for VGG, with ReLU [33] following).和our output layer has 2.8 × 10 4 parameters (<strong>512</strong> × (4 + 2) × 9 for VGG-16)，这里的<strong>512是指这个分支卷积层的channel数</strong>，就像隐藏层的参数个数一样，他这边的参数量应该不是所有的feature map上的点，因为输入图像大小不一样，featuremap大小也是不一样的。所有这里说的参数是featuremap上<strong>一个点的region proposal的参数量</strong>，所有文章又补充说明了一下，如下图：
<img src="https://jzq6520.github.io/post-images/1572244888520.png" alt=""></li>
</ul>
<h2 id="正负样本选择">正负样本选择</h2>
<p>二分类，是目标或者不是目标。<strong>每个anchor都有一个坐标和一个类别</strong>。</p>
<p><strong>正样本选择</strong>，有两种anchor都是算作正样本：</p>
<ol>
<li>和ground truth的 IOU最高的anchor；</li>
<li>和ground truth的 IOU大于0.7的anchor；</li>
</ol>
<p>注：<strong>通常，第二个条件足以确定阳性样本;但我们仍然采用第一种情况，因为在少数情况下，第二种情况可能找不到阳性样本</strong>。</p>
<p>负样本选择：</p>
<ol>
<li>和ground truth的 IOU小于0.3的算负样本的anchor。</li>
</ol>
<p>这样哪个anchor是正样本和负样本分配好了就方便了，这样数据做出来就是每个anchor的分类的label就有了，1或者0 。</p>
<h2 id="rpn的损失函数">RPN的损失函数</h2>
<p>损失函数如下图所示：
<img src="https://jzq6520.github.io/post-images/1572256564654.png" alt=""></p>
<p>可以看出又两个损失函数组成，一个是分类loss，另外一个是坐标回归的loss：</p>
<ul>
<li>带星号的表示ground truth,即label，1表示正样本，0表示负样本。</li>
<li><strong>分类</strong>为log loss，也就是交叉熵。</li>
<li><strong>回归</strong>为smooth L1，从公式可以看出乘以一个p星，说明只计算正样本的损失，负样本是不计算损失的。</li>
<li>乘以一个N分之一，这个是做一个归一化，分类的N为batch-size的大小，回归的N是batch-size乘以anchor数（即所有anchor的数量），这里<em>可以看到回归是除以了所以anchor的数量，而分类的只除以batch-size</em>。然后这里的lambda参数又是来平衡两个loss，这里论文中设置了一个10效果最好，<strong>那么傻逼了</strong>，因为文章anchor是9，那么乘10再除9 来平衡，tm的为什么不直接除batch-size，干嘛还乘一个anchor个数来平衡。</li>
<li>坐标回归的loss：
<img src="https://jzq6520.github.io/post-images/1572271188684.png" alt="">
<strong>可以看出这里的label是经过转换得到的</strong>，所以预测出来的结果还需要经过变换，转换到原来的坐标和长宽。可以把回归看做是和最接近的anchor计算相对位置的，因为做label（正负样本）的时候就是选择IOU(接近)大的anchor的。</li>
</ul>
<h2 id="训练rpn">训练RPN</h2>
<ul>
<li>loss: SGD</li>
<li>batch size=256,由于单个图像会存在非常多的正样本和负样本anchor，但是负样本总是会比正样本多，所以为了防止偏向于负样本，所以每次选择256个anchor进行计算loss作为mini-batch，这样让正负样本接近1：1，如果正样本少于128，则用负样本填充。</li>
<li>backbone使用迁移ImageNet预训练参数，其他层使用高斯分布随机初始化。</li>
<li>初始学习率0.001训练60k次，然后0.0001训练20k次。</li>
<li>momentum = 0.9 ， weight decay = 0.0005</li>
<li>交替训练RPN网络和Fast R-CNN是交替训练的。原文：<em>Alternating training. In this solution, we ﬁrst train RPN, and use the proposals to train Fast R-CNN. The network tuned by Fast R-CNN is then used to initialize RPN, and this process is iterated. This is the solution that is used in all experiments in this paper.</em></li>
</ul>
<h2 id="实现细节">实现细节</h2>
<ul>
<li>Multi-scale feature extraction (using an image pyramid) may improve accuracy but does not exhibit a good speed-accuracy trade-off</li>
<li>total stride for both ZF and VGG nets on the last convolutional layer is 16 pixels，Even such a large stride provides good results, though accuracy may be further improved with a smaller stride.</li>
<li>For anchors, we use 3 scales with box areas of 128 2 , 256 2 , and 512 2 pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1.</li>
<li>We note that our algorithm allows predictions that are larger than the underlying receptive ﬁeld. Such predictions are not impossible—one may still roughly infer the extent of an object if only the middle of the object is visible.看见局部也可能认识整个物体，但是边缘怎么解释呢，rpn怎么知道物体多大，所以其实感受野还是大点好。</li>
<li>训练的适合超过边框的anchor不计算loss，什么叫超过边框呢？因为anchor是以最后一层特征图映射回到原图的点作为中心点的，所以边框是有可能超过图像的。这个以前困扰我很多，<strong>因为很多博客都画了超过图像大小的anchor，所以很困扰，其实那样画是有问题的</strong>，明明是不计算loss的，所以就不用设计无论怎么样都会超过图像的边框的anchor。测试的适合预测出来这种，就阶段到边缘就可以了，不是丢掉。</li>
<li>一些RPN建议彼此高度重叠。为了减少冗余，我们根据建议区域的cls评分，对其采用非最大抑制(non-maximum suppression, NMS)。我们将NMS的IoU阈值设为0.7，这样每个图像就有<strong>大约2000个</strong>建议区域。</li>
<li>在NMS之后，我们使用排名前n的建议区域进行检测。下面，我们使用2000个RPN建议来训练Fast R-CNN，但是在测试时评估不同数量的建议。</li>
</ul>
<h2 id="整体的网络结构就是这样">整体的网络结构就是这样</h2>
<p>ROI Pooling结构可以仔细看Fast R-CNN，这篇文章没有仔细介绍。</p>
<p><img src="https://jzq6520.github.io/post-images/1572275302379.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-数据增强-Learning to Generate Synthetic Data via Compositing]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-shu-ju-zeng-qiang-learning-to-generate-synthetic-data-via-compositing</id>
        <link href="https://jzq6520.github.io/post/cv-paper-shu-ju-zeng-qiang-learning-to-generate-synthetic-data-via-compositing">
        </link>
        <updated>2019-10-24T08:43:36.000Z</updated>
        <content type="html"><![CDATA[<p>20191024，今天是程序员日，天气不错。</p>
<p>这是一篇做数据增强的文章。</p>
<h2 id="1-简介">1 简介</h2>
<p>传统的图像增强方法一般是对图像做一些转换，另外一种较新的方法就是合成数据。</p>
<p>主要思想是合成一些图片，加入到训练中，从而提高检测和分类的精度。这里的合成是指将目标和背景结合。例如我把另外一张图片里面的人抠出来，放到另外一个背景图片里面去，就好像本来是在室内的人，抠出来放到一个室外的照片里面去。这样做的好处是增加了图片的多样性。增加一个上下文的多样性。</p>
<p>例如，我们真实世界里面的人是在什么地方都会出现的，马路上、车里、草坪上、床上，都会有，但是我们作为训练集的图像中可能只出现了一个马路上的人的照片，那么当测试集中出现了一个人在室内的照片，可能模型就不认识。</p>
<p>由于我们的数据只是真实世界分布的一个采样，所以数据永远是不够了。这就涉及到了泛化能力，一个好的模型就具有更好的泛化能力，但是有时候数据不够往往是一些其他的网络上的技巧（dropout这种）弥补不了的。</p>
<p>所以这篇文章做的就是生成一些样本来弥补数据过少，数据多样性小的问题。这篇文章的主要一点是提出了我们生成的样本要<strong>符合真实世界的分布</strong>，例如一个人不可能就是没有借助的飞在天上。所以文章的框架增加了一个<strong>判别器</strong>来判断生成的数据是不是符合真实世界的分布，也就是说是不是符合常理的数据(真不真实)。这样来大家也都可以看出，这其实就是一个GAN网络。</p>
<p>这篇文章是基于<strong>Cut-Paste-Learn</strong>是基于这篇文章之上的一篇文章。主要就是加了一个判别器来判断生成的数据是否是符合真实世界的分布。<strong>当然因为是通过GAN训练，那么生成器生成的就是以假乱真的数据了，而不是有监督的学习</strong>。因为作者说，如果引入了一些不符合真实场景的数据那么会造成<strong>过拟合</strong>。简单的理解就是这部分不符合常理的数据其实就是噪声，大家都知道噪声是会给模型带来负担的。</p>
<p>一般来说生成数据的方式主要有三种：1)图像合成，2)对抗生成，3)渲染。本文应该是结合了图像合成和对抗这两种方法。</p>
<h2 id="2-数据合成">2 数据合成</h2>
<h3 id="21-合成网络synthesizer-network">2.1 合成网络Synthesizer Network</h3>
<p>文章中的核心思想就是这个网络。里面还有一个小技巧来<strong>避免过拟合这种合成的数据</strong>。</p>
<p>首先，合成网络<strong>不是</strong>我们想的那样输入两个图形然后自动输出一个图片，其实这个网络是输入两张图片然后预测一个2D affine transformations（二维刚性变换）的矩阵，这个矩阵是一个<strong>维度为6的向量</strong>，如下图所示（文章中没给，见Spatial Transformer Networks论文）：
<img src="https://jzq6520.github.io/post-images/1571909222916.png" alt="">
<img src="https://jzq6520.github.io/post-images/1571909211491.png" alt=""></p>
<p>这个网络结构如下：
<img src="https://jzq6520.github.io/post-images/1571910057977.png" alt=""></p>
<p>感觉文章的图片画的不是很清晰，且参数共享那一块容易给人造成误解，所以重新画了一幅图，网络有部分是参数共享，但是又有部分是不进行参数共享的，结构和孪生网络相似。最终预测的就是一个变换矩阵的参数。具体设置如下：
<img src="https://jzq6520.github.io/post-images/1571910821669.png" alt=""></p>
<h3 id="22-合成数据">2.2 合成数据</h3>
<p>那么怎么将前景和背景合为一体呢呢，其实和Cut-Paste-Learn一样，把背景扣掉一块然后将前景放进去，其实<strong>说白了就是把前景贴到背景上</strong>，但是文章中给了一个<em>故弄玄虚的公式</em>说是叫做alpha-blending，其实就是贴图的方式。<strong>但是多了对图像做一个仿射变换以后叠加</strong>，再加上有一个判别网络来保证合成的网络是真实的。</p>
<p>如下图所示，上一行就是不真实（不符合真实世界的分布）的数据，下一行就是合成的符合真实世界的分布的数据：
<img src="https://jzq6520.github.io/post-images/1571911885967.png" alt=""></p>
<h3 id="23-防止过拟合贴图数据">2.3 防止过拟合贴图数据</h3>
<p>为了防止过拟合这些合成的数据，本身将一些和目标形状一样的背景图像贴到一些训练的图像中去（其实就是目标的mask乘以一些背景区域，这样就取出和目标一样形状的背景图片，然后贴到其他图片中去）。如图所示：
<img src="https://jzq6520.github.io/post-images/1571910704686.png" alt=""></p>
<h2 id="3-loss">3 loss</h2>
<p>判别网络loss，就是个交叉熵：
<img src="https://jzq6520.github.io/post-images/1571910918644.png" alt=""></p>
<p>GAN的 loss：
<img src="https://jzq6520.github.io/post-images/1571910951399.png" alt=""></p>
<p>生成仿射变换的6维向量的适合是没有监督的，是通过对抗学习来学习合成网络的参数的。</p>
<h2 id="4-result">4 result</h2>
<p>只贴了少部分实验数据，具体可以看论文。
<img src="https://jzq6520.github.io/post-images/1571911142039.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571911157597.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571911183410.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571911212706.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571911278459.png" alt=""></p>
<h2 id="引用">引用</h2>
<ul>
<li>Learning to Generate Synthetic Data via Compositing</li>
<li>Cut, paste and learn: Surprisingly easy synthesis for instance detection.</li>
<li>Spatial Transformer Networks</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-增量学习-Large Scale Incremental Learning]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-zeng-liang-xue-xi-large-scale-incremental-learning</id>
        <link href="https://jzq6520.github.io/post/cv-paper-zeng-liang-xue-xi-large-scale-incremental-learning">
        </link>
        <updated>2019-10-21T09:54:46.000Z</updated>
        <content type="html"><![CDATA[<p>就简单的说明一下好了，首先是使用蒸馏学习，然后再利用验证集来学习一个简单的线性变换 ax + b 来减少偏差。</p>
<p>这里是把验证集也拿过来训练了，虽然只是学习一个简单的线性变换，因为这个线性变换只有两个参数，所以需要的数据量非常少，虽然这个变换很简单，但是非常有效的提高精度。</p>
<p>文章中说的偏差指的是增量学习的精度和全部数据一起训练的精度的差距。</p>
<h2 id="1-什么是偏差">1 什么是偏差</h2>
<p>偏差就是增量学习的精度和用全部数据训练的精度的差。
<img src="https://jzq6520.github.io/post-images/1571711000642.png" alt=""></p>
<h2 id="2-网络">2 网络</h2>
<p><img src="https://jzq6520.github.io/post-images/1571710167211.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571710183774.png" alt=""></p>
<p>这里蒸馏学习的一个loss是logit之间的loss（即没有softmax激活的）。</p>
<p>结合蒸馏学习和普通分类loss来进行学习。</p>
<h2 id="3-loss">3 loss</h2>
<p>蒸馏学习的时候，新数据和老数据的蒸馏loss都是要计算的，无论是新的类别还是老的类别，这里其实是计算一个特征提取器的loss，让新的特征提取器提取的特征和老的一样。</p>
<p><img src="https://jzq6520.github.io/post-images/1571710427789.png" alt=""></p>
<h2 id="4-偏差矫正层">4 偏差矫正层</h2>
<p><img src="https://jzq6520.github.io/post-images/1571710889389.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-增量学习incremental learning]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-zeng-liang-xue-xi-incremental-learning</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-zeng-liang-xue-xi-incremental-learning">
        </link>
        <updated>2019-10-21T09:49:24.000Z</updated>
        <content type="html"><![CDATA[<p>又是一种深度学习的学习策略。</p>
<p>自然学习（Natural learning）系统本质上是<strong>渐进的</strong>，新知识是随着时间的推移而不断学习的，而现有的知识是保持不变的。现实世界中的许多计算机视觉应用程序都需要<strong>增量学习</strong>能力。例如，人脸识别系统应该能够在不忘记已学过的面孔的情况下添加新面孔。然而，大多数深度学习方法都存在灾难性的遗忘——<strong>当无法获得过去的数据时</strong>，性能会显著下降。</p>
<p><strong>旧类数据的缺失带来了两个挑战</strong>:(a)维护旧类的分类性能; (b)平衡旧类和新类。<strong>知识蒸馏</strong>已被用来有效地解决前一个挑战。最近的研究也表明，从旧的类中<strong>选择几个样本(抽样)</strong> 可以缓解不平衡问题。这些方法在小数据集上运行良好。然而，<strong>当类的数量变得很大</strong>(例如，成千上万个类)时，它们的性能会显著下降。图1显示了以非增量分类器为参考的这些最先进算法的性能退化情况。当类的数量从100增加到1000时，iCaRL和EEIL都有更多的精度下降。</p>
<p>为什么处理大量的类进行增量学习更具挑战性? 我们认为这是由于两个因素的耦合。首先，<strong>训练数据不平衡</strong>。其次，随着类数量的增加，在不同的增量步骤中更<strong>可能出现类似的类</strong>(例如ImageNet中的多个dog类)。在数据不平衡的增量约束下，视觉上相似的类数量的增加尤其具有挑战性，因为类之间边界的<strong>小边界</strong>对数据不平衡过于敏感。边界被推到支持具有更多示例的类。</p>
<h2 id="引用">引用</h2>
<ul>
<li>Large Scale Incremental Learning</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-主动学习-Learning Loss for Active Learning]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-zhu-dong-xue-xi-learning-loss-for-active-learning</id>
        <link href="https://jzq6520.github.io/post/cv-paper-zhu-dong-xue-xi-learning-loss-for-active-learning">
        </link>
        <updated>2019-10-17T09:36:24.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-简介">1 简介</h2>
<p>主动学习，即在拥有少部分监督数据的情况下，主动的去选择一部分对训练模型<strong>有较大提高的</strong>未标注数据，然后对选择出来的数据进行标注，标注后加入到训练集进行训练。为什么这么做的，我们把数据都标注一下不行吗？当然可以，但是标注是要时间和金钱的，我们希望选择更有助于模型提高的数据先进行标注。如下图所示：
<img src="https://jzq6520.github.io/post-images/1571305200850.png" alt="主动学习流程图"></p>
<p>以往的研究结果表明，主动学习实际上降低了标注成本。最开始提出主动学习的方法是选择出不确信的数据进行标注。主动学习的核心思想是信息量最大的数据比随机选择的数据更有利于模型的改进。无论任务是什么、有多少任务以及体系结构有多复杂，深度网络都是通过最小化单个损失来学习的。</p>
<h2 id="2-学习loss">2 学习Loss</h2>
<p>这个方法可以归为不确定度方法，但不同之处在于<strong>它是根据输入内容来预测“损失”</strong>，而不是根据输出来统计估计不确定度。这类似于各种难例挖掘，因为他们认为损失较大的训练数据点对模型的改进意义重大</p>
<p>去学习损失，这是一个很有创新的想法，但有用。既然我们选择数据的时候没有标签（未打标签），那么是不是可以先选择出预测较差的数据进行打标签。这时候loss确实是一个可以用来做这种选择的东西。当loss较大的时候说明和真实标签的差异性很大，loss较小则差异较小。其实最终我们是要对未打标签的数据进行<strong>排序</strong>，先是模型较难预测的，然后再是较好预测的。</p>
<p>作者就很聪明，没有label那么我们可以去预测一个loss啊。利用损失来进行排序。其实这个loss的得到也是很悬，可能是有某个隐藏的特征空间进行映射，<strong>或者说这里说是loss，但是网络本身学习到的并不是loss呢，而是一个排序</strong>。但是文章这里就把这个学习到的东西叫做loss，<strong>其实不然，我觉得更侧重于一个排序的信息</strong>。这里的思想和一篇图像质量评价的论文很像，学习怎么去排序，见之前的图像评价论文RankIQA。</p>
<p>那么这个loss如何学习呢？第一时间我们可以想到的是用回归的方法学习，即MSE，利用我们训练分类器或者检测或则定位的loss来作为label（见图）。作者也是这么想的，但是实验下来效果很差，还不如没用来的好。<strong>所以作者解释是由于在梯度下降的时候loss也是下降的，所以学习loss是不稳定的，无法学习</strong>。所以作者想到了用一个<strong>排序的loss</strong>（也可以认为是<strong>比较大小的</strong>loss）来作为损失函数，这样即使作为label的loss一直在变，但是loss的相对大小是不会变的，难的样本的loss就是比容易样本的loss来的大（和RankIQA中的思想一毛一样）。</p>
<p>这里计算排序的loss和RankIQA中的思想一毛一样，但是网络架构不一样，RankIQA中是使用参数共享的两个网络进行排序，这里是将一个batch分为两部分。但是仔细想想是一毛一样的，都是同一个端到端的网络，只是说一个是分开的 一个是画到一块。</p>
<p>这里的排序loss（不行叫他学习loss的loss，因为容易混 还不太准确）：
<img src="https://jzq6520.github.io/post-images/1571306285061.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571306940422.png" alt=""></p>
<p>最终端到端loss：
<img src="https://jzq6520.github.io/post-images/1571307032943.png" alt=""></p>
<h2 id="3-网络结构">3 网络结构</h2>
<p><img src="https://jzq6520.github.io/post-images/1571307076883.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571307093582.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571307115680.png" alt=""></p>
<h2 id="4-结果">4 结果</h2>
<p>如果为标注数据多的时候，这个方法选择样本是这样的：首先对所有未标注的进行随机采样，然后在选择前k个最不确信的样本，这样可以避免都选择出相同类别的。因为可能会出现前k个最不确信的样本中大部分都是一个类别的。</p>
<p>这个主动学习的方法比之前的几个要好,learn loss表示现在这个方法：
<img src="https://jzq6520.github.io/post-images/1571307198541.png" alt=""></p>
<p>mse损失和排序损失的效果比较：
<img src="https://jzq6520.github.io/post-images/1571307238951.png" alt=""></p>
<h2 id="5-总结">5 总结</h2>
<p>一个是通过排序的损失，一个是mse损失。排序损失会更好。其实这里更重要的是得到一个损失的相对大小，而不是损失本身，因为最后的目的是根据损失对未标注的样本进行挑选，所以排序准确才是最终目的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[cv-笔记-主动学习active learning的思想]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-zhu-dong-xue-xi-active-learning</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-zhu-dong-xue-xi-active-learning">
        </link>
        <updated>2019-10-17T02:27:00.000Z</updated>
        <content type="html"><![CDATA[<p>数据不断涌入，但深度神经网络仍然对数据如饥似渴。实证分析表明，就训练数据的规模而言，近期深度网络的表现尚未达到饱和。因此，从半监督学习到非监督学习的学习方法，以及弱标记或未标记的大规模数据都受到关注。</p>
<p>但是，在数据量一定的情况下，半监督或无监督学习的性能仍然与全监督学习的性能有一定的差距。标注数据的比例越高，性能越好。这就是为什么我们要忍受注释的劳动和时间成本。</p>
<ul>
<li><strong>主动学习通过一定的算法查询最有用的未标记样本，并交由专家进行标记，然后用查询到的样本训练分类模型来提高模型的精确度。</strong></li>
<li>那么被动学习就是说别人标注了什么数据我们就拿什么数据进行训练，而不是主动的去挖掘一些模型需要的数据去标注。</li>
<li>查询函数（就是从未标注数据中挖掘需要进行标注的数据的方法）的设计最常用的策略是：不确定性准则（uncertainty）和差异性准则（diversity）。
<ul>
<li>不确定性：对于分类选择出那些概率解决于0.5，或者是不确定的数据进行标注，对于分割可选择出那些分割错误较大的数据进行标注。</li>
<li>差异性：可以建立多个不同的模型，然后挑选出那些预测不一致的数据进行标注。</li>
</ul>
</li>
</ul>
<p><img src="https://jzq6520.github.io/post-images/1571280110452.png" alt=""></p>
<h2 id="引用">引用</h2>
<ul>
<li>https://www.cnblogs.com/hust-yingjie/p/8522165.html</li>
<li>https://www.jianshu.com/p/e908c3595fc0</li>
<li>learn loss:《Learning Loss for Active Learning》</li>
<li>entropy-based sampling：《Latent structured active learning》</li>
<li>random sampling:《An analysis of active learning strategies for sequence labeling tasks》</li>
<li>core-set:《Active learning for convolutional neural networks: A core-set approach.》</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-深度学习模型压缩]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-shen-du-xue-xi-mo-xing-ya-suo</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-shen-du-xue-xi-mo-xing-ya-suo">
        </link>
        <updated>2019-10-11T09:03:34.000Z</updated>
        <content type="html"><![CDATA[<p>在不降低或者是不大幅度减少精度的条件下，目前的模型压缩方式有：</p>
<ul>
<li>知识蒸馏；</li>
<li>量化</li>
<li>稀疏</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-图像质量评价Image quality assessment IQA简介]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-tu-xiang-zhi-liang-ping-jie-image-quality-assessment-iqa-jian-jie</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-tu-xiang-zhi-liang-ping-jie-image-quality-assessment-iqa-jian-jie">
        </link>
        <updated>2019-10-10T02:32:14.000Z</updated>
        <content type="html"><![CDATA[<p>2019年10月10日</p>
<h2 id="1-简介">1 简介</h2>
<p>图像质量评价（Image quality assessment，IQA）按照是否有参考图像可以分为三种：1. 有参IQA，2.半参IQA，3. 无参IQA。</p>
<p>有参图像质量评价虽然效果很好，但是前提是要有一个参考图像，也就是说一张质量好的图像作为参考，然后给评价的图像进行打分，而且参考图像和被打分的图像是要同一个图。但是在某些场景下参考图像是比较难获得的。</p>
<p>在测试图像压缩、图像传输效果时候或许可以有一个原始图像作为参考，然后测试被压缩或被传输后的图像知道。但是在很多场景是无法获得参考图像的，例如你拍摄一张图像，如果你没有拍摄一个绝对清晰的图像，那就无法拿到参考图像。比还有一些实时的场景下我们是不好获得参考图像的。</p>
<p><strong>所以这时候无参图像质量评价就出来了。</strong></p>
<h2 id="2-基于深度学习的图像质量评价">2 基于深度学习的图像质量评价</h2>
<p>废话不多说直接进入正题，这里我们主要讨论利用深度学习来解决图像质量评价问题。传统方法可以自行谷歌。</p>
<p>在我看来基于深度学习的图像质量评价主要有两种：</p>
<ol>
<li>直接利用回归，输入为图像，输出为质量评价分数。那么训练数据集就是 图像和label。</li>
<li>利用伪参考图像，然后将待评价图像和伪参考图像一起输入到回归网络中，输出质量评价分数。那么训练数据要包含：待评价图像、参考图像和label。注：伪参考图像这个说法是我自己想的。</li>
</ol>
<p>接下来对这两种基于深度学习的方法进行简单介绍，并且附上论文。</p>
<h2 id="21-直接利用回归">2.1 直接利用回归</h2>
<p>直接利用回归进行质量评价。这个思路比较简单，就是我们输入一张待评价图像然后输出的是一个值（score）。</p>
<p>网络就可以设计为一个卷积神经网络，例如我们可以选用VGG网络进行回归。</p>
<p><strong>但是由于图像质量评价的数据往往难以获得，且存在很大的主观性，所以说数据量是不大的。所以Xialei Liu等人就提出了RankIQA来解决数据量小的问题。</strong></p>
<h3 id="211-rankiqa">2.1.1 RankIQA</h3>
<p>别看名字这么玄乎，其实本身思想其实很简单，就是利用迁移学习。那么从哪里迁移网络参数呢？是从参数共享的网络得到。<strong>这个参数共享又不能说是孪生网络（注：论文中说是孪生网络），因为这个网络不像孪生网络一样有连接在一块，所以应该只能算是参数共享的网络</strong>。</p>
<p>首先构建两个参数共享的网络，这时候将两张图分别从两个网络输入，网络输出的是一个质量评价分数，这时候我们是知道两张输入图像哪张好哪张坏的，所以这时候就可以比较两个网络输出的质量评价分数来进行计算loss。如果输出的质量评价分数的大小关系和实际一样，那么就不计算loss。</p>
<p><img src="https://jzq6520.github.io/post-images/1570677420978.png" alt=""></p>
<p>那么输入的图像是哪里来的呢？是我们自己构造的，利用高斯核对图像进行模糊，这样我们就可以生成一组不同模糊程度的图像。因为我们不知道模糊程度该打几分，所以在训练参数共享网络的时候，loss是通过比较两个网络的数据来进行计算的，而不是和传统的一样计算均方差。</p>
<p>LOSS:
<img src="https://jzq6520.github.io/post-images/1570677513453.png" alt=""></p>
<p>这里有个假设，就是x1是大于x2的，所以从这个公式1的Loss公式可以看出，当网络输出x1大于x2时不计算loss，反之则计算一个loss。</p>
<p>最后，由于我们自己生成了许多可以指定图像质量相对好坏的数据，所以可以训练更深的网络，克服了一部分数据量的问题。然后利用在人工标注的groundtruth上进行finetune（微调）得到最终的回归网络（质量评价网络）。</p>
<p><strong>总结：数据上只需要图像和对应的质量分数，网络相对简单。利用了迁移学习和很聪明的学习技巧和损失函数</strong></p>
<h2 id="22-利用伪参考图像">2.2  利用伪参考图像</h2>
<p>为什么我说这个是伪参考图像，因为这里面<strong>训练的时候是需要参考图像</strong>的，并且<strong>网络会生成一个假的参考图像</strong>，这个参考图像是通过网络对质量差的图像<strong>修复得到</strong>，例如一种模糊图像，然后由网络对其进行修复生成一张清晰的图像。然后利用这个清晰的图像作为参考图像对图像进行质量评价。</p>
<p>这里介绍两篇文章：RAN4IQA和Hallucinated-IQA，这两篇文章都是这样的思路，都是利用GAN（对抗生成网络）修复质量差的图像，然后将得到的修复后的图像（伪参考图像）作为参考图像对待评价图像进行打分。</p>
<p>所以这两个网络可以分解成三部分：<strong>伪参考图像评价网络 = GAN + 评价网络 = 修复网络（生成网络）+ 判别网络 + 评价网络</strong></p>
<p><strong>其实我们可以发现，当我们抛开评价网络，剩下的GAN就成了一个图像修复网络了</strong>。</p>
<p>再说一下这两个网络的区别：</p>
<ul>
<li>RAN4IQA是将修复后的图像和待参考图像输入到<strong>孪生网络</strong>中去回归得到分数。当然这里是基于patch的，所以还会有一个patch的权重，最后将所有patch的权重加权平均得到最后整张图像的分数。</li>
<li>Hallucinated-IQA是将<strong>差值图像</strong>（修复后的和待评价图像相减后得到的图）和待参考图像输入到一个<strong>双输入网络</strong>中，回归得到一个分数。并且使用了修复网络的下采样输出的特征。</li>
</ul>
<h3 id="221-ran4iqa">2.2.1 RAN4IQA</h3>
<p><img src="https://jzq6520.github.io/post-images/1570678849786.png" alt=""></p>
<h3 id="222-hallucinated-iqa">2.2.2 Hallucinated-IQA</h3>
<p><img src="https://jzq6520.github.io/post-images/1570678886713.png" alt=""></p>
<h2 id="3-总结">3 总结</h2>
<p>当有参考图像的时候可以训练一个伪参考图像网络，当没有参考图像的时候可以训练一个RankIQA网络。</p>
<h2 id="4-参考">4 参考</h2>
<ul>
<li>Hallucinated-IQA: No-Reference Image Quality Assessment via Adversarial Learning</li>
<li>RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality Assessment</li>
<li>RankIQA: Learning from Rankings for No-reference Image Quality Assessment</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[keras-Callback回调函数]]></title>
        <id>https://jzq6520.github.io/post/keras-callback-hui-diao-han-shu</id>
        <link href="https://jzq6520.github.io/post/keras-callback-hui-diao-han-shu">
        </link>
        <updated>2019-07-29T06:30:07.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-回调函数的定义">1 回调函数的定义</h2>
<p>当程序跑起来时，一般情况下，应用程序（application program）会时常通过API调用库里所预先备好的函数。但是有些库函数（library function）却要求应用先传给它一个函数，好在合适的时候调用，以完成目标任务。这个被传入的、后又被调用的函数就称为回调函数（callback function）。</p>
<h4 id="回调函数的例子">回调函数的例子</h4>
<pre><code class="language-python">### 回调函数
#回调函数1
#生成一个2k形式的偶数
def double(x):
    return x * 2
    
#回调函数2
#生成一个4k形式的偶数
def quadruple(x):
    return x * 4

## 使用回调函数的中间函数，也就是回调函数的使用者
#中间函数
#接受一个生成偶数的函数作为参数
#返回一个奇数
def getOddNumber(k, getEvenNumber):
    return 1 + getEvenNumber(k)
    
#起始函数，这里是程序的主函数
def main():    
    k = 1
    #当需要生成一个2k+1形式的奇数时
    i = getOddNumber(k, double)
    print(i)
    #当需要一个4k+1形式的奇数时
    i = getOddNumber(k, quadruple)
    print(i)
    #当需要一个8k+1形式的奇数时
    i = getOddNumber(k, lambda x: x * 8)
    print(i)
    
if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>可以发现，其实我们只需要知道回调函数是<strong>传入什么参数，有什么功能即可</strong>，而不能自己去定义传入参数，<strong>所以我们去写回调函数的时候要按照中间函数传入的参数去写</strong>。</p>
<h2 id="2-keras中的回调函数">2 keras中的回调函数</h2>
<p>这里我们不讲那些keras中已经定义的回调函数，这里说一下如何创建自己的回调函数。
keras中定义了一个回调函数的抽象类，这个类包含多个回调函数（即一个类里面有很多方法，这些方法是不同的时期被中间函数调用的）。我们继承这个类，然后重新其中的回调函数即可。
下面看一下这个基类：
https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L275</p>
<pre><code class="language-python">class Callback(object):
			 &quot;&quot;&quot;Abstract base class used to build new callbacks.
    # Properties
        params: dict. Training parameters
            (eg. verbosity, batch size, number of epochs...).
        model: instance of `keras.models.Model`.
            Reference of the model being trained.
    The `logs` dictionary that callback methods
    take as argument will contain keys for quantities relevant to
    the current batch or epoch.
    Currently, the `.fit()` method of the `Sequential` model class
    will include the following quantities in the `logs` that
    it passes to its callbacks:
        on_epoch_end: logs include `acc` and `loss`, and
            optionally include `val_loss`
            (if validation is enabled in `fit`), and `val_acc`
            (if validation and accuracy monitoring are enabled).
        on_batch_begin: logs include `size`,
            the number of samples in the current batch.
        on_batch_end: logs include `loss`, and optionally `acc`
            (if accuracy monitoring is enabled).
    &quot;&quot;&quot;
		
     def __init__(self):
        self.validation_data = None
        self.model = None
		
		## 注意：set params和 set_model是已经定义好的，也就是说继承这个类以后回调函数本身就会有self.params 和self.model，不需要我们去关心有没有或者怎么得到这两个变量。
		# 既然是回调函数，那么params这个参数是中间函数传给它的，不需要我们去传。
    def set_params(self, params): 
        self.params = params

    def set_model(self, model):
        self.model = model
		
		## 什么时候会调用，直接可以看方法名。
		# Arguments
    #       logs: 具体可以看上面链接中给出的注释，每个都不一样。
	def on_batch_begin(self, batch, logs=None)
	def on_batch_end(self, batch, logs=None)
	def on_epoch_begin(self, epoch, logs=None)
	def on_epoch_end(self, epoch, logs=None)
	def on_train_batch_begin(self, batch, logs=None)
	def on_train_batch_end(self, batch, logs=None)
	def on_test_batch_begin(self, batch, logs=None)
	def on_test_batch_end(self, batch, logs=None)
	def on_predict_batch_begin(self, batch, logs=None)
	def on_predict_batch_end(self, batch, logs=None)
	def on_train_begin(self, logs=None)
	def on_train_end(self, logs=None)
	def on_test_begin(self, logs=None)
	def on_test_end(self, logs=None)
	def on_predict_begin(self, logs=None)
	def on_predict_end(self, logs=None)	
</code></pre>
<p>在回调函数中可以使用这两个参数。</p>
<ul>
<li>
<pre><code>    self.params = params： 字典。训练参数， (例如，verbosity, batch size, number of epochs...)。可以打印出来看看。
</code></pre>
</li>
<li>
<pre><code>    self.model = model：keras.models.Model 的实例。 指代被训练模型。
</code></pre>
</li>
</ul>
<p>通过类的属性 self.model，回调函数可以获得它所联系的模型。</p>
<h3 id="keras自定义回调函数的例子">keras自定义回调函数的例子</h3>
<p>回调函数使用以后，还可以通过类实例来访问实例变量。</p>
<pre><code class="language-python">class LossHistory(keras.callbacks.Callback):
    def __init__(self):
				super(LossHistory, self).__init__()
				self.losses = []
				
    def on_train_begin(self, logs={}):
        self.losses = []

    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))

model = Sequential()
model.add(Dense(10, input_dim=784, kernel_initializer='uniform'))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

history = LossHistory()
model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history])

print(history.losses)
</code></pre>
<h2 id="引用">引用</h2>
<ul>
<li>https://github.com/chensvm/Keras-Callback-F1/blob/master/roc_auc_score_Metrics</li>
<li>https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L275</li>
</ul>
]]></content>
    </entry>
</feed>