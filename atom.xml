<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jzq6520.github.io</id>
    <title>chuck</title>
    <updated>2019-11-04T01:58:08.215Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jzq6520.github.io"/>
    <link rel="self" href="https://jzq6520.github.io/atom.xml"/>
    <subtitle>天地不仁以万物为刍狗</subtitle>
    <logo>https://jzq6520.github.io/images/avatar.png</logo>
    <icon>https://jzq6520.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, chuck</rights>
    <entry>
        <title type="html"><![CDATA[CV-笔记-RetinaNet和Focal Loss]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-retinanet-he-focal-loss</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-retinanet-he-focal-loss">
        </link>
        <updated>2019-10-31T07:34:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-focal-loss">1 Focal Loss</h2>
<p>文章的题目是Focal Loss for Dense Object Detection，密集目标检测。主要还是解决那些和目标没有交集的anchor的分类，这些anchor是最简单的负样本，但是他们数量巨多。</p>
<ul>
<li><strong>样本不平衡：对于RPN来说大部分anchor都是负样本，对于yolo来说大部分格子都是负样本</strong>。</li>
<li>两阶段目标检测训练的最后的分类部分通常都是稀疏的，什么意思呢，<em>ﬁrst stage generates a sparse set of candidate object locations, and the second stage classiﬁes each candidate location as one of the foreground classes or as background using a convolutional neural net- work</em>.（第一阶级会产生一些建议区域,然后第二阶段对这些区域进行分类，这样对第二阶段来说需要分类的区域已经大大减少了，<strong>所以说是稀疏的</strong>)，见 Fast R-CNN。</li>
<li>而单阶段的目标检测训练通常是密集的，什么意思呢，就是需要对所有可能区域进行分类，见YOLO。举个例子：以输入为800X800,五层金字塔尺度分别为100X100,50X50,25X25,12X12,6X6,<strong>算一下anchor数量，119745</strong>， <strong>10几万anchor覆盖在整张图像中，密密麻麻</strong>，让你知道什么是密集检测，哈哈，插句话，<strong>10几万anchor中绝大多数都是负样本</strong>，无用的计算太多，这也是现在anchor free的一个出发点。【2】</li>
<li>单阶段检测（高密度检测）训练遇到的主要问题是<strong>样本不平衡</strong>，因为通常正样本总是小于负样本的，例如yolo是根据<strong>目标中心</strong>是不是落在格子里，落在格子里那么这个格子就是正样本，否则这个格子就是负样本，<strong>那么大部分格子肯定是没有目标的，也就是说正负样本是非常不平衡的</strong>。而Fast R-CNN也是，region proposal很多，但是里面<strong>大部分区域（anchor）应该是负样本，少部分（anchor）是正样本</strong>，所以Fast R-CNN训练分类的时候是通过(稀疏)采样。因为anchor是要分类的，所以大部分anchor是负样本。</li>
<li>在这篇文章之前一般都是双阶段的效果比较好（proposal-driven mechanism）。</li>
<li>所以有没有办法让单阶段的效果变好，毕竟单阶段的速度快。</li>
<li><strong>文章就抓住了训练时候正负样本既不平衡的这个主要矛盾</strong>来提高模型精度。最后达到了和FPN， mask R-CNN这样的精度。</li>
<li>最好的模型是ResNet-101- FPN(当然是文章提出的时候，现在backbone都有很多优秀的了）， 5fps。</li>
</ul>
<h3 id="11-之前目标检测中解决样本不平衡的方法">1.1 之前目标检测中解决样本不平衡的方法</h3>
<p>利用第一阶段<strong>过滤到大部分假阳（背景）</strong>，然后第二阶段再进行下面的工作：</p>
<ol>
<li>对需要分类的区域进行采样，见Fast R-CNN。</li>
<li>在线难例挖掘 online hard example mining (OHEM)</li>
</ol>
<p>相比之下，一个单级目标检测必须处理一组更大的候选对象位置，这些对象位置是通过图像定期采样的。实际上，这常常相当于枚举约100k个位置，这些位置密集地覆盖了空间位置、尺度和纵横比。虽然也可以采用类似的抽样启发法，但它们的效率很低，因为训练过程仍然由容易分类的背景示例所主导。这种低效率是对象检测中的一个经典问题，通常通过引导或硬示例挖掘等技术来解决。</p>
<p>这种不平衡导致了两个问题:(1)训练效率低，因为大多数区域都是<strong>简单的负样本</strong>，这种简单的负样本对训练作用不大;(2)整体而言，很容易被这些<strong>容易的但是很多</strong>的负样本影响，<strong>导致模型退化</strong>。</p>
<h3 id="12-focal-loss">1.2 Focal loss</h3>
<ul>
<li><strong>Huber损失（类似smooth L1）</strong>，该函数通过降低具有较大错误的示例(硬示例)的损失的权重来<strong>减少异常值的贡献</strong>，因为在误差大的时候损失的梯度是不变的，不变很重要 这样就不容易被误差值所影响，其实也就是差值一个是不进行平方，一个进行平方计算，平方计算就会增长很快，而在误差小的时候梯度慢慢减少。</li>
<li>相比之下，<strong>Focal loss</strong>的重点并不是处理异常值，而是通过减权<strong>简单的例子</strong>来解决阶级不平衡，这样即使简单的例子的数量很大，它们<strong>对总损失的贡献也很小</strong>。<strong>这点非常重要，是减少简单例子的影响，所以选择这个loss的适合一定要思考你的负样本是不是所谓的简单的负样本</strong>。</li>
</ul>
<p>下面来看一下cross entropy的损失函数曲线（其实就是-log(p)）：
<img src="https://jzq6520.github.io/post-images/1572512564723.png" alt="">
cross entropy可以看着只计算这个神经元负责的当前正样本的loss，假设有两个神经元分别预测目标和背景，那么对于两个神经元来说这两个都是正样本（因为第一个负责预测目标，第二负责预测背景呀，所以第一个神经元的正样本就是目标，第二个神经元的正样本就是背景，都是要预测概率为1的）因为是<strong>onehot</strong>。</p>
<p>这里有个假设，假设有个正样本，我们预测他为正样本的概率是<code>0.6~1</code>之间，我们就算这个样本是简单的，<strong>也就说能预测对的就算是简单的</strong>。</p>
<p><strong>所以我们观察上图可以发现，在我们定义的简单样本的区间内，其实普通的cross entropy的loss还是继续下降的（也就是说还是有一点的loss的），因为cross entropy总是希望概率可以很大很确信</strong>。所以可以看出focal loss在这个区间内的loss是很小的，而预测不对的话损失是很大的。<strong>这就是Focal loss核心思想</strong>。划重点。</p>
<p>因为我们希望在目标检测中那么多的<strong>负样本不要共享很多loss，预测的差不多得了，能大于0.6就好了，知道你是就行了，超过0.6，loss就少贡献一点</strong>，但是因为正负都是同等对待的，所以正样本预测的概率数值也会小一点。</p>
<h4 id="121-普通的加权方式">1.2.1 普通的加权方式</h4>
<p><img src="https://jzq6520.github.io/post-images/1572514035161.png" alt=""></p>
<ul>
<li>虽然alpha参数可以平衡正负样本，但是还无法区分简单和困难的例子，我们希望减少简单例子的权重而关注（Focus）那些难的样本。</li>
<li>α ∈ [0, 1] for class 1（正样本） and 1−α for class −1（负样本）.</li>
<li>因为RetinaNet里面是直接预测k个类，而不是先预测是不是目标。所以这里的加权应该是直接在是目标的类别加权α，然后负样本加权1−α。</li>
<li>下图可以看出<strong>只加α</strong>和<strong>加了focal与α</strong>的α的值是不一样的，与我们直观理解一样只加α的时候正样本的权重应该要高一点，而加了focal以后反而正样本的权重要小一点好：
<img src="https://jzq6520.github.io/post-images/1572601464394.png" alt=""></li>
</ul>
<h4 id="122-focal-loss">1.2.2 focal loss</h4>
<p>所以focal loss增加了一个调制的因子<code>(1 − pt )^γ</code>，当γ大于0时就是focal loss，等于0的适合是普通的交叉熵loss。
<img src="https://jzq6520.github.io/post-images/1572577100834.png" alt=""></p>
<p>有两点：</p>
<ol>
<li>可以发现当pt趋向于0的时候，<code>(1 − pt )^γ</code>因子就接近1，loss的值是非常大的，当pt趋向于1的时候（或者说是子啊0.6到1的时候）<code>(1 − pt )^γ</code>这个因子就会解决与0，那么loss贡献就会非常的小，这个和γ参数也有关，γ越大那么预测概率解决于0的时候的loss越小。</li>
<li><em>we found γ = 2 to work best in our experiments</em>，作者实验下来 γ = 2的时候最好。</li>
</ol>
<p>实际使用中作者还加了alpha权重：
<img src="https://jzq6520.github.io/post-images/1572577872330.png" alt=""></p>
<h2 id="2-retinanet">2 RetinaNet</h2>
<p><strong>单阶段（one-stage）的目标检测，使用了Focal Loss解决分类的正负样本不平衡问题</strong>。</p>
<p>为什么叫这个名字，因为是一个密集目标检测（像视网膜一样密集），这里的密集意思是一次性在所有区域里面检测模型，而不是像两阶段一样从建议区域里面检测。</p>
<p>retinanet只是使用了Focal Loss的一个网络，其他和这个loss无关，采样的都是以前的一些结构，然后很好的组合成一块。</p>
<p>总的结构是这样的：
<img src="https://jzq6520.github.io/post-images/1572589354355.png" alt=""></p>
<p>分别说一下RetinaNet里面用的这4个东西的设置：</p>
<ol>
<li>FPN</li>
<li>Anchor</li>
<li>打label</li>
<li>分类分支</li>
<li>回归分支</li>
</ol>
<h3 id="21-fpn">2.1 FPN</h3>
<ul>
<li>右边特征金字塔层（level）<strong>从p3到p7</strong>用来检测，FPN原文中只是用了p3到p5.<em>We construct a pyramid with levels P3 through P7 , where <code>l</code> indicates pyramid level (P <code>l</code> has resolution 2^<code>l</code>(2的l次方) lower than the input)</em>.</li>
<li>金字塔层channel为256。</li>
<li>P6是在p5上用stride为2的3x3卷积。P7也是这样，然后都有relu。, P 6 is obtained via a 3×3 stride-2 conv on C 5 , and P 7 is computed by applying ReLU followed by a 3×3 stride-2 conv on P 6 .</li>
</ul>
<h3 id="22-anchor">2.2 anchor</h3>
<ul>
<li>P3, P4, P5,P6,P7 的基础面积分别是32，64，128，256，512，然后实际面积是基础面积乘以三个倍数，得到三种面积，倍数分别是<code>2^0=1，2^(1/3)=1.26，2^(2/3)=1.58</code>；然后每种面积都有三种宽高比1：1， 1：2，2：1，所以每个金字塔层总共有9个anchor。</li>
</ul>
<h3 id="23-打label">2.3 打label</h3>
<p><strong>记住正负样本是对anchor打的</strong>。</p>
<p>这里有个特例就是<strong>iou=0的时候</strong>，iou等于0说明anchor的框是完全和目标没有交集的，这个时候就<strong>没有box相对坐标可以回归了</strong>。</p>
<p>这里打label的阈值和RPN中有点不同，这里是与目标框iou大于等于0.5的anchor算是正样本，小于0.4的anchor算是负样本，<code>[0.4，5）</code>iou之间不计算loss.</p>
<p><strong>这里来区分一下简单负样本，难负样本</strong>：</p>
<ul>
<li><strong>简单负样本</strong>：那些和目标框没有交集的anchor，也就是IOU为0的anchor。</li>
<li><strong>难负样本</strong>：和目标框有交集的，然后iou大于0小于0.4的，当然在这个范围内越接近0.4越难了，因为很接近目标啦，但还是负样本。</li>
</ul>
<h3 id="24-分类分支">2.4 分类分支</h3>
<ul>
<li>不同金字塔层的分类分支是参数共享的。</li>
<li>分类分支和回归分支没有共同的conv，这个和RPN不一样。</li>
<li>分支的conv很多，加上最后一个预测的有5个3x3conv，最后预测的时候也是3x3conv。</li>
<li>最后预测的适合是sigmoid激活，而不是softmax。这样好像对Focal loss的使用好，因为可以想象focalloss应用范围都是概率要0到1之间的，softmax是加起来才是1 那可能本身概率就会小一点。</li>
<li>每个conv层的channel都是256，最后输出channel是k个类别乘以A个anchor。</li>
</ul>
<h3 id="25-回归分支">2.5 回归分支</h3>
<ul>
<li>4乘A个channel。</li>
<li>回归离anchor最近的目标，<em>regressing the offset from each anchor box to a nearby ground-truth object</em>。<strong>一个目标可能分配给多个anchor，但是一个anchor只会分配一个目标</strong>。</li>
<li>其他设置和分类分支一样，channel，参数共享，conv什么的都一样</li>
</ul>
<p><em>那么iou为0的时候要回归坐标吗？负样本要回归坐标吗？</em>：</p>
<ol>
<li>regressing the offset from each anchor box to a nearby ground-truth object, <strong>if one exists</strong>.即如果存在的话要回归，<strong>具体要不要得看代码了</strong>。<strong>但是RPN里面是只计算正样本的回归损失，见RPN博客中的损失函数</strong>。</li>
</ol>
<h3 id="26-推理阶段-inference阶段-也就是预测阶段">2.6 推理阶段 Inference阶段 也就是预测阶段</h3>
<p>在推理阶段，由于预测出来的款也很多，作者对金字塔每层特征图都使用0.05（为什么这么低？不是0.5吗？因为后面还进行排序然后选前1000）的置信度阈值进行筛选，然后取置信度前1000的候选框（不足1000更好） ，接下来收集所有层的候选框，进行NMS,阈值0.5。</p>
<h3 id="25-训练阶段">2.5 训练阶段</h3>
<ul>
<li>γ = 2这是最好，是通过【0.5~5】测出来的。</li>
<li>再次强调<strong>使用所有anchor训练</strong>的，<em>We emphasize that when training RetinaNet, the focal loss is applied to all <strong>∼ 100k anchors</strong> in each sampled image</em>。以输入为800X800,五层金字塔尺度分别为100X100,50X50,25X25,12X12,6X6,<strong>算一下anchor数量，119745个</strong>。</li>
<li><strong>所有的loss相加哦，然后除以分配的ground-truth box的anchor（有交集）数量</strong>，因为<strong>简单负样本</strong>贡献loss少，几乎为0，<strong>难负样本还是贡献loss的</strong>，所以加入计算，因为负样本也是和目标框有交集的。<em>The total focal loss of an image is computed as the sum of the focal loss over all ∼ 100k anchors, normalized by the number of anchors assigned to a ground-truth box.</em></li>
<li>因为去除掉iou为0的anchor以后，还剩下一部分有和目标交集的难负样本呀，所以这部分的负样本的数量还是大于正样本的，所以文章中还对loss加了个权重，来平衡这部分的loss。最后我们注意,α,重量分配到罕见的类。<em>Finally we note that α, the weight assigned to the rare class, also has a stable range.</em>(for γ = 2, α = 0.25 works best)，这两个参数需要相互结合调整。</li>
<li>α ∈ [0, 1] for class 1 and 1−α for class −1.</li>
</ul>
<h4 id="251-参数初始化">2.5.1 参数初始化</h4>
<p>所有新conv层RetinaNet子网中除了最后一个conv外都初始化为bias=0。最后conv层用下面的公式算出来的值初始化，文章中设置为π = .01：
<img src="https://jzq6520.github.io/post-images/1572600783017.png" alt="">
作用：<strong>这样初始化可以防止大量的背景锚点在训练的第一次迭代中产生较大的、不稳定的损失值</strong>。</p>
<h3 id="网络训练细节">网络训练细节</h3>
<p>RetinaNet is trained with stochastic gradient descent (SGD). We use synchronized <strong>SGD</strong> over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise speciﬁed, all models are <strong>trained for 90k iterations</strong> with an <strong>initial learning rate of 0.01</strong>, which is then <strong>divided by 10 at 60k and again at 80k iterations</strong>. We use horizontal image ﬂipping as the only form of data augmentation unless otherwise noted. <strong>Weight decay of 0.0001 and momentum of 0.9 are used</strong>. The training loss is the sum the focal loss and the standard smooth L 1 loss used for box regression [10]. <strong>Training time</strong> ranges between <strong>10 and 35 hours</strong> for the models in Table 1e.</p>
<h2 id="结果">结果</h2>
<p>信息量确实很大，文章很好，看不动了，好累，直接贴实验结果吧。
<img src="https://jzq6520.github.io/post-images/1572601681760.png" alt="">
<img src="https://jzq6520.github.io/post-images/1572601690019.png" alt="">
<img src="https://jzq6520.github.io/post-images/1572601695560.png" alt="">
<img src="https://jzq6520.github.io/post-images/1572601732469.png" alt="">
<img src="https://jzq6520.github.io/post-images/1572601737614.png" alt=""></p>
<h2 id="引用">引用</h2>
<ul>
<li>各种loss：http://baijiahao.baidu.com/s?id=1603857666277651546&amp;wfr=spider&amp;for=pc</li>
<li>fpn anchor：https://www.cnblogs.com/wzyuan/p/10847478.html</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读特征金字塔网络 (FPN)]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-te-zheng-jin-zi-ta-wang-luo-fpn</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-te-zheng-jin-zi-ta-wang-luo-fpn">
        </link>
        <updated>2019-10-31T02:39:07.000Z</updated>
        <content type="html"><![CDATA[<p>对于网络的卷积特征的几个重要理解：</p>
<ul>
<li>但由于深度不同，导致了不同层较大的<strong>语义差异</strong>。高分辨率图的低层特征损害了其对目标识别的表征能力。</li>
<li>语义差异：语义差异就说对目标的类别的识别的能力的差异，低层特征主要是一些边缘，形状等交基础的特征，对整个物体的类别识别帮助不大，而高层特征可能是更加具体的特征，已经很能反映整个是什么物体了，所以底层特征语义较差。</li>
<li>利用卷积神经网络特征层次结构的金字塔形状，同时构建一个在所有尺度上都具有<strong>强大语义</strong>的特征金字塔，如果不看skip连接的话FPN相当于是在网络后面接了另外的很多层卷积，所以属于高层特征。</li>
<li>所以有了另外一路<strong>上采样的卷积层</strong>以后，就具有了在<strong>每个尺度上都有高语义的特征图</strong>。即依赖于一个通过<strong>自顶向下（即上采样路径）路径和横向连接</strong>将<strong>低分辨率语义强的特征</strong>与<strong>高分辨率语义弱的特征</strong>相结合的架构。所以这也是最后特征图的channel都是一样的原因吧，这里更加注重语义特征。</li>
<li>FPN文章中是用FPN和Faster R-CNN结合。</li>
</ul>
<h2 id="结构实现细节">结构实现细节</h2>
<ul>
<li>上采样使用最近邻插值</li>
<li>只使用c2,c3,c4,c5尺度的特征图。</li>
<li>中间的skip连接是用1x1卷积把channel数降到256.</li>
<li>fpn是完全对称的
<img src="https://jzq6520.github.io/post-images/1572492067196.png" alt=""></li>
<li><strong>使用同样的channel的原因</strong>：因为金字塔的所有层次都使用<strong>共享的分类器/回归器</strong>，就像传统的特征图金字塔一样，我们修正了所有特征图的特征维数。Because all levels of the pyramid use shared classiﬁers/regressors as in a traditional featurized image pyramid, we ﬁx the feature dimension (numbers of channels, denoted as d) in all the feature maps. We set d = 256 in this pa- per and thus all <strong>extra convolutional</strong> layers have 256-channel outputs.There are no non-linearities in these <strong>extra layers</strong>, which we have empirically found to have minor impacts.<strong>在这些额外的层中不存在非线性，我们根据经验发现它们的影响很小</strong>。</li>
</ul>
<h2 id="基于特征金字塔fpn的rpn">基于特征金字塔(FPN)的RPN</h2>
<p>虽然fpn和rpn很相似，但是这两个p意思不一样，一个是pyramid（金字塔），一个是proposal（建议）。</p>
<ul>
<li><strong>RPN参数共享</strong>：<strong>head</strong>：文章中把RPN的那个操作的模块叫做<strong>头部</strong>，即一个3x3的卷积，然后加两个1x1的卷积进行分类和回归。</li>
<li>FPN中的RPN head（头部）是<strong>参数共享</strong>的。</li>
<li><strong>每个特征图负责检查一种尺度</strong>：<strong>assign anchors of a single scale to each level</strong>.并且每层只有一个尺度的anchor，即一个层上没有多尺度，每层只负责一种大小的目标检测，这里说的大小是指物体的面积，用面积衡量大小，但是物体的宽高比是不一样的。因为RPN已经是在不同尺度的特征图上做了，所以不需要再在一个特征图上做不同尺度的anchor了。</li>
<li>anchor设置：对于在faster R-CNN上用的anchor，<strong>在高分辨率特征图上检测小目标，在低分辨率上检测大目标</strong>。面积设置，特征图<strong>从大到小</strong>p2,p3,p4,p5分别的anchor面积是32^2 , 64^2 , 128^2 , 256^2，且每个面积有三个宽高比：{1:2, 1:1, 2:1} 。</li>
<li>实验对比共享rpn头和不共享，共享的效果比较好。</li>
</ul>
<h2 id="打label">打label</h2>
<ul>
<li>正样本：iou大于0.7的所有anchor或和ground-truth iou最早的anchor（因为可能存在所以anchor都和ground-truth iou小于0.7，那么这个是后就取iou最大的anchor了），<em>positive label if it has the highest IoU for a given ground- truth box or an IoU over 0.7 with any ground-truth box</em>。</li>
<li>负样本：所有iou小于0.3的anchor，a negative label if it has IoU lower than 0.3 for all ground-truth boxes。</li>
<li>ground-truth没明确分配到哪个尺度的特征图，只要anchor分配好就可以了。原文：<strong>Note that scales of ground-truth boxes are not explicitly used to assign them to the levels of the pyramid; instead, ground-truth boxes are associated with anchors, which have been assigned to pyramid levels. As such, we introduce no extra rules in addition to those</strong>。因为iou是两个物体面积差不多大才是大的，一个大物体和一个小物体全部覆盖，那iou也是低的。所以物体会根据iou大小自动分配的。</li>
<li></li>
</ul>
<h2 id="fpn的roi-pooling">FPN的ROI pooling</h2>
<p>那么FPN的roi pooling怎么做呢，因为有这么多个的特征图。</p>
<p>首先我们要搞清楚一个概念，<strong>RPN的作用是确定出原图上的目标ROI区域</strong>（而非特征图上的区域），这时候我们再将原图上的ROI坐标映射到特征图上，然后把特征图上的roi区域拿过来进行分类和区域的回归矫正。</p>
<p>理解了这一点，那就清楚了，RPN确定的只是在原图上的roi区域，所以RPN做完以后，anchor就没用了，这时候就根据roi来确定我要在哪一层选择这个区域的特征来进行分类和回归。</p>
<p>虽然我们直观上理解是哪个特征图检测出了这个目标，就由哪个特征图所roi pooling，<strong>其实不是的</strong>，文章中是对roi进行重新分配了，<strong>所以RPN做完以后预测出的候选区域就和特征图没有半毛钱关系了，后面就等着再分配了</strong>。</p>
<p>那么分配规则是怎么样的呢，文章中是按照下面这个公式来确定分配给哪一层，那个类似于中括号的是<strong>取整</strong>，应该是向上取整，如果是5到4.1就是再第5层特征层做。。
<img src="https://jzq6520.github.io/post-images/1572504129264.png" alt=""></p>
<p>首先，先通俗理解一下，前面最开始分配anchor的时候就知道了，深层特征图检测大目标，浅层的检测小目标，所以这里也是一样，<strong>大目标（即大ROI）分配给低分辨率（小特征图）的特征图即P5，小目标（小ROI）分配给到的高分辨率（大特征图）的特征图</strong>。</p>
<p>如果我们最小的特征图是P5，那么k0初始化为5，然后这里的224应该不要这样写好，应该写成输入图像的大小，如果输入是448那这里就是448了，意思就是如果roi是图像的一半，那么应该分配给P4特征图做roi pooling。以此类推。</p>
<p>其实这个公式算出来就是，缩小一半（2倍）就是到倒数第二层，缩小4倍就是到再下一层，缩小8倍就是再下一层。那么1到2倍就是再最后一层了。因为以2为底，log二分1就是-1，log四分之一就是-2。</p>
<p>然后做roi pooling都是7x7，最后堆叠到一个batchsize里面（这个和Fast R-CNN是一样的，见之前的博客）。然后预测分类和回归的适合连两个全连接。<em>1024-d fully-connected (fc) layers (each followed by ReLU) before the ﬁnal classiﬁcation and bounding box regression layers</em>.</p>
<h2 id="参数细节">参数细节</h2>
<h3 id="region-proposal-with-rpn">Region Proposal with RPN</h3>
<p>All architectures in Table 1 are trained end-to-end. The input image is resized such that its shorter side has 800 pixels. We adopt synchronized SGD training on 8 GPUs. A mini-batch involves 2 images per GPU and 256 anchors per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the ﬁrst 30k mini-batches and 0.002 for the next 10k. For all RPN experiments (including baselines), we include the anchor boxes that are outside the image for training, which is unlike [29] where these anchor boxes are ignored. Other implementation details are as in [29]. Training RPN with FPN on 8 GPUs takes about 8 hours on COCO.</p>
<h3 id="object-detection-with-fastfaster-r-cnn">Object Detection with Fast/Faster R-CNN</h3>
<p>The input image is resized such that its shorter side has 800 pixels. Synchronized SGD is used to train the model on 8 GPUs. Each mini-batch in- volves 2 image per GPU and 512 RoIs per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the ﬁrst 60k mini-batches and 0.002 for the next 20k. We use 2000 RoIs per image for training and 1000 for testing. Training Fast R-CNN with FPN takes about 10 hours on the COCO dataset.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读YOLO目标检测系列 v2]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-yolo-mu-biao-jian-ce-xi-lie-v2</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-yolo-mu-biao-jian-ce-xi-lie-v2">
        </link>
        <updated>2019-10-30T02:59:08.000Z</updated>
        <content type="html"><![CDATA[<p>哎，看下来 yolo v1 v2总感觉作者写文章一般般，写的不清晰。</p>
<h2 id="anchor尺寸设置">anchor尺寸设置</h2>
<p>不像Faster R-CNN里面一样使用手动设置anchor的尺寸，这里使用k-means来进行聚类，例如我们要五种类型的anchor，那么就聚为5类，那么聚类中心的尺寸就是我们的anchor尺寸。</p>
<p>但是这里的聚类距离选择的不是欧式距离，而是<code>1-IOU(box, centroid)</code>作为<strong>距离度量函数</strong>。这里说明一下怎么聚类，因为一个anchor是有两个值（RPN中提到的），一个是像素个数，还有一个长宽比，所以这里应该<strong>将长宽的大小作为聚类特征</strong>（因为长宽大小就决定了有多少像素和长宽比了），论文中说在做目标检测的时候主要是想要一个较高的IOU，但是k-means在聚类的时候是计算一个较近的距离，所以这里是1减去IOU（因为iou越大，1减去iou就越小了）。所以这里用<code>1-IOU</code>衡量，那怎么计算iou呢，我们可以假设box的中心点是<code>（0，0）</code>，因为是对现在所有的目标的box进行聚类找出我们的anchor的尺寸，所以我们的数据就是所有目标的box的长宽，然后假设中心点是<code>（0，0）</code>，这样我们就可以计算两个box之间的iou了。</p>
<p>其实RPN中是想要多种尺寸的anchor，也就是不同长宽的box，但不是说一定要像RPN中说的一样要指定几种面积，然后每种面积有几种长宽比。所以YOLO V2中就使用聚类来得到不同长宽的anchor（box）。</p>
<h2 id="label制作">label制作</h2>
<p>置信度：应该也是一个1和0，落在网格里面就是算有目标，这个和RPN不一样，RPN是通过iou计算正负样本的，这里是看是不是落在网格里面，如果不是这个区别那么就变成了普通的RPN了。原文：The network predicts 5 coordinates for each bounding box, t x , t y , t w , t h , and t o 。to就是置信度。</p>
<p>但是，需要注意的是，每个cell中实际上有5个anchor，并不是每个anchor的会预测这个物体，我们只会选择一个长宽和这个bbox最匹配的anchor来负责预测这个物体。那么什么叫长宽最为匹配？这个实际上就是将anchor移动到图像的右上角，bbox也移动到图像的左上角，然后去计算它们的iou，iou最大的其中一个anchor负责预测这个物体。这点和RPN中不一样，RPN只需要iou符合大于某个阈值就可以了。感觉yolo复杂多了。</p>
<p>和RPN还有个不同的是，RPN的是目标和不是目标是两个神经元预测，也就是softmax，这里是一个预测 sigmoid。</p>
<p>在RPN 里面中心点的是回归一个相对于宽高的偏移量，即：
<img src="https://jzq6520.github.io/post-images/1572412895262.png" alt="">
原文中符号写错了：
<img src="https://jzq6520.github.io/post-images/1572414868247.png" alt="">
这样回归其实是有缺点的，这个公式回归出来的中心点是不受约束的，它的<strong>活动范围是全图</strong>，可以想象其实我们回归出t以后然后计算出的<strong>x,y的值可以是图像中的任意位置</strong>，<strong>在随机初始化的情况下，模型需要很长时间才能稳定地预测出合理的偏移量</strong>。</p>
<p>所以yolo v2就把关键点回归约束在网格之内，是相对于网格左上角的一个偏移量，这样归一化以后的偏移量就是在0到1之间。<strong>这样回归会更加稳定</strong>。
即<code>tx = bx - cx, ty = by - cy</code>，</p>
<p>距离如下图所示，那个<strong>对t做变换的参数不用管他</strong>，只是为了说明最好是加sigmoid的激活的， σ是sigmoid函数，也就是为了约束到0到1之间。实际上做数据的时候就是转换成t就可以了。
<img src="https://jzq6520.github.io/post-images/1572413565039.png" alt="">
宽高和RPN一样：
<img src="https://jzq6520.github.io/post-images/1572413551539.png" alt=""></p>
<h2 id="输入图像的大小">输入图像的大小</h2>
<p>为了使得目标占很大，或者是中心点在图的中间的目标在预测的适合更加准确，所以最好让输出的特征图的像素是个奇数，也就是说把图像隔成奇数个网格，这样就不会像偶数格一样把图像中心分成两部分了，大家可以想象一下3格和4格的区别，4格就把图的中心分割成一半了，如果目标中心点正好在中心区域的周围，那么偏移一点误差也很大了。</p>
<h2 id="其他一些加入的改进">其他一些加入的改进</h2>
<ol>
<li>加入了一个不同层特征的融合。concat连接</li>
<li>多尺度训练，Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: {320, 352, ..., 608}. Thus the smallest option is 320 × 320 and the largest is 608 × 608. We resize the network to that dimension and continue training.</li>
</ol>
<h2 id="网络">网络</h2>
<p><strong>主干网络使用了darknet-19，因为文章说vgg16虽然效果精度高，但是计算量大</strong>。darknet-19是使用1x1来减少计算量，就是bottleneck。网络如下：
<img src="https://jzq6520.github.io/post-images/1572415768870.png" alt=""></p>
<p>网上的图：
<img src="https://jzq6520.github.io/post-images/1572417862443.jpg" alt="">
这是在imagenet上进行预训练的结构，训练检测的时候去掉最后一个卷积层，然后增加一个3x3的卷积层和一个1x1的卷积层。原文：<em>We modify this network for detection by removing the last convolutional layer and instead adding on three 3 × 3 convolutional layers with 1024 ﬁlters each followed by a ﬁnal 1 × 1 convolutional layer with the number of outputs we need for detection。</em></p>
<p>并且，还增加了一个从最后的3×3×512层到第二个到最后一个卷积层的透传层，这样我们的模型可以使用细粒度的特征。原文（看的不是很清晰）：We also add a passthrough layer from the ﬁnal 3 × 3 × 512 layer to the second to last convolutional layer so that our model can use ﬁne grain features.</p>
<p>anchor的个数也是通过实验选择出来的。最后选择了5性价比比较高，越多效果越好。
<img src="https://jzq6520.github.io/post-images/1572416170928.png" alt=""></p>
<h3 id="网络的预测输出">网络的预测输出</h3>
<p>其实每个anchor都是预测：4个（坐标值）+1个（置信度）+ n个类别：
<img src="https://jzq6520.github.io/post-images/1572419086667.png" alt=""></p>
<p>对比YOLO1的输出张量，YOLO2的主要变化就是会输出5个先验框，且每个先验框都会尝试预测一个对象。输出的 <code>13*13*5*25</code> 张量中，25维向量包含 20个对象的分类概率+4个边框坐标+1个边框置信度。</p>
<h2 id="损失函数">损失函数</h2>
<p>？？？？？？what？
文章中居然没说用什么loss？？？， 难道用的是smooth L1，还是yolo中的loss？？
<strong>应该是结合吧，既然预测宽和高用的是RPN的，预测中心点用的是yolo，预测有没有目标也是yolo的。那么应该就是结合一下了，宽高用smooth L1， 中心点用均方差，有没有目标（置信度）用均方差</strong>以上只是猜测，具体看代码。</p>
<p>重温yolo v2 - stone的文章 - 知乎https://zhuanlan.zhihu.com/p/40659490 中的图：
<img src="https://jzq6520.github.io/post-images/1572420200314.jpg" alt="">
<img src="https://jzq6520.github.io/post-images/1572420615339.png" alt="">
<img src="https://jzq6520.github.io/post-images/1572420619180.png" alt=""></p>
<p><strong>卧槽 差点被坑了，文章中是把网络输出换算成x，y，w，h之后才进行计算loss的</strong>。<strong>不是直接回归t。RPN中是回归t的</strong>。</p>
<h2 id="训练参数学习率">训练参数，学习率</h2>
<ul>
<li>学习率：train the network for 160 epochs with a starting learning rate of 10−3 , dividing it by 10 at 60 and 90 epochs.</li>
<li>sgd：We use a weight decay of 0.0005 and momentum of 0.9.</li>
<li>数据增强：We use a similar data augmentation to YOLO and SSD with random crops, color shifting, etc</li>
</ul>
<h2 id="实验结果">实验结果</h2>
<p>分辨率越高效果越好：
<img src="https://jzq6520.github.io/post-images/1572414966868.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1572421391035.png" alt=""></p>
<h2 id="引用">引用</h2>
<ul>
<li>YOLOv2 / YOLO9000 深入理解 - X-猪的文章 - 知乎https://zhuanlan.zhihu.com/p/47575929</li>
<li><strong>重温yolo v2</strong> - stone的文章 - 知乎https://zhuanlan.zhihu.com/p/40659490</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读YOLO目标检测系列 v1]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-yolo-mu-biao-jian-ce-xi-lie-v1</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-yolo-mu-biao-jian-ce-xi-lie-v1">
        </link>
        <updated>2019-10-29T07:46:45.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>将对象检测定义为一个<strong>回归问题</strong>，回归到空间分离的边界框和相关的类概率。</li>
<li>与最先进的检测系统相比，YOLO会产生更多的定位错误，但不太可能预测背景上的误报<em>less likely to predict false positives on background</em>（假阳少）</li>
<li>都看做一个回归问题，所以不需要复杂的pipeline。</li>
<li>titan x gpu实现每秒150帧。</li>
<li>yolo是看<strong>整张图片</strong>进行预测的，相对来说区域建议网络第二阶段是是看roi。</li>
<li>yolo的背景的假阳少，因为他可以看到更大的信息（现在Faster R-CNN的第一个RPN模块其实也是看整个上下文的，这里对比的只是Fast R-CNN）。原文：<em>Fast R-CNN, a top detection method， mistakes back-ground patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.</em></li>
</ul>
<h2 id="如何检测">如何检测</h2>
<ol>
<li>将图片分成SxS个格子；</li>
<li>如果目标落在哪个格子上，哪个格子就负责整个目标。简单的说就是做数据的时候，这个格子的class label是1，在yolo里面是使用置信度来计算的。</li>
</ol>
<p>有些博客不懂乱说，例如知乎上的一篇文章，上来就说最后一层的featuremap对应的就是前面分好的网格里面的格子的特征，可见对感受野的概念不是很理解，其实这里的想法和Faster R-CNN里面的概念比较像，anchor其实也是在原图上滑动的呀，只是说这里没有anchor，而是这里的featuremap上的像素对应的是一个格子，也不能说是格子，因为格子的作用和anchor也是非常类似的，<strong>作用都是来进行分配任务，教网络怎么预测，网络学的是什么，那么预测的也是什么了</strong>。只是说这里是根据中心点是不是落在网络里面来选择正样本，而RPN里面是根据和anchor的IOU的大小来规定哪个是正样本哪个是负样本。</p>
<p>只是这里说的比较明确，说是把原图分割成最后一层feature map大小的个数，feature map是多少个像素，那么就分成几个格子。</p>
<p>只是说<strong>Faster R-CNN里面好像没有明确提到anchor的中心点在原图的哪个地方</strong>。其实这也是我们规定哪个点就是哪个点，<strong>但是按照感受野的概念，感受野中心的区域的权重往往会高一点</strong>，所以anchor的中心点可以设置成格子的中心。所以anchor的中心点在原图的坐标经过换算一下也是很简单的。应该就是<code>(x*stride + stride/2, y*stride + stride/2)</code>,这里的x和y表示在feature map上的坐标，计算出来的表示原图上的坐标，这也是我自己推测的（<strong>代码还没看过</strong>）。</p>
<h2 id="定义label">定义label</h2>
<p>每个格子预测<strong>坐标（x,y,w,h）+ 置信度（是否有目标，有、没有）+ 类别（class）</strong>：
* <strong>置信度=Pr(Object)+ IOU</strong>，这是论文里面说的，iou论文里面是说预测的和真实的。但是没有预测哪里来的iou，<strong>所以做数据的是置信度就是有目标是1，没有目标是0</strong>，不用多想这里的置信度就是有没有目标，就是0和1。
* 坐标：宽和高是<strong>相对于图像</strong>大小的，中心点是<strong>相对于格子</strong>的的。
* 类别：是哪个类别</p>
<p>置信度和坐标是绑定在一起的，类别是独立的。例如一个格子检测两个目标，那么就有两组坐标和对应的置信度，但是类别可以是有20个。<em>但是这样有个问题，就是如果两个框都是有目标，那么怎么知道哪个框是哪个类别的目标呢？</em> 是不是这里的意思就是一个格子就只能预测一个类别，但是可以有两个包围框预测，最后应该是看哪个包围框更准（置信度高，即是目标的概率高）就选哪个？确实和我想的一样，<strong>两个包围框都是预测一个类别的（见引用）</strong>。如图所示：
<img src="https://jzq6520.github.io/post-images/1572353048785.png" alt=""></p>
<p>Fast R-CNN最后只是有一个类别和坐标，但是他的类别里面是包含背景的，yolo的类别里面是不包含背景。</p>
<p>下面这幅图终于看懂了，Class probability map确实只是class，但这里只是表示这个格子预测的目标是哪个类（就是目标中心对于的框的目标），下面这个图片的Class probability map画的应该是预测出来的，那么做数据的时候是怎么做的。。。如果没有目标那么类别是什么？即使不计算loss那也总要填一个值吧。这里好像又没有背景这个类别。
这个应该是预测时候的示意图。
<img src="https://jzq6520.github.io/post-images/1572338771015.png" alt=""></p>
<h2 id="训练的时候">训练的时候</h2>
<ul>
<li>训练的时候只预测一个目标，选择iou高的。原文：<em>YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth.</em></li>
</ul>
<h2 id="损失函数">损失函数</h2>
<p>通常没有目标的区域比较多，而有目标的区域少，所以<strong>坐标的回归</strong>在loss里面占的比重很少，所以为了<strong>减少这种不平衡</strong>，将<strong>非目标的置信度回归的权重减少</strong>。
<img src="https://jzq6520.github.io/post-images/1572341526648.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1572341047772.png" alt=""></p>
<p>这个长的像1的系数表示有目标的适合是1，没有的适合是0。那头上写的是noobj的时候就反一下。这里大写的C表示confidence置信度。p那个表示类别。计算不是目标的置信度的适合权重设置为0.5 。</p>
<p>根号的作用：
<img src="https://jzq6520.github.io/post-images/1572352978309.png" alt=""></p>
<h2 id="缺点">缺点</h2>
<ul>
<li>不能同时检测两个中心点在同一个网格的物体。</li>
<li>对于不常见的长宽比的物体效果较差</li>
<li>对于多尺度效果较差</li>
</ul>
<h2 id="引用">引用</h2>
<ul>
<li>图解YOLO - 晓雷的文章 - 知乎 https://zhuanlan.zhihu.com/p/24916786</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读Fast R-CNN的ROI pooling]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-fast-r-cnn-ji-roi-pooling</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-fast-r-cnn-ji-roi-pooling">
        </link>
        <updated>2019-10-29T03:59:07.000Z</updated>
        <content type="html"><![CDATA[<p>Fast R-CNN主要是使用了一个ROI Pooling操作来对候选区域进行分类和包围框的微调。
这里主要说明一下ROI pooling是怎么做的，主要包括：</p>
<ol>
<li>原图上的ROI坐标如何映射到feature map上？</li>
<li>ROI Pooling是如何做？</li>
<li>ROI Pooling的梯度反向传播是怎么做的？</li>
<li>那么多的ROI Pooling做完以后是怎么进入到全连接层进行训练的？</li>
<li>正负样本怎么制作？</li>
<li>loss怎么计算？</li>
<li>回归的数值是什么？</li>
</ol>
<p>以上就是我这篇文章看下来的疑问或文章中的重点，然后查找资料和阅读相关论文来寻找答案。
下面一个个来说明。</p>
<h2 id="1-原图上的roi坐标如何映射到feature-map上">1 原图上的ROI坐标如何映射到feature map上？</h2>
<p>这个问题相对简单，可以这样理解，最后一层的feature map是网络多次下采样得到的，所以只要对原图上的坐标进行相同倍数的缩放即可。vgg16最后一个特征图是经过4次下采样，即缩小16倍得到的，所以坐标<code>x' = [x/下采样倍数] + 1</code> 中括号表示向下取整，vgg16的下采样倍数就是16。这里为什么要+1还不清楚。</p>
<h2 id="2-roi-pooling是如何做">2 ROI Pooling是如何做？</h2>
<p>因为物体有大有小，所以物体的ROI映射到特征图上以后也是有大有小 尺寸不一样，所以如果想输入到全连接层进行分类，那么就必须要同样的神经元个数，因为全连接的输入大小是固定的。</p>
<p>所以这里使用了一个max pooling方式将任意大小的roi对应的featuremap（这里的对应的roi featuremap只是最后输出的featuremap中切下的一小块）弄到一个尺度，比如文章中就是都弄到了7x7 。</p>
<p>其实很简单，就是将roi feature map隔成7x7的格子，然后每个格子里取最大的数（max pooling) 。如下图所示，图中的黑框就是原图中的roi在featuremap中对应的特征区域：
<img src="https://jzq6520.github.io/post-images/1572328585352.png" alt=""></p>
<h2 id="3-roi-pooling的梯度反向传播是怎么做的">3 ROI Pooling的梯度反向传播是怎么做的？</h2>
<p>这个梯度计算在文章中的公式写的很难看懂，但是看了其他人的<a href="https://zhuanlan.zhihu.com/p/30368989">博客</a>解释就比较清晰了，其实还是很简单的。</p>
<p>首先，了解一下max pooling是怎么反向传播的，其实就是max的区域有梯度，为1。
<img src="https://jzq6520.github.io/post-images/1572328987720.jpg" alt="">
<img src="https://jzq6520.github.io/post-images/1572329086778.png" alt="">
有了上面的了解，再来看ROI pooling，其中不一样的是<strong>特征图中的一个像素</strong>可能是多个感兴趣区域的重合部分。例如：
<img src="https://jzq6520.github.io/post-images/1572329164506.jpg" alt=""></p>
<p>那么这个个像素的位置的梯度就累积就可以了，比如假设3个roi pooling都用到了这个像素，那么梯度就是3了。</p>
<h2 id="4-那么多的roi-pooling做完以后是怎么进入到全连接层进行训练的">4 那么多的ROI Pooling做完以后是怎么进入到全连接层进行训练的？</h2>
<p>这一点细节其实文章中没提到，但是我想文章中是说到了每次选择2张图片，然后每张图片中选择64个roi进行训练，那么可以看出最后预测的全连接层的batch size是128，其实可以把roi pooling去掉，那么后面的网络其实就是一个输入一个7x7个神经元个数的全连接神经网络，batchsize为128。</p>
<p>所以这里是<strong>对选择的128个区域做ROI Pooling，然后把128个特征作为batchsize的形式传到网络中训练的</strong>。可以看下图：
<img src="https://jzq6520.github.io/post-images/1572329514254.png" alt=""></p>
<h2 id="5-正负样本怎么制作">5 正负样本怎么制作？</h2>
<p>说到训练肯定是要去看怎么做数据，和选择正负样本的。</p>
<ul>
<li>正样本：IOU 大于等于0.5；</li>
<li>负样本：IOU在<code>[0.1, 0.5)</code>区间。</li>
</ul>
<p>正样本25%左右。</p>
<h2 id="6-loss怎么计算">6 loss怎么计算？</h2>
<p>loss也包括两部，分类+坐标回归。</p>
<ul>
<li>分类采用log loss，也就是交叉熵；</li>
<li>坐标回归采用smooth L1，当回归的数值非常大的时候，如果用L2loss就要非常小心，要防止梯度爆炸，因为抛物线两边是梯度越来越大的，L1是不变的。</li>
</ul>
<h3 id="smooth-l1">smooth L1</h3>
<p><img src="https://jzq6520.github.io/post-images/1572330028401.png" alt="">
<img src="https://jzq6520.github.io/post-images/1572330081867.png" alt=""></p>
<h2 id="7回归的数值是什么">7回归的数值是什么？</h2>
<p>现在检测的话都是回归一个相对坐标。
这里的话<strong>参照边框</strong>就是<strong>proposal的框</strong>了，而不是anchor的框。</p>
<p>回归的label是这么制作的：
<img src="https://jzq6520.github.io/post-images/1572330297928.png" alt=""></p>
<p><strong>我们实际回归的就是这个t，然后再反过来求一下坐标和宽高。R-CNN论文里面写的有点复杂，其实就是回归一t就对了</strong>。</p>
<h2 id="引用">引用</h2>
<ol>
<li>roi pooling反向传播和roi在featuremap上的位置：https://zhuanlan.zhihu.com/p/30368989</li>
<li>roi pooling的操作细节：https://deepsense.ai/region-of-interest-pooling-explained/</li>
<li>R-CNN</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读Faster R-CNN即region proposal network(RPN)区域建议网络]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-faster-r-cnn</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-faster-r-cnn">
        </link>
        <updated>2019-10-28T02:35:49.000Z</updated>
        <content type="html"><![CDATA[<p>Faster R-CNN主要是讲区域建议网络，而ROI pooling部分还是在Fast R-CNN里面介绍的。</p>
<h2 id="rpn-region-proposal-network">RPN region proposal network</h2>
<ul>
<li>利用网络产生region proposal（区域建立，候选区域），在网络的最后一层增加几个卷积层在一个<strong>固定的网格</strong>中<strong>回归包围框和目标分数</strong>来构建RPN。</li>
<li>交替训练region proposal和object detection。region proposal和object detection的基础特征是共享的，所以效率非常高。</li>
<li>RPN使用了anchor，所以不需要利用图像金字塔或者特征金字塔来进行多尺度的目标检测，使用了anchor以后只需要在一张特征图上进行检测出不同大小的目标。如果使用图像金字塔就需要缩放，利用特征金字塔就需要在不同尺度的特征图上检测对应大小的目标（FPN其实也证明了特征金字塔是有效的，但是一直特征图上检测多个尺度的目标还是很必要的），如下图c所示就是anchor机制，图a是图像金字塔，b是特征金字塔处理的效果。</li>
<li><img src="https://jzq6520.github.io/post-images/1572231445375.png" alt=""></li>
<li>RPN的<strong>输入</strong>是任意形状的图像，<strong>输出</strong>是一个长方形的目标建议区域和目标置信度。从这里可以看到其实<strong>单单一个RPN就可以看做是一个完整的end-to-end网络</strong>了。只是说后面的二次分类是共用之前backbone的特征图的。</li>
<li>文章中的 top一般是指网络的深层。down是指浅层。计算感受野从输入到输出的计算公式是 ：
<ul>
<li>rf = 上一层rf + (kernel size - 1)* stride * dilated rate， 这个是递推法，这里的 stride是之前的所以stride累乘得到。</li>
<li>这里有个重点就是有stride的时候下一次才会乘stride，例如做max pooling的时候是不立即计算stride的，而是下一步加上stride计算。</li>
<li>还有个是stride是累乘的。</li>
<li>参考链接：
<ul>
<li>http://zike.io/posts/calculate-receptive-field-for-vgg-16/</li>
<li>https://blog.csdn.net/Fanniexia/article/details/102512267</li>
</ul>
</li>
</ul>
</li>
<li>为了产生region proposal，用一个<strong>小的网络进行卷积</strong>，文章中使用了<strong>一个3x3的卷积层作为小的网络</strong>如下图所示，然后文章中计算出来做了3x3的卷积以后的感受野是228，因为vgg16最后一个特征图（不算最后的maxpooling）的感受野是196，根据上面给出的公式，那最后<strong>region proposal的感受野就是</strong> <code>196+(3-1)*16=228</code> 。这里的16是之前做了4次pooling，stride累乘就是16。
<img src="https://jzq6520.github.io/post-images/1572242265100.png" alt=""></li>
</ul>
<h3 id="anchor">anchor</h3>
<ul>
<li><strong>anchor</strong>
<ul>
<li>其实这一块<strong>不用想的特别复杂</strong>，他虽然画了一个anchor再上面，这里<strong>画出来</strong>只是表示最终anchor的特征是在这里提取的，<strong>重点不是在这个anchor上</strong>，anchor只是一个虚拟的大小的宽，用来给坐标回归提供一个相对位置的，或者<strong>说是一个参考的坐标系（类似于这个作用）</strong>，但是其实做数据还是按照规定在固定的网格中做数据就可以了，对应的<strong>坐标和置信度，主要的还是这两个分支</strong>。</li>
<li>anchor的个数限制了feature map上每个点可以预测几个region proposal。这个其实就像平时分类一样，我可以最后输出4个神经元来预测四个类别，那我也可以输出8个神经元来预测8个类别。一个道理，这里假设规定了k个anchor，那么我们就预测k个框就好了。</li>
<li>这里的feature map上的点可以怎么理解呢，就是说每个feature map的点是预测k组（这里的组是指坐标+置信度）目标，就像<strong>平时我们是把特征图拉直或者是做过global avg pooling，因为分类是最终分出几个类别，是根据整体的信息来的</strong> ，这里只是不再进行降维了，而是每个点我出来预测一个东西。这样其实有个好处就是，<strong>特征图上的每个点对应的感受野其实是原图上的一块区域的</strong>，而且通常来说感受野是<strong>输入中越靠感受野中间的元素对特征的贡献越大</strong>的，如下图所示，出自论文understanding-the-effective-receptive-field-in-deep-convolutional-neural-networks。所以这样来预测其实是符合计算的。
<img src="https://jzq6520.github.io/post-images/1572243550140.png" alt=""></li>
<li>我靠之前怎么想不到，<strong>这个其实就是和分割差不多啊</strong>，分割的输出也是不进行降维的，也是<strong>一个像素其实对应一块区域的感受野的</strong>。</li>
</ul>
</li>
<li>文章中老是说滑动窗口（sliding windows on the feature map）<strong>太令人误会</strong>了，其实就是<strong>加了一个卷积</strong>层然后再进行预测宽和置信度。</li>
<li>The design of multiscale anchors is a key component for sharing features <strong>without extra cost for addressing scales</strong>.</li>
</ul>
<h3 id="参数量">参数量</h3>
<ul>
<li>region proposal小网络（文章中说小网络，其<strong>实就是1个分支，也就是1个卷积层</strong>）提到了Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and <strong>512-d</strong> for VGG, with ReLU [33] following).和our output layer has 2.8 × 10 4 parameters (<strong>512</strong> × (4 + 2) × 9 for VGG-16)，这里的<strong>512是指这个分支卷积层的channel数</strong>，就像隐藏层的参数个数一样，他这边的参数量应该不是所有的feature map上的点，因为输入图像大小不一样，featuremap大小也是不一样的。所有这里说的参数是featuremap上<strong>一个点的region proposal的参数量</strong>，所有文章又补充说明了一下，如下图：
<img src="https://jzq6520.github.io/post-images/1572244888520.png" alt=""></li>
</ul>
<h2 id="正负样本选择">正负样本选择</h2>
<p>二分类，是目标或者不是目标。<strong>每个anchor都有一个坐标和一个类别</strong>。</p>
<p><strong>正样本选择</strong>，有两种anchor都是算作正样本：</p>
<ol>
<li>和ground truth的 IOU最高的anchor；</li>
<li>和ground truth的 IOU大于0.7的anchor；</li>
</ol>
<p>注：<strong>通常，第二个条件足以确定阳性样本;但我们仍然采用第一种情况，因为在少数情况下，第二种情况可能找不到阳性样本</strong>。</p>
<p>负样本选择：</p>
<ol>
<li>和ground truth的 IOU小于0.3的算负样本的anchor。</li>
</ol>
<p>这样哪个anchor是正样本和负样本分配好了就方便了，这样数据做出来就是每个anchor的分类的label就有了，1或者0 。</p>
<h2 id="rpn的损失函数">RPN的损失函数</h2>
<p>损失函数如下图所示：
<img src="https://jzq6520.github.io/post-images/1572256564654.png" alt=""></p>
<p>可以看出又两个损失函数组成，一个是分类loss，另外一个是坐标回归的loss：</p>
<ul>
<li>带星号的表示ground truth,即label，1表示正样本，0表示负样本。</li>
<li><strong>分类</strong>为log loss，也就是交叉熵。</li>
<li><strong>回归</strong>为smooth L1，从公式可以看出乘以一个p星，说明只计算正样本的损失，负样本是不计算损失的。</li>
<li>乘以一个N分之一，这个是做一个归一化，分类的N为batch-size的大小，回归的N是batch-size乘以anchor数（即所有anchor的数量），这里<em>可以看到回归是除以了所以anchor的数量，而分类的只除以batch-size</em>。然后这里的lambda参数又是来平衡两个loss，这里论文中设置了一个10效果最好，<strong>那么傻逼了</strong>，因为文章anchor是9，那么乘10再除9 来平衡，tm的为什么不直接除batch-size，干嘛还乘一个anchor个数来平衡。</li>
<li>坐标回归的loss：
<img src="https://jzq6520.github.io/post-images/1572271188684.png" alt="">
<strong>可以看出这里的label是经过转换得到的</strong>，所以预测出来的结果还需要经过变换，转换到原来的坐标和长宽。可以把回归看做是和最接近的anchor计算相对位置的，因为做label（正负样本）的时候就是选择IOU(接近)大的anchor的。</li>
</ul>
<h2 id="训练rpn">训练RPN</h2>
<ul>
<li>loss: SGD</li>
<li>batch size=256,由于单个图像会存在非常多的正样本和负样本anchor，但是负样本总是会比正样本多，所以为了防止偏向于负样本，所以每次选择256个anchor进行计算loss作为mini-batch，这样让正负样本接近1：1，如果正样本少于128，则用负样本填充。</li>
<li>backbone使用迁移ImageNet预训练参数，其他层使用高斯分布随机初始化。</li>
<li>初始学习率0.001训练60k次，然后0.0001训练20k次。</li>
<li>momentum = 0.9 ， weight decay = 0.0005</li>
<li>交替训练RPN网络和Fast R-CNN是交替训练的。原文：<em>Alternating training. In this solution, we ﬁrst train RPN, and use the proposals to train Fast R-CNN. The network tuned by Fast R-CNN is then used to initialize RPN, and this process is iterated. This is the solution that is used in all experiments in this paper.</em></li>
</ul>
<h2 id="实现细节">实现细节</h2>
<ul>
<li>Multi-scale feature extraction (using an image pyramid) may improve accuracy but does not exhibit a good speed-accuracy trade-off</li>
<li>total stride for both ZF and VGG nets on the last convolutional layer is 16 pixels，Even such a large stride provides good results, though accuracy may be further improved with a smaller stride.</li>
<li>For anchors, we use 3 scales with box areas of 128 2 , 256 2 , and 512 2 pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1.</li>
<li>We note that our algorithm allows predictions that are larger than the underlying receptive ﬁeld. Such predictions are not impossible—one may still roughly infer the extent of an object if only the middle of the object is visible.看见局部也可能认识整个物体，但是边缘怎么解释呢，rpn怎么知道物体多大，所以其实感受野还是大点好。</li>
<li>训练的适合超过边框的anchor不计算loss，什么叫超过边框呢？因为anchor是以最后一层特征图映射回到原图的点作为中心点的，所以边框是有可能超过图像的。这个以前困扰我很多，<strong>因为很多博客都画了超过图像大小的anchor，所以很困扰，其实那样画是有问题的</strong>，明明是不计算loss的，所以就不用设计无论怎么样都会超过图像的边框的anchor。测试的适合预测出来这种，就阶段到边缘就可以了，不是丢掉。</li>
<li>一些RPN建议彼此高度重叠。为了减少冗余，我们根据建议区域的cls评分，对其采用非最大抑制(non-maximum suppression, NMS)。我们将NMS的IoU阈值设为0.7，这样每个图像就有<strong>大约2000个</strong>建议区域。</li>
<li>在NMS之后，我们使用排名前n的建议区域进行检测。下面，我们使用2000个RPN建议来训练Fast R-CNN，但是在测试时评估不同数量的建议。</li>
</ul>
<h2 id="整体的网络结构就是这样">整体的网络结构就是这样</h2>
<p>ROI Pooling结构可以仔细看Fast R-CNN，这篇文章没有仔细介绍。</p>
<p><img src="https://jzq6520.github.io/post-images/1572275302379.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-数据增强-Learning to Generate Synthetic Data via Compositing]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-shu-ju-zeng-qiang-learning-to-generate-synthetic-data-via-compositing</id>
        <link href="https://jzq6520.github.io/post/cv-paper-shu-ju-zeng-qiang-learning-to-generate-synthetic-data-via-compositing">
        </link>
        <updated>2019-10-24T08:43:36.000Z</updated>
        <content type="html"><![CDATA[<p>20191024，今天是程序员日，天气不错。</p>
<p>这是一篇做数据增强的文章。</p>
<h2 id="1-简介">1 简介</h2>
<p>传统的图像增强方法一般是对图像做一些转换，另外一种较新的方法就是合成数据。</p>
<p>主要思想是合成一些图片，加入到训练中，从而提高检测和分类的精度。这里的合成是指将目标和背景结合。例如我把另外一张图片里面的人抠出来，放到另外一个背景图片里面去，就好像本来是在室内的人，抠出来放到一个室外的照片里面去。这样做的好处是增加了图片的多样性。增加一个上下文的多样性。</p>
<p>例如，我们真实世界里面的人是在什么地方都会出现的，马路上、车里、草坪上、床上，都会有，但是我们作为训练集的图像中可能只出现了一个马路上的人的照片，那么当测试集中出现了一个人在室内的照片，可能模型就不认识。</p>
<p>由于我们的数据只是真实世界分布的一个采样，所以数据永远是不够了。这就涉及到了泛化能力，一个好的模型就具有更好的泛化能力，但是有时候数据不够往往是一些其他的网络上的技巧（dropout这种）弥补不了的。</p>
<p>所以这篇文章做的就是生成一些样本来弥补数据过少，数据多样性小的问题。这篇文章的主要一点是提出了我们生成的样本要<strong>符合真实世界的分布</strong>，例如一个人不可能就是没有借助的飞在天上。所以文章的框架增加了一个<strong>判别器</strong>来判断生成的数据是不是符合真实世界的分布，也就是说是不是符合常理的数据(真不真实)。这样来大家也都可以看出，这其实就是一个GAN网络。</p>
<p>这篇文章是基于<strong>Cut-Paste-Learn</strong>是基于这篇文章之上的一篇文章。主要就是加了一个判别器来判断生成的数据是否是符合真实世界的分布。<strong>当然因为是通过GAN训练，那么生成器生成的就是以假乱真的数据了，而不是有监督的学习</strong>。因为作者说，如果引入了一些不符合真实场景的数据那么会造成<strong>过拟合</strong>。简单的理解就是这部分不符合常理的数据其实就是噪声，大家都知道噪声是会给模型带来负担的。</p>
<p>一般来说生成数据的方式主要有三种：1)图像合成，2)对抗生成，3)渲染。本文应该是结合了图像合成和对抗这两种方法。</p>
<h2 id="2-数据合成">2 数据合成</h2>
<h3 id="21-合成网络synthesizer-network">2.1 合成网络Synthesizer Network</h3>
<p>文章中的核心思想就是这个网络。里面还有一个小技巧来<strong>避免过拟合这种合成的数据</strong>。</p>
<p>首先，合成网络<strong>不是</strong>我们想的那样输入两个图形然后自动输出一个图片，其实这个网络是输入两张图片然后预测一个2D affine transformations（二维刚性变换）的矩阵，这个矩阵是一个<strong>维度为6的向量</strong>，如下图所示（文章中没给，见Spatial Transformer Networks论文）：
<img src="https://jzq6520.github.io/post-images/1571909222916.png" alt="">
<img src="https://jzq6520.github.io/post-images/1571909211491.png" alt=""></p>
<p>这个网络结构如下：
<img src="https://jzq6520.github.io/post-images/1571910057977.png" alt=""></p>
<p>感觉文章的图片画的不是很清晰，且参数共享那一块容易给人造成误解，所以重新画了一幅图，网络有部分是参数共享，但是又有部分是不进行参数共享的，结构和孪生网络相似。最终预测的就是一个变换矩阵的参数。具体设置如下：
<img src="https://jzq6520.github.io/post-images/1571910821669.png" alt=""></p>
<h3 id="22-合成数据">2.2 合成数据</h3>
<p>那么怎么将前景和背景合为一体呢呢，其实和Cut-Paste-Learn一样，把背景扣掉一块然后将前景放进去，其实<strong>说白了就是把前景贴到背景上</strong>，但是文章中给了一个<em>故弄玄虚的公式</em>说是叫做alpha-blending，其实就是贴图的方式。<strong>但是多了对图像做一个仿射变换以后叠加</strong>，再加上有一个判别网络来保证合成的网络是真实的。</p>
<p>如下图所示，上一行就是不真实（不符合真实世界的分布）的数据，下一行就是合成的符合真实世界的分布的数据：
<img src="https://jzq6520.github.io/post-images/1571911885967.png" alt=""></p>
<h3 id="23-防止过拟合贴图数据">2.3 防止过拟合贴图数据</h3>
<p>为了防止过拟合这些合成的数据，本身将一些和目标形状一样的背景图像贴到一些训练的图像中去（其实就是目标的mask乘以一些背景区域，这样就取出和目标一样形状的背景图片，然后贴到其他图片中去）。如图所示：
<img src="https://jzq6520.github.io/post-images/1571910704686.png" alt=""></p>
<h2 id="3-loss">3 loss</h2>
<p>判别网络loss，就是个交叉熵：
<img src="https://jzq6520.github.io/post-images/1571910918644.png" alt=""></p>
<p>GAN的 loss：
<img src="https://jzq6520.github.io/post-images/1571910951399.png" alt=""></p>
<p>生成仿射变换的6维向量的适合是没有监督的，是通过对抗学习来学习合成网络的参数的。</p>
<h2 id="4-result">4 result</h2>
<p>只贴了少部分实验数据，具体可以看论文。
<img src="https://jzq6520.github.io/post-images/1571911142039.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571911157597.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571911183410.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571911212706.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571911278459.png" alt=""></p>
<h2 id="引用">引用</h2>
<ul>
<li>Learning to Generate Synthetic Data via Compositing</li>
<li>Cut, paste and learn: Surprisingly easy synthesis for instance detection.</li>
<li>Spatial Transformer Networks</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-增量学习-Large Scale Incremental Learning]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-zeng-liang-xue-xi-large-scale-incremental-learning</id>
        <link href="https://jzq6520.github.io/post/cv-paper-zeng-liang-xue-xi-large-scale-incremental-learning">
        </link>
        <updated>2019-10-21T09:54:46.000Z</updated>
        <content type="html"><![CDATA[<p>就简单的说明一下好了，首先是使用蒸馏学习，然后再利用验证集来学习一个简单的线性变换 ax + b 来减少偏差。</p>
<p>这里是把验证集也拿过来训练了，虽然只是学习一个简单的线性变换，因为这个线性变换只有两个参数，所以需要的数据量非常少，虽然这个变换很简单，但是非常有效的提高精度。</p>
<p>文章中说的偏差指的是增量学习的精度和全部数据一起训练的精度的差距。</p>
<h2 id="1-什么是偏差">1 什么是偏差</h2>
<p>偏差就是增量学习的精度和用全部数据训练的精度的差。
<img src="https://jzq6520.github.io/post-images/1571711000642.png" alt=""></p>
<h2 id="2-网络">2 网络</h2>
<p><img src="https://jzq6520.github.io/post-images/1571710167211.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571710183774.png" alt=""></p>
<p>这里蒸馏学习的一个loss是logit之间的loss（即没有softmax激活的）。</p>
<p>结合蒸馏学习和普通分类loss来进行学习。</p>
<h2 id="3-loss">3 loss</h2>
<p>蒸馏学习的时候，新数据和老数据的蒸馏loss都是要计算的，无论是新的类别还是老的类别，这里其实是计算一个特征提取器的loss，让新的特征提取器提取的特征和老的一样。</p>
<p><img src="https://jzq6520.github.io/post-images/1571710427789.png" alt=""></p>
<h2 id="4-偏差矫正层">4 偏差矫正层</h2>
<p><img src="https://jzq6520.github.io/post-images/1571710889389.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-增量学习incremental learning]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-zeng-liang-xue-xi-incremental-learning</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-zeng-liang-xue-xi-incremental-learning">
        </link>
        <updated>2019-10-21T09:49:24.000Z</updated>
        <content type="html"><![CDATA[<p>又是一种深度学习的学习策略。</p>
<p>自然学习（Natural learning）系统本质上是<strong>渐进的</strong>，新知识是随着时间的推移而不断学习的，而现有的知识是保持不变的。现实世界中的许多计算机视觉应用程序都需要<strong>增量学习</strong>能力。例如，人脸识别系统应该能够在不忘记已学过的面孔的情况下添加新面孔。然而，大多数深度学习方法都存在灾难性的遗忘——<strong>当无法获得过去的数据时</strong>，性能会显著下降。</p>
<p><strong>旧类数据的缺失带来了两个挑战</strong>:(a)维护旧类的分类性能; (b)平衡旧类和新类。<strong>知识蒸馏</strong>已被用来有效地解决前一个挑战。最近的研究也表明，从旧的类中<strong>选择几个样本(抽样)</strong> 可以缓解不平衡问题。这些方法在小数据集上运行良好。然而，<strong>当类的数量变得很大</strong>(例如，成千上万个类)时，它们的性能会显著下降。图1显示了以非增量分类器为参考的这些最先进算法的性能退化情况。当类的数量从100增加到1000时，iCaRL和EEIL都有更多的精度下降。</p>
<p>为什么处理大量的类进行增量学习更具挑战性? 我们认为这是由于两个因素的耦合。首先，<strong>训练数据不平衡</strong>。其次，随着类数量的增加，在不同的增量步骤中更<strong>可能出现类似的类</strong>(例如ImageNet中的多个dog类)。在数据不平衡的增量约束下，视觉上相似的类数量的增加尤其具有挑战性，因为类之间边界的<strong>小边界</strong>对数据不平衡过于敏感。边界被推到支持具有更多示例的类。</p>
<h2 id="引用">引用</h2>
<ul>
<li>Large Scale Incremental Learning</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-主动学习-Learning Loss for Active Learning]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-zhu-dong-xue-xi-learning-loss-for-active-learning</id>
        <link href="https://jzq6520.github.io/post/cv-paper-zhu-dong-xue-xi-learning-loss-for-active-learning">
        </link>
        <updated>2019-10-17T09:36:24.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-简介">1 简介</h2>
<p>主动学习，即在拥有少部分监督数据的情况下，主动的去选择一部分对训练模型<strong>有较大提高的</strong>未标注数据，然后对选择出来的数据进行标注，标注后加入到训练集进行训练。为什么这么做的，我们把数据都标注一下不行吗？当然可以，但是标注是要时间和金钱的，我们希望选择更有助于模型提高的数据先进行标注。如下图所示：
<img src="https://jzq6520.github.io/post-images/1571305200850.png" alt="主动学习流程图"></p>
<p>以往的研究结果表明，主动学习实际上降低了标注成本。最开始提出主动学习的方法是选择出不确信的数据进行标注。主动学习的核心思想是信息量最大的数据比随机选择的数据更有利于模型的改进。无论任务是什么、有多少任务以及体系结构有多复杂，深度网络都是通过最小化单个损失来学习的。</p>
<h2 id="2-学习loss">2 学习Loss</h2>
<p>这个方法可以归为不确定度方法，但不同之处在于<strong>它是根据输入内容来预测“损失”</strong>，而不是根据输出来统计估计不确定度。这类似于各种难例挖掘，因为他们认为损失较大的训练数据点对模型的改进意义重大</p>
<p>去学习损失，这是一个很有创新的想法，但有用。既然我们选择数据的时候没有标签（未打标签），那么是不是可以先选择出预测较差的数据进行打标签。这时候loss确实是一个可以用来做这种选择的东西。当loss较大的时候说明和真实标签的差异性很大，loss较小则差异较小。其实最终我们是要对未打标签的数据进行<strong>排序</strong>，先是模型较难预测的，然后再是较好预测的。</p>
<p>作者就很聪明，没有label那么我们可以去预测一个loss啊。利用损失来进行排序。其实这个loss的得到也是很悬，可能是有某个隐藏的特征空间进行映射，<strong>或者说这里说是loss，但是网络本身学习到的并不是loss呢，而是一个排序</strong>。但是文章这里就把这个学习到的东西叫做loss，<strong>其实不然，我觉得更侧重于一个排序的信息</strong>。这里的思想和一篇图像质量评价的论文很像，学习怎么去排序，见之前的图像评价论文RankIQA。</p>
<p>那么这个loss如何学习呢？第一时间我们可以想到的是用回归的方法学习，即MSE，利用我们训练分类器或者检测或则定位的loss来作为label（见图）。作者也是这么想的，但是实验下来效果很差，还不如没用来的好。<strong>所以作者解释是由于在梯度下降的时候loss也是下降的，所以学习loss是不稳定的，无法学习</strong>。所以作者想到了用一个<strong>排序的loss</strong>（也可以认为是<strong>比较大小的</strong>loss）来作为损失函数，这样即使作为label的loss一直在变，但是loss的相对大小是不会变的，难的样本的loss就是比容易样本的loss来的大（和RankIQA中的思想一毛一样）。</p>
<p>这里计算排序的loss和RankIQA中的思想一毛一样，但是网络架构不一样，RankIQA中是使用参数共享的两个网络进行排序，这里是将一个batch分为两部分。但是仔细想想是一毛一样的，都是同一个端到端的网络，只是说一个是分开的 一个是画到一块。</p>
<p>这里的排序loss（不行叫他学习loss的loss，因为容易混 还不太准确）：
<img src="https://jzq6520.github.io/post-images/1571306285061.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571306940422.png" alt=""></p>
<p>最终端到端loss：
<img src="https://jzq6520.github.io/post-images/1571307032943.png" alt=""></p>
<h2 id="3-网络结构">3 网络结构</h2>
<p><img src="https://jzq6520.github.io/post-images/1571307076883.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571307093582.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571307115680.png" alt=""></p>
<h2 id="4-结果">4 结果</h2>
<p>如果为标注数据多的时候，这个方法选择样本是这样的：首先对所有未标注的进行随机采样，然后在选择前k个最不确信的样本，这样可以避免都选择出相同类别的。因为可能会出现前k个最不确信的样本中大部分都是一个类别的。</p>
<p>这个主动学习的方法比之前的几个要好,learn loss表示现在这个方法：
<img src="https://jzq6520.github.io/post-images/1571307198541.png" alt=""></p>
<p>mse损失和排序损失的效果比较：
<img src="https://jzq6520.github.io/post-images/1571307238951.png" alt=""></p>
<h2 id="5-总结">5 总结</h2>
<p>一个是通过排序的损失，一个是mse损失。排序损失会更好。其实这里更重要的是得到一个损失的相对大小，而不是损失本身，因为最后的目的是根据损失对未标注的样本进行挑选，所以排序准确才是最终目的。</p>
]]></content>
    </entry>
</feed>