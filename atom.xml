<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jzq6520.github.io</id>
    <title>chuck</title>
    <updated>2019-10-22T02:21:34.164Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jzq6520.github.io"/>
    <link rel="self" href="https://jzq6520.github.io/atom.xml"/>
    <subtitle>天地不仁以万物为刍狗</subtitle>
    <logo>https://jzq6520.github.io/images/avatar.png</logo>
    <icon>https://jzq6520.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, chuck</rights>
    <entry>
        <title type="html"><![CDATA[CV-Paper-增量学习-Large Scale Incremental Learning]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-zeng-liang-xue-xi-large-scale-incremental-learning</id>
        <link href="https://jzq6520.github.io/post/cv-paper-zeng-liang-xue-xi-large-scale-incremental-learning">
        </link>
        <updated>2019-10-21T09:54:46.000Z</updated>
        <content type="html"><![CDATA[<p>就简单的说明一下好了，首先是使用蒸馏学习，然后再利用验证集来学习一个简单的线性变换 ax + b 来减少偏差。</p>
<p>这里是把验证集也拿过来训练了，虽然只是学习一个简单的线性变换，因为这个线性变换只有两个参数，所以需要的数据量非常少，虽然这个变换很简单，但是非常有效的提高精度。</p>
<p>文章中说的偏差指的是增量学习的精度和全部数据一起训练的精度的差距。</p>
<h2 id="网络">网络</h2>
<p><img src="https://jzq6520.github.io/post-images/1571710167211.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571710183774.png" alt=""></p>
<p>这里蒸馏学习的一个loss是logit之间的loss（即没有softmax激活的）。</p>
<p>结合蒸馏学习和普通分类loss来进行学习。</p>
<h2 id="loss">loss</h2>
<p>蒸馏学习的时候，新数据和老数据的蒸馏loss都是要计算的，无论是新的类别还是老的类别，这里其实是计算一个特征提取器的loss，让新的特征提取器提取的特征和老的一样。</p>
<p><img src="https://jzq6520.github.io/post-images/1571710427789.png" alt=""></p>
<h2 id="偏差矫正层">偏差矫正层</h2>
<p><img src="https://jzq6520.github.io/post-images/1571710889389.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-增量学习incremental learning]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-zeng-liang-xue-xi-incremental-learning</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-zeng-liang-xue-xi-incremental-learning">
        </link>
        <updated>2019-10-21T09:49:24.000Z</updated>
        <content type="html"><![CDATA[<p>又是一种深度学习的学习策略。</p>
<p>自然学习（Natural learning）系统本质上是<strong>渐进的</strong>，新知识是随着时间的推移而不断学习的，而现有的知识是保持不变的。现实世界中的许多计算机视觉应用程序都需要<strong>增量学习</strong>能力。例如，人脸识别系统应该能够在不忘记已学过的面孔的情况下添加新面孔。然而，大多数深度学习方法都存在灾难性的遗忘——<strong>当无法获得过去的数据时</strong>，性能会显著下降。</p>
<p><strong>旧类数据的缺失带来了两个挑战</strong>:(a)维护旧类的分类性能; (b)平衡旧类和新类。<strong>知识蒸馏</strong>已被用来有效地解决前一个挑战。最近的研究也表明，从旧的类中<strong>选择几个样本(抽样)</strong> 可以缓解不平衡问题。这些方法在小数据集上运行良好。然而，<strong>当类的数量变得很大</strong>(例如，成千上万个类)时，它们的性能会显著下降。图1显示了以非增量分类器为参考的这些最先进算法的性能退化情况。当类的数量从100增加到1000时，iCaRL和EEIL都有更多的精度下降。</p>
<p>为什么处理大量的类进行增量学习更具挑战性? 我们认为这是由于两个因素的耦合。首先，<strong>训练数据不平衡</strong>。其次，随着类数量的增加，在不同的增量步骤中更<strong>可能出现类似的类</strong>(例如ImageNet中的多个dog类)。在数据不平衡的增量约束下，视觉上相似的类数量的增加尤其具有挑战性，因为类之间边界的<strong>小边界</strong>对数据不平衡过于敏感。边界被推到支持具有更多示例的类。</p>
<h2 id="引用">引用</h2>
<ul>
<li>Large Scale Incremental Learning</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-主动学习-Learning Loss for Active Learning]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-zhu-dong-xue-xi-learning-loss-for-active-learning</id>
        <link href="https://jzq6520.github.io/post/cv-paper-zhu-dong-xue-xi-learning-loss-for-active-learning">
        </link>
        <updated>2019-10-17T09:36:24.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-简介">1 简介</h2>
<p>主动学习，即在拥有少部分监督数据的情况下，主动的去选择一部分对训练模型<strong>有较大提高的</strong>未标注数据，然后对选择出来的数据进行标注，标注后加入到训练集进行训练。为什么这么做的，我们把数据都标注一下不行吗？当然可以，但是标注是要时间和金钱的，我们希望选择更有助于模型提高的数据先进行标注。如下图所示：
<img src="https://jzq6520.github.io/post-images/1571305200850.png" alt="主动学习流程图"></p>
<p>以往的研究结果表明，主动学习实际上降低了标注成本。最开始提出主动学习的方法是选择出不确信的数据进行标注。主动学习的核心思想是信息量最大的数据比随机选择的数据更有利于模型的改进。无论任务是什么、有多少任务以及体系结构有多复杂，深度网络都是通过最小化单个损失来学习的。</p>
<h2 id="2-学习loss">2 学习Loss</h2>
<p>这个方法可以归为不确定度方法，但不同之处在于<strong>它是根据输入内容来预测“损失”</strong>，而不是根据输出来统计估计不确定度。这类似于各种难例挖掘，因为他们认为损失较大的训练数据点对模型的改进意义重大</p>
<p>去学习损失，这是一个很有创新的想法，但有用。既然我们选择数据的时候没有标签（未打标签），那么是不是可以先选择出预测较差的数据进行打标签。这时候loss确实是一个可以用来做这种选择的东西。当loss较大的时候说明和真实标签的差异性很大，loss较小则差异较小。其实最终我们是要对未打标签的数据进行<strong>排序</strong>，先是模型较难预测的，然后再是较好预测的。</p>
<p>作者就很聪明，没有label那么我们可以去预测一个loss啊。利用损失来进行排序。其实这个loss的得到也是很悬，可能是有某个隐藏的特征空间进行映射，<strong>或者说这里说是loss，但是网络本身学习到的并不是loss呢，而是一个排序</strong>。但是文章这里就把这个学习到的东西叫做loss，<strong>其实不然，我觉得更侧重于一个排序的信息</strong>。这里的思想和一篇图像质量评价的论文很像，学习怎么去排序，见之前的图像评价论文RankIQA。</p>
<p>那么这个loss如何学习呢？第一时间我们可以想到的是用回归的方法学习，即MSE，利用我们训练分类器或者检测或则定位的loss来作为label（见图）。作者也是这么想的，但是实验下来效果很差，还不如没用来的好。<strong>所以作者解释是由于在梯度下降的时候loss也是下降的，所以学习loss是不稳定的，无法学习</strong>。所以作者想到了用一个<strong>排序的loss</strong>（也可以认为是<strong>比较大小的</strong>loss）来作为损失函数，这样即使作为label的loss一直在变，但是loss的相对大小是不会变的，难的样本的loss就是比容易样本的loss来的大（和RankIQA中的思想一毛一样）。</p>
<p>这里计算排序的loss和RankIQA中的思想一毛一样，但是网络架构不一样，RankIQA中是使用参数共享的两个网络进行排序，这里是将一个batch分为两部分。但是仔细想想是一毛一样的，都是同一个端到端的网络，只是说一个是分开的 一个是画到一块。</p>
<p>这里的排序loss（不行叫他学习loss的loss，因为容易混 还不太准确）：
<img src="https://jzq6520.github.io/post-images/1571306285061.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571306940422.png" alt=""></p>
<p>最终端到端loss：
<img src="https://jzq6520.github.io/post-images/1571307032943.png" alt=""></p>
<h2 id="3-网络结构">3 网络结构</h2>
<p><img src="https://jzq6520.github.io/post-images/1571307076883.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571307093582.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1571307115680.png" alt=""></p>
<h2 id="4-结果">4 结果</h2>
<p>如果为标注数据多的时候，这个方法选择样本是这样的：首先对所有未标注的进行随机采样，然后在选择前k个最不确信的样本，这样可以避免都选择出相同类别的。因为可能会出现前k个最不确信的样本中大部分都是一个类别的。</p>
<p>这个主动学习的方法比之前的几个要好,learn loss表示现在这个方法：
<img src="https://jzq6520.github.io/post-images/1571307198541.png" alt=""></p>
<p>mse损失和排序损失的效果比较：
<img src="https://jzq6520.github.io/post-images/1571307238951.png" alt=""></p>
<h2 id="5-总结">5 总结</h2>
<p>一个是通过排序的损失，一个是mse损失。排序损失会更好。其实这里更重要的是得到一个损失的相对大小，而不是损失本身，因为最后的目的是根据损失对未标注的样本进行挑选，所以排序准确才是最终目的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[cv-笔记-主动学习active learning的思想]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-zhu-dong-xue-xi-active-learning</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-zhu-dong-xue-xi-active-learning">
        </link>
        <updated>2019-10-17T02:27:00.000Z</updated>
        <content type="html"><![CDATA[<p>数据不断涌入，但深度神经网络仍然对数据如饥似渴。实证分析表明，就训练数据的规模而言，近期深度网络的表现尚未达到饱和。因此，从半监督学习到非监督学习的学习方法，以及弱标记或未标记的大规模数据都受到关注。</p>
<p>但是，在数据量一定的情况下，半监督或无监督学习的性能仍然与全监督学习的性能有一定的差距。标注数据的比例越高，性能越好。这就是为什么我们要忍受注释的劳动和时间成本。</p>
<ul>
<li><strong>主动学习通过一定的算法查询最有用的未标记样本，并交由专家进行标记，然后用查询到的样本训练分类模型来提高模型的精确度。</strong></li>
<li>那么被动学习就是说别人标注了什么数据我们就拿什么数据进行训练，而不是主动的去挖掘一些模型需要的数据去标注。</li>
<li>查询函数（就是从未标注数据中挖掘需要进行标注的数据的方法）的设计最常用的策略是：不确定性准则（uncertainty）和差异性准则（diversity）。
<ul>
<li>不确定性：对于分类选择出那些概率解决于0.5，或者是不确定的数据进行标注，对于分割可选择出那些分割错误较大的数据进行标注。</li>
<li>差异性：可以建立多个不同的模型，然后挑选出那些预测不一致的数据进行标注。</li>
</ul>
</li>
</ul>
<p><img src="https://jzq6520.github.io/post-images/1571280110452.png" alt=""></p>
<h2 id="引用">引用</h2>
<ul>
<li>https://www.cnblogs.com/hust-yingjie/p/8522165.html</li>
<li>https://www.jianshu.com/p/e908c3595fc0</li>
<li>learn loss:《Learning Loss for Active Learning》</li>
<li>entropy-based sampling：《Latent structured active learning》</li>
<li>random sampling:《An analysis of active learning strategies for sequence labeling tasks》</li>
<li>core-set:《Active learning for convolutional neural networks: A core-set approach.》</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-深度学习模型压缩]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-shen-du-xue-xi-mo-xing-ya-suo</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-shen-du-xue-xi-mo-xing-ya-suo">
        </link>
        <updated>2019-10-11T09:03:34.000Z</updated>
        <content type="html"><![CDATA[<p>在不降低或者是不大幅度减少精度的条件下，目前的模型压缩方式有：</p>
<ul>
<li>知识蒸馏；</li>
<li>量化</li>
<li>稀疏</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-图像质量评价Image quality assessment IQA简介]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-tu-xiang-zhi-liang-ping-jie-image-quality-assessment-iqa-jian-jie</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-tu-xiang-zhi-liang-ping-jie-image-quality-assessment-iqa-jian-jie">
        </link>
        <updated>2019-10-10T02:32:14.000Z</updated>
        <content type="html"><![CDATA[<p>2019年10月10日</p>
<h2 id="1-简介">1 简介</h2>
<p>图像质量评价（Image quality assessment，IQA）按照是否有参考图像可以分为三种：1. 有参IQA，2.半参IQA，3. 无参IQA。</p>
<p>有参图像质量评价虽然效果很好，但是前提是要有一个参考图像，也就是说一张质量好的图像作为参考，然后给评价的图像进行打分，而且参考图像和被打分的图像是要同一个图。但是在某些场景下参考图像是比较难获得的。</p>
<p>在测试图像压缩、图像传输效果时候或许可以有一个原始图像作为参考，然后测试被压缩或被传输后的图像知道。但是在很多场景是无法获得参考图像的，例如你拍摄一张图像，如果你没有拍摄一个绝对清晰的图像，那就无法拿到参考图像。比还有一些实时的场景下我们是不好获得参考图像的。</p>
<p><strong>所以这时候无参图像质量评价就出来了。</strong></p>
<h2 id="2-基于深度学习的图像质量评价">2 基于深度学习的图像质量评价</h2>
<p>废话不多说直接进入正题，这里我们主要讨论利用深度学习来解决图像质量评价问题。传统方法可以自行谷歌。</p>
<p>在我看来基于深度学习的图像质量评价主要有两种：</p>
<ol>
<li>直接利用回归，输入为图像，输出为质量评价分数。那么训练数据集就是 图像和label。</li>
<li>利用伪参考图像，然后将待评价图像和伪参考图像一起输入到回归网络中，输出质量评价分数。那么训练数据要包含：待评价图像、参考图像和label。注：伪参考图像这个说法是我自己想的。</li>
</ol>
<p>接下来对这两种基于深度学习的方法进行简单介绍，并且附上论文。</p>
<h2 id="21-直接利用回归">2.1 直接利用回归</h2>
<p>直接利用回归进行质量评价。这个思路比较简单，就是我们输入一张待评价图像然后输出的是一个值（score）。</p>
<p>网络就可以设计为一个卷积神经网络，例如我们可以选用VGG网络进行回归。</p>
<p><strong>但是由于图像质量评价的数据往往难以获得，且存在很大的主观性，所以说数据量是不大的。所以Xialei Liu等人就提出了RankIQA来解决数据量小的问题。</strong></p>
<h3 id="211-rankiqa">2.1.1 RankIQA</h3>
<p>别看名字这么玄乎，其实本身思想其实很简单，就是利用迁移学习。那么从哪里迁移网络参数呢？是从参数共享的网络得到。<strong>这个参数共享又不能说是孪生网络（注：论文中说是孪生网络），因为这个网络不像孪生网络一样有连接在一块，所以应该只能算是参数共享的网络</strong>。</p>
<p>首先构建两个参数共享的网络，这时候将两张图分别从两个网络输入，网络输出的是一个质量评价分数，这时候我们是知道两张输入图像哪张好哪张坏的，所以这时候就可以比较两个网络输出的质量评价分数来进行计算loss。如果输出的质量评价分数的大小关系和实际一样，那么就不计算loss。</p>
<p><img src="https://jzq6520.github.io/post-images/1570677420978.png" alt=""></p>
<p>那么输入的图像是哪里来的呢？是我们自己构造的，利用高斯核对图像进行模糊，这样我们就可以生成一组不同模糊程度的图像。因为我们不知道模糊程度该打几分，所以在训练参数共享网络的时候，loss是通过比较两个网络的数据来进行计算的，而不是和传统的一样计算均方差。</p>
<p>LOSS:
<img src="https://jzq6520.github.io/post-images/1570677513453.png" alt=""></p>
<p>这里有个假设，就是x1是大于x2的，所以从这个公式1的Loss公式可以看出，当网络输出x1大于x2时不计算loss，反之则计算一个loss。</p>
<p>最后，由于我们自己生成了许多可以指定图像质量相对好坏的数据，所以可以训练更深的网络，克服了一部分数据量的问题。然后利用在人工标注的groundtruth上进行finetune（微调）得到最终的回归网络（质量评价网络）。</p>
<p><strong>总结：数据上只需要图像和对应的质量分数，网络相对简单。利用了迁移学习和很聪明的学习技巧和损失函数</strong></p>
<h2 id="22-利用伪参考图像">2.2  利用伪参考图像</h2>
<p>为什么我说这个是伪参考图像，因为这里面<strong>训练的时候是需要参考图像</strong>的，并且<strong>网络会生成一个假的参考图像</strong>，这个参考图像是通过网络对质量差的图像<strong>修复得到</strong>，例如一种模糊图像，然后由网络对其进行修复生成一张清晰的图像。然后利用这个清晰的图像作为参考图像对图像进行质量评价。</p>
<p>这里介绍两篇文章：RAN4IQA和Hallucinated-IQA，这两篇文章都是这样的思路，都是利用GAN（对抗生成网络）修复质量差的图像，然后将得到的修复后的图像（伪参考图像）作为参考图像对待评价图像进行打分。</p>
<p>所以这两个网络可以分解成三部分：<strong>伪参考图像评价网络 = GAN + 评价网络 = 修复网络（生成网络）+ 判别网络 + 评价网络</strong></p>
<p><strong>其实我们可以发现，当我们抛开评价网络，剩下的GAN就成了一个图像修复网络了</strong>。</p>
<p>再说一下这两个网络的区别：</p>
<ul>
<li>RAN4IQA是将修复后的图像和待参考图像输入到<strong>孪生网络</strong>中去回归得到分数。当然这里是基于patch的，所以还会有一个patch的权重，最后将所有patch的权重加权平均得到最后整张图像的分数。</li>
<li>Hallucinated-IQA是将<strong>差值图像</strong>（修复后的和待评价图像相减后得到的图）和待参考图像输入到一个<strong>双输入网络</strong>中，回归得到一个分数。并且使用了修复网络的下采样输出的特征。</li>
</ul>
<h3 id="221-ran4iqa">2.2.1 RAN4IQA</h3>
<p><img src="https://jzq6520.github.io/post-images/1570678849786.png" alt=""></p>
<h3 id="222-hallucinated-iqa">2.2.2 Hallucinated-IQA</h3>
<p><img src="https://jzq6520.github.io/post-images/1570678886713.png" alt=""></p>
<h2 id="3-总结">3 总结</h2>
<p>当有参考图像的时候可以训练一个伪参考图像网络，当没有参考图像的时候可以训练一个RankIQA网络。</p>
<h2 id="4-参考">4 参考</h2>
<ul>
<li>Hallucinated-IQA: No-Reference Image Quality Assessment via Adversarial Learning</li>
<li>RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality Assessment</li>
<li>RankIQA: Learning from Rankings for No-reference Image Quality Assessment</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[keras-Callback回调函数]]></title>
        <id>https://jzq6520.github.io/post/keras-callback-hui-diao-han-shu</id>
        <link href="https://jzq6520.github.io/post/keras-callback-hui-diao-han-shu">
        </link>
        <updated>2019-07-29T06:30:07.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-回调函数的定义">1 回调函数的定义</h2>
<p>当程序跑起来时，一般情况下，应用程序（application program）会时常通过API调用库里所预先备好的函数。但是有些库函数（library function）却要求应用先传给它一个函数，好在合适的时候调用，以完成目标任务。这个被传入的、后又被调用的函数就称为回调函数（callback function）。</p>
<h4 id="回调函数的例子">回调函数的例子</h4>
<pre><code class="language-python">### 回调函数
#回调函数1
#生成一个2k形式的偶数
def double(x):
    return x * 2
    
#回调函数2
#生成一个4k形式的偶数
def quadruple(x):
    return x * 4

## 使用回调函数的中间函数，也就是回调函数的使用者
#中间函数
#接受一个生成偶数的函数作为参数
#返回一个奇数
def getOddNumber(k, getEvenNumber):
    return 1 + getEvenNumber(k)
    
#起始函数，这里是程序的主函数
def main():    
    k = 1
    #当需要生成一个2k+1形式的奇数时
    i = getOddNumber(k, double)
    print(i)
    #当需要一个4k+1形式的奇数时
    i = getOddNumber(k, quadruple)
    print(i)
    #当需要一个8k+1形式的奇数时
    i = getOddNumber(k, lambda x: x * 8)
    print(i)
    
if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>可以发现，其实我们只需要知道回调函数是<strong>传入什么参数，有什么功能即可</strong>，而不能自己去定义传入参数，<strong>所以我们去写回调函数的时候要按照中间函数传入的参数去写</strong>。</p>
<h2 id="2-keras中的回调函数">2 keras中的回调函数</h2>
<p>这里我们不讲那些keras中已经定义的回调函数，这里说一下如何创建自己的回调函数。
keras中定义了一个回调函数的抽象类，这个类包含多个回调函数（即一个类里面有很多方法，这些方法是不同的时期被中间函数调用的）。我们继承这个类，然后重新其中的回调函数即可。
下面看一下这个基类：
https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L275</p>
<pre><code class="language-python">class Callback(object):
			 &quot;&quot;&quot;Abstract base class used to build new callbacks.
    # Properties
        params: dict. Training parameters
            (eg. verbosity, batch size, number of epochs...).
        model: instance of `keras.models.Model`.
            Reference of the model being trained.
    The `logs` dictionary that callback methods
    take as argument will contain keys for quantities relevant to
    the current batch or epoch.
    Currently, the `.fit()` method of the `Sequential` model class
    will include the following quantities in the `logs` that
    it passes to its callbacks:
        on_epoch_end: logs include `acc` and `loss`, and
            optionally include `val_loss`
            (if validation is enabled in `fit`), and `val_acc`
            (if validation and accuracy monitoring are enabled).
        on_batch_begin: logs include `size`,
            the number of samples in the current batch.
        on_batch_end: logs include `loss`, and optionally `acc`
            (if accuracy monitoring is enabled).
    &quot;&quot;&quot;
		
     def __init__(self):
        self.validation_data = None
        self.model = None
		
		## 注意：set params和 set_model是已经定义好的，也就是说继承这个类以后回调函数本身就会有self.params 和self.model，不需要我们去关心有没有或者怎么得到这两个变量。
		# 既然是回调函数，那么params这个参数是中间函数传给它的，不需要我们去传。
    def set_params(self, params): 
        self.params = params

    def set_model(self, model):
        self.model = model
		
		## 什么时候会调用，直接可以看方法名。
		# Arguments
    #       logs: 具体可以看上面链接中给出的注释，每个都不一样。
	def on_batch_begin(self, batch, logs=None)
	def on_batch_end(self, batch, logs=None)
	def on_epoch_begin(self, epoch, logs=None)
	def on_epoch_end(self, epoch, logs=None)
	def on_train_batch_begin(self, batch, logs=None)
	def on_train_batch_end(self, batch, logs=None)
	def on_test_batch_begin(self, batch, logs=None)
	def on_test_batch_end(self, batch, logs=None)
	def on_predict_batch_begin(self, batch, logs=None)
	def on_predict_batch_end(self, batch, logs=None)
	def on_train_begin(self, logs=None)
	def on_train_end(self, logs=None)
	def on_test_begin(self, logs=None)
	def on_test_end(self, logs=None)
	def on_predict_begin(self, logs=None)
	def on_predict_end(self, logs=None)	
</code></pre>
<p>在回调函数中可以使用这两个参数。</p>
<ul>
<li>
<pre><code>    self.params = params： 字典。训练参数， (例如，verbosity, batch size, number of epochs...)。可以打印出来看看。
</code></pre>
</li>
<li>
<pre><code>    self.model = model：keras.models.Model 的实例。 指代被训练模型。
</code></pre>
</li>
</ul>
<p>通过类的属性 self.model，回调函数可以获得它所联系的模型。</p>
<h3 id="keras自定义回调函数的例子">keras自定义回调函数的例子</h3>
<p>回调函数使用以后，还可以通过类实例来访问实例变量。</p>
<pre><code class="language-python">class LossHistory(keras.callbacks.Callback):
    def __init__(self):
				super(LossHistory, self).__init__()
				self.losses = []
				
    def on_train_begin(self, logs={}):
        self.losses = []

    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))

model = Sequential()
model.add(Dense(10, input_dim=784, kernel_initializer='uniform'))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

history = LossHistory()
model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history])

print(history.losses)
</code></pre>
<h2 id="引用">引用</h2>
<ul>
<li>https://github.com/chensvm/Keras-Callback-F1/blob/master/roc_auc_score_Metrics</li>
<li>https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L275</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-超分辨率-Image super-resolution using a dilated convolutional neural network]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-chao-fen-bian-lu-image-super-resolution-using-a-dilated-convolutional-neural-network</id>
        <link href="https://jzq6520.github.io/post/cv-paper-chao-fen-bian-lu-image-super-resolution-using-a-dilated-convolutional-neural-network">
        </link>
        <updated>2019-05-07T09:02:24.000Z</updated>
        <content type="html"><![CDATA[<h1 id="image-super-resolution-using-a-dilated-convolutional-neural-network">Image super-resolution using a dilated convolutional neural network</h1>
<p>这是一篇很有意思的文章，首先他的应用场景是超分辨率，然后他用到的网络结合了很多优秀的结构，通过这些优秀的结构解决了很多超分辨中的问题。</p>
<p>而且这个网络非常简单，并且很优雅，让人流连忘返。</p>
<p>文章提出的网络主要有以下几个部件组成：</p>
<ol>
<li><strong>空洞卷积</strong>（或叫扩张卷积，dilated convolutional);</li>
<li><strong>跨越连接</strong>（skip connect）；</li>
<li><strong>不下采样</strong>，保留分辨率。</li>
</ol>
<h2 id="1-网络">1 网络</h2>
<p><img src="https://jzq6520.github.io/post-images/1557220077383.png" alt=""></p>
<p>从上图可以看到以下几点：</p>
<ul>
<li>网络的<strong>输入输出大小是一样的</strong>，那么如何获得超分辨率的图像的，是这样的，首先将图像插值放大到2倍，然后输入到这个网络中，网络输出的是分辨率一样，但是细节更加清晰的图像。</li>
<li><strong>网络只有7层</strong>，是一个非常小的网络，因为在使用空洞卷积的到时候，网络并没有进行下采样，所以造成显存和计算量会占用很大，所以文章采用了较小的网络，实现了节省计算资源和显存的作用。</li>
<li>使用了<strong>skip连接</strong>，使用skip连接是将<strong>低维信息和高维相结合</strong>，低维信息往往代表一些<strong>图像的细节</strong>，而高维信息往往表示一些高维的<strong>语义特征</strong>，所以通过skip连接然网络的数据具有更多的细节，从而这个超分辨率网络才会更好。</li>
<li>使用了<strong>空洞卷积</strong>（dilated conv），空洞卷积具有<strong>保留信息</strong>而<strong>增加感受野</strong>的作用，所以空洞卷积在分割和检测中也常常使用。</li>
</ul>
<h2 id="2-放大超过两倍">2 放大超过两倍</h2>
<p>如果想将图像放大4倍、5倍或者跟大，那要怎么实现呢？ 文章利用联机的方式来进行放大，只需要将图像多次经过超分辨率网络就可以实现，如果放大倍数不是2的倍数，那么怎么办呢？ 这时候我们可以先放大，再缩小。例如想放大3倍，我们可以先放大4倍，然后再进行下采样即可。</p>
<p><img src="https://jzq6520.github.io/post-images/1557220626079.png" alt=""></p>
<h2 id="3-引用">3 引用</h2>
<ul>
<li>Image super-resolution using a dilated convolutional neural network</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-文字检测-Character Region Awareness for Text Detection]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-wen-zi-jian-ce-character-region-awareness-for-text-detection</id>
        <link href="https://jzq6520.github.io/post/cv-paper-wen-zi-jian-ce-character-region-awareness-for-text-detection">
        </link>
        <updated>2019-05-06T11:46:34.000Z</updated>
        <content type="html"><![CDATA[<h1 id="character-region-awareness-for-text-detection-基于字符识别的文字检测">Character Region Awareness for Text Detection 基于字符识别的文字检测</h1>
<p>这篇文章是利用关键点检测的思想来进行文字检测。检测单个<strong>字符</strong>，并且识别出哪些字符是组成<strong>文字</strong>的，这样就可以检测出一组组文字。</p>
<p>以前的一些方法都是检测word-level的bounding box，但是这样会遇到一些难点，例如文字是弯曲的，不规则的，或则是特别长的。如果是基于character-level的话就没用这些难点了，因为是检测单个字符，所以没有文字形状的规定，并且，只需要小的感受野即可以了， 但是以前的检测包围框的方法就需要很大的感受野才行。</p>
<p><strong>那么，基于单个字符区域的文字检测存在两个难点</strong>：</p>
<ol>
<li>如何确定哪些字符是连接在一起组成文字的，而哪些字符是分离的；</li>
<li>数据标注问题，因为当前的数据集都是文字级别（word-level）的标注。</li>
</ol>
<p>下面分别来说明解决这两个难点的方法。</p>
<h2 id="1-确定哪些字符是连接的">1 确定哪些字符是连接的</h2>
<p><strong>字符检测</strong>：本文使用的网络使用了目前关键点检测中常用的网络结构，即采用预测热图的方式来进行检测关键点，那么在这里我们就可以把每个<strong>字符</strong>当做一个关键点，所以每个<strong>字符</strong>其实对应着一个<strong>热点</strong>，只要预测每个文字所对应的热点那么就可检测出每个字符。</p>
<p><strong>字符连接的识别</strong>：那么，现在字符的检测方法有了，我们要怎么知道哪些字符是组成一个文字的。这里我觉得作者特别聪明，作者也使用热图的方式来表示文字的连接，如果两个字符是相连接的，那么这两个字符之间就有一个<strong>热点</strong>。很高明的做法，利用热点图来确定两个字符是不是一组的。</p>
<p>热点就代表着一个响应，如果图片中的某个地方有热点响应，那么表示这个地方存在我们需要的信息，热点的值的大小就代表着置信度，如果置信度越高，那么越确定。</p>
<h3 id="11-ground-truth-构造">1.1 ground truth 构造</h3>
<p>如何构造我们的监督信息，可以看下面这幅图。
<img src="https://jzq6520.github.io/post-images/1557144607556.png" alt=""></p>
<p>从上图可以看出，我们的监督信息（或者说网络的预测）有两个，一个是Region Score GT（区域分数），这个是预测字符位置的热图，另外一个是Affinity Score GT（关联分数），这个是预测两个字符是否<strong>关联</strong>的热图。</p>
<p>region score其实就是单个字符的包围框的一个二维高斯图，他是通过对<strong>二维正态分布的高斯图</strong>进行仿射变换得到的。
Affinity Score 通过画对角线来连接每个字符框的对角，我们可以生成两个三角形——我们将其称为上字符三角形和下字符三角形。然后，对于每个相邻的字符框对，通过将上三角形和下三角形的中心设置为框的角，生成一个关联框。然后将<strong>二维正态分布的高斯图</strong>进行仿射变换到关联框来获得对应的热图。</p>
<h2 id="2-数据标注问题-获得字符级别character-level的标注">2 数据标注问题-获得字符级别（Character-level）的标注</h2>
<p>如果想通过人工进行对字符进行标注，那么可想而知是非常耗时的。所以本文使用<strong>人工生成数据和弱监督</strong>结合的方式来解决这个问题。</p>
<h3 id="21-人工生成数据">2.1 人工生成数据</h3>
<p><img src="https://jzq6520.github.io/post-images/1557145267677.png" alt="">
人工生成就是将文字黏贴到一下图片上，这时候因为是自己的文字，所以我们可以有字符级别的包围框，所以我们就有了字符级别的标注。</p>
<h3 id="22-弱监督">2.2 弱监督</h3>
<p><img src="https://jzq6520.github.io/post-images/1557145261386.png" alt=""></p>
<p>上图就是一个弱监督的网络框架，作者将人工生成的数据和我们word-level标注数据一起进行训练。红色箭头预测的是region score，然后经过字符检测的热图来得到每个字符的框，这样进一步又可以得到affinity score的热图。这样就间接获得了文字的affinity score的热图。然后将预测得到的两个score热图作为ground truth进一步监督网络的训练。</p>
<p>当使用弱监督训练模型时，我们被迫训练不完整的伪GT。 如果使用不准确的区域分数训练模型，则输出可能在字符区域内模糊。 为了防止这种情况，我们测量模型生成的每个伪GT的质量。 幸运的是，文本注释中有一个非常强大的提示，即单词长度。 在大多数数据集中，提供了单词的转录，并且单词的长度可用于评估伪GT的置信度。</p>
<p>所以我们在训练的时候要根据<strong>伪GT</strong>的置信度来计算loss，如果生成的GT和真实情况很接近，那么这个loss就是有用的，如果生成的GT都很假，我们肯定是不接受这个loss。所以loss的计算方式如下：
<img src="https://jzq6520.github.io/post-images/1557145511065.png" alt=""></p>
<p>那么如何计算这个置信度呢，我们可以通过计算伪GT的字符长度和真实的GT的字符长度比较来得到：
<img src="https://jzq6520.github.io/post-images/1557145731860.png" alt=""></p>
<p>L(w)表示单词的长度，右上角加c的表示预测的得到的长度，取min得到的是不大于L(w)的值，这样Sconf的值就不会是负数，并且Sconf的值是0~1之间的。</p>
<p>并且预测的热图在这个单词的包围框外面则权重为1，其实简单的理解就是只在word的包围框内的时候它的loss需要加权，因为word（所有的单词区域）包围框外面是没有文字的，所以只要预测出来有字符那么都是假阳。
<img src="https://jzq6520.github.io/post-images/1557146064702.png" alt=""></p>
<h3 id="23-弱监督gt的生成过程">2.3 弱监督GT的生成过程</h3>
<p><img src="https://jzq6520.github.io/post-images/1557146113840.png" alt="">
如图所示，就是先根据word-level标注的数据，将单词切出来，然后再进行预测，得到热图以后再进行处理。</p>
<h2 id="3-热图预测网络">3 热图预测网络</h2>
<p>最后热图预测的网络是一个类U-net的网络。
<img src="https://jzq6520.github.io/post-images/1557146202867.png" alt=""></p>
<h2 id="4-网络收敛过程">4 网络收敛过程</h2>
<p><img src="https://jzq6520.github.io/post-images/1557146285216.png" alt=""></p>
<h2 id="5-网络预测性能">5 网络预测性能</h2>
<p><img src="https://jzq6520.github.io/post-images/1557146323704.png" alt=""></p>
<h2 id="6-引用">6 引用</h2>
<p>论文：Character Region Awareness for Text Detection</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-Paper-文字检测-Shape Robust Text Detection PSENet]]></title>
        <id>https://jzq6520.github.io/post/cv-paper-wen-zi-jian-ce-shape-robust-text-detection-psenet</id>
        <link href="https://jzq6520.github.io/post/cv-paper-wen-zi-jian-ce-shape-robust-text-detection-psenet">
        </link>
        <updated>2019-04-30T06:39:38.000Z</updated>
        <content type="html"><![CDATA[<h1 id="shape-robust-text-detection-with-progressive-scale-expansion-network">Shape Robust Text Detection with Progressive Scale Expansion Network</h1>
<p>这篇文章是利用分割方法来做文字检测，主要克服了弯曲的文字区域检测问题。所以这篇文章叫做形状鲁棒性的文字检测。</p>
<p>主要有以下几个重点思想：</p>
<ol>
<li>利用分割方法来做检测，提高了对非矩形文字区域检测的鲁棒性；</li>
<li>利用Progressive Scale Expansion方法实现了instance segment（实例分割）；</li>
<li>多尺度分割。</li>
</ol>
<p>当然这篇文章的方法对于矩形区域的文字检测也是相当准确的。</p>
<h2 id="1-网络">1 网络</h2>
<p>首先来看一下这篇文章使用的网络结构，如下图所示。
<img src="https://jzq6520.github.io/post-images/1556608333773.png" alt="network"></p>
<p>作者是将这个网络分成两个部分来讲述，左边是一个FPN结构，右边是一个特征融合部分和progressive scale expansion部分。</p>
<p>其实我个人是把这个网络看做分割网络和后处理两个部分。</p>
<p>首先从网络输入到mask预测输出看做分割部分，后面将多个分割结果整合成一个最终的结果看做是progressive scale expansion后处理部分。</p>
<h3 id="11-分割网络">1.1 分割网络</h3>
<p>分割网络由FPN, 特征融合，加上一个预测组成。</p>
<p>首先分割网络采用了几个技术：</p>
<ol>
<li>FPN： FPN是一个解决多尺度的常用的解决方案，上采样的不同层的feature map大小不一样，从大到小，这样在小的featuremap上检测相对大的目标，在大的featuremap上可以检测相对小的目标（因为分辨率更高），实现了一个多尺度检测；</li>
<li>concat：concat在图像处理中也经常遇到，经常用来融合不同尺度或不同层级特征的信息，在这里就是将各个不同尺度的特征图（feature map）的特征进行融合，实现一个多尺度的识别。</li>
<li>concat后面的卷积：concat只是简单的将特征堆叠起来，后面一般会接一个conv层来融合特征。</li>
<li>concat后的卷积还用了一次upsample，即上采样，这是因为FPN最后一层的数据是原图的一半，所以这里特征融合一个又进行了一次上采样来让预测结果和原图一样大小。</li>
<li>sigmoid来预测n个mask。</li>
</ol>
<h3 id="12-后处理">1.2 后处理</h3>
<p>后处理采用的是一个简单的像素点划分，有点类似区域生长，只是说在一个范围内进行区域生长。</p>
<p>mask最小的预测结果实现了instance分割，即每个文字块是相互分离的。最大的mask就是最终的分割结果（但是没有进行instance分割），但有时候文字块区域是相互黏连的，所以需要通过最小的mask的信息来讲这些黏连的区域分离。</p>
<p>具体过程就如下所示：
<img src="https://jzq6520.github.io/post-images/1556609357492.png" alt="scale expansen"></p>
<p>扩展基于广度优先搜索算法，该算法从多个内核的像素开始，迭代地合并相邻的文本像素。对于冲突的像素，只能被一个kernel合并，通过先到先得的规则合并。由于是这种根据下一个mask的边界来扩展区域的，所以不会影响最后的结果。</p>
<h2 id="2-构建数据集">2 构建数据集</h2>
<p><img src="https://jzq6520.github.io/post-images/1556610390720.png" alt=""></p>
<p>需要创建多个不同目标大小（经过不同尺度的缩小）的mask。文章中是使用图形学的方法来进行创建的。但是我决定也可以使用形态学腐蚀的方法创建。</p>
<p>文章使用Vatti clipping algorithm方法来进行mask的腐蚀。
腐蚀的边界距离是通过这个公式计算得到的：
<img src="https://jzq6520.github.io/post-images/1556610096797.png" alt=""></p>
<p>并且缩小的倍数是计算得到的，其实就是一个插值得到中间的倍数，例如从0.4-1倍，总共5个mask，那么就不是完整的从0.4，0.5，0.6，...这样下去，所以还是要计算一下的。公式如下：
<img src="https://jzq6520.github.io/post-images/1556610255688.png" alt=""></p>
<p>m表示最低是多少倍，n表示有几个mask，r就是倍数，d是边界到腐蚀以后边界的距离，p表示mask区域。</p>
<p>代码：</p>
<pre><code class="language-python">import pyclipper
import Polygon as plg
def get_bboxes(img, gt_path):
    h, w = img.shape[0:2]
    lines = util.io.read_lines(gt_path)
    bboxes = []
    tags = []
    for line in lines:
        line = util.str.remove_all(line, '\xef\xbb\xbf')
        gt = util.str.split(line, ',')

        x1 = np.int(gt[0])
        y1 = np.int(gt[1])

        bbox = [np.int(gt[i]) for i in range(4, 32)]
        bbox = np.asarray(bbox) + ([x1 * 1.0, y1 * 1.0] * 14)
        bbox = np.asarray(bbox) / ([w * 1.0, h * 1.0] * 14)
        
        bboxes.append(bbox)
        tags.append(True)
    return np.array(bboxes), tags
		
def dist(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

def perimeter(bbox):
    peri = 0.0
    for i in range(bbox.shape[0]):
        peri += dist(bbox[i], bbox[(i + 1) % bbox.shape[0]])
    return peri

def shrink(bboxes, rate, max_shr=20):
    rate = rate * rate
    shrinked_bboxes = []
    for bbox in bboxes:
        area = plg.Polygon(bbox).area()
        peri = perimeter(bbox)

        pco = pyclipper.PyclipperOffset()
        pco.AddPath(bbox, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)
        offset = min((int)(area * (1 - rate) / (peri + 0.001) + 0.5), max_shr)
        
        shrinked_bbox = pco.Execute(-offset)
        if len(shrinked_bbox) == 0:
            shrinked_bboxes.append(bbox)
            continue
        
        shrinked_bbox = np.array(shrinked_bbox[0])
        if shrinked_bbox.shape[0] &lt;= 2:
            shrinked_bboxes.append(bbox)
            continue
        
        shrinked_bboxes.append(shrinked_bbox)
    
    return np.array(shrinked_bboxes)
</code></pre>
<h2 id="3-损失函数-loss">3 损失函数 LOSS</h2>
<p>损失由完整的mask的loss加上腐蚀以后mask的loss，其中两项是加权相加：
<img src="https://jzq6520.github.io/post-images/1556610560713.png" alt="">
c表示完整的（complete），s表示（腐蚀后的，shrunk），Ls是所以腐蚀后的mask loss相加得到的。
使用1-dice coefﬁcient作为loss，来避免样本不平衡（前景和背景）的影响。dice coefﬁcient计算如下：
<img src="https://jzq6520.github.io/post-images/1556610778071.png" alt=""></p>
<p><img src="https://jzq6520.github.io/post-images/1556610842164.png" alt="">
并且腐蚀后的mask的loss是在预测区域（注意不是标注区域内）内计算的，这个和之前我其他任务中在G里面计算loss不一样，这个是在预测的区域内计算loss，这样的一个好处可能是假阳会少，因为如果在G里面计算，那么G外面预测的假阳就没用计算在loss里面。</p>
<p>最后还使用了在线难例挖掘。</p>
<h2 id="4-augment">4 augment</h2>
<p>数据增强使用了，水平翻转，随机旋转，随机缩放，随机剪裁640x640大小。
代码：</p>
<pre><code class="language-python">import pyclipper
import Polygon as plg
def random_horizontal_flip(imgs):
    if random.random() &lt; 0.5:
        for i in range(len(imgs)):
            imgs[i] = np.flip(imgs[i], axis=1).copy()
    return imgs

def random_rotate(imgs):
    max_angle = 10
    angle = random.random() * 2 * max_angle - max_angle
    for i in range(len(imgs)):
        img = imgs[i]
        w, h = img.shape[:2]
        rotation_matrix = cv2.getRotationMatrix2D((h / 2, w / 2), angle, 1)
        img_rotation = cv2.warpAffine(img, rotation_matrix, (h, w))
        imgs[i] = img_rotation
    return imgs

def scale(img, long_size=2240):
    h, w = img.shape[0:2]
    scale = long_size * 1.0 / max(h, w)
    img = cv2.resize(img, dsize=None, fx=scale, fy=scale)
    return img

def random_scale(img, min_size):
    h, w = img.shape[0:2]
    if max(h, w) &gt; 1280:
        scale = 1280.0 / max(h, w)
        img = cv2.resize(img, dsize=None, fx=scale, fy=scale)

    h, w = img.shape[0:2]
    random_scale = np.array([0.5, 1.0, 2.0, 3.0])
    scale = np.random.choice(random_scale)
    if min(h, w) * scale &lt;= min_size:
        scale = (min_size + 10) * 1.0 / min(h, w)
    img = cv2.resize(img, dsize=None, fx=scale, fy=scale)
    return img

def random_crop(imgs, img_size):
    h, w = imgs[0].shape[0:2]
    th, tw = img_size
    if w == tw and h == th:
        return imgs
    
    if random.random() &gt; 3.0 / 8.0 and np.max(imgs[1]) &gt; 0:
        tl = np.min(np.where(imgs[1] &gt; 0), axis = 1) - img_size
        tl[tl &lt; 0] = 0
        br = np.max(np.where(imgs[1] &gt; 0), axis = 1) - img_size
        br[br &lt; 0] = 0
        br[0] = min(br[0], h - th)
        br[1] = min(br[1], w - tw)
        
        i = random.randint(tl[0], br[0])
        j = random.randint(tl[1], br[1])
    else:
        i = random.randint(0, h - th)
        j = random.randint(0, w - tw)
    
    # return i, j, th, tw
    for idx in range(len(imgs)):
        if len(imgs[idx].shape) == 3:
            imgs[idx] = imgs[idx][i:i + th, j:j + tw, :]
        else:
            imgs[idx] = imgs[idx][i:i + th, j:j + tw]
    return imgs
</code></pre>
<h2 id="5-最后">5 最后</h2>
<p>细节可看论文，这里只介绍了文章的主要思想和内容。</p>
<h2 id="6-引用">6 引用</h2>
<ul>
<li>Shape Robust Text Detection with Progressive Scale Expansion Network</li>
<li>https://github.com/whai362/PSENet</li>
<li>https://github.com/whai362/PSENet/blob/master/dataset/ctw1500_loader.py#L40</li>
</ul>
]]></content>
    </entry>
</feed>