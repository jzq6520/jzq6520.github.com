<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jzq6520.github.io</id>
    <title>chuck</title>
    <updated>2019-11-26T15:19:02.002Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jzq6520.github.io"/>
    <link rel="self" href="https://jzq6520.github.io/atom.xml"/>
    <subtitle>天地不仁以万物为刍狗</subtitle>
    <logo>https://jzq6520.github.io/images/avatar.png</logo>
    <icon>https://jzq6520.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, chuck</rights>
    <entry>
        <title type="html"><![CDATA[目标检测--RetinaNet代码实现的细节]]></title>
        <id>https://jzq6520.github.io/post/mu-biao-jian-ce-retinanet-dai-ma-shi-xian-de-xi-jie-wen-ti</id>
        <link href="https://jzq6520.github.io/post/mu-biao-jian-ce-retinanet-dai-ma-shi-xian-de-xi-jie-wen-ti">
        </link>
        <updated>2019-11-26T14:13:37.000Z</updated>
        <content type="html"><![CDATA[<ol>
<li>网络：
<ol>
<li>除了backbone部分，其他的fpn、分类subnet和回归subnet都不加bn；</li>
<li>fpn的部分不加激活，回归和分类subnet加relu激活，最后的回归不加激活；</li>
<li>p6到p7要做激活；</li>
</ol>
</li>
<li>loss：
<ol>
<li>回归只计算正样本的loss，<strong>且除以正样本的个数</strong>。</li>
<li>分类只计算正样本和负样本的loss，并且也是<strong>除以正样本的个数</strong>，而不是除以正负样本的。</li>
</ol>
</li>
<li><strong>回归subnet是不共享参数的，分类subnet共享参数</strong>；</li>
<li>最后预测的时候，对预测的所有概率进行排序，然后取前1000个bbox，然后卡0.05的阈值，最后做nms，<strong>nms的阈值是0.5</strong>.</li>
<li>分类的最后一个conv的bias初始化为<code>-log((1-0.01)/0.01)</code>,其他照样初始化为0，子网络其他部分卷积核进行标准差为0.01的高斯初始化，bias为0初始化。</li>
</ol>
<h2 id="keras中是直接回归两个角的坐标">keras中是直接回归两个角的坐标</h2>
<p>其实也是可以回归中心点和宽高的。<br>
如下所示，直接回归角点坐标的一个变换，而不是中心点和宽高：</p>
<pre><code class="language-python">targets_dx1 = (gt_boxes[:, 0] - anchors[:, 0]) / anchor_widths
targets_dy1 = (gt_boxes[:, 1] - anchors[:, 1]) / anchor_heights
targets_dx2 = (gt_boxes[:, 2] - anchors[:, 2]) / anchor_widths
targets_dy2 = (gt_boxes[:, 3] - anchors[:, 3]) / anchor_heights
</code></pre>
<p>然后最后的网络回归的数值还要再在之前的上面除以一个0.2</p>
<pre><code>if mean is None:
        mean = np.array([0, 0, 0, 0])
if std is None:
    std = np.array([0.2, 0.2, 0.2, 0.2])
targets = np.stack((targets_dx1, targets_dy1, targets_dx2, targets_dy2))
targets = targets.T

targets = (targets - mean) / std
</code></pre>
<h2 id="子网络subnet">子网络subnet</h2>
<p>代码：https://github.com/fizyr/keras-retinanet</p>
<pre><code class="language-python">## 回归的subnet
options = {
        'kernel_size'        : 3,
        'strides'            : 1,
        'padding'            : 'same',
        'kernel_initializer' : keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),
        'bias_initializer'   : 'zeros'}
for i in range(4):
        outputs = keras.layers.Conv2D(
            filters=regression_feature_size,
            activation='relu',
            name='pyramid_regression_{}'.format(i),
            **options
        )(outputs)

    outputs = keras.layers.Conv2D(num_anchors * num_values, name='pyramid_regression', **options)(outputs)

## 分类的subnet
options = {
        'kernel_size' : 3,
        'strides'     : 1,
        'padding'     : 'same',
    }
for i in range(4):
        outputs = keras.layers.Conv2D(
            filters=classification_feature_size,
            activation='relu',
            name='pyramid_classification_{}'.format(i),
            kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),
            bias_initializer='zeros',
            **options
        )(outputs)

    outputs = keras.layers.Conv2D(
        filters=num_classes * num_anchors,
        kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),
        bias_initializer=initializers.PriorProbability(probability=prior_probability),
        name='pyramid_classification',
        **options
    )(outputs)
</code></pre>
<h2 id="fpn">fpn</h2>
<p>代码：https://github.com/fizyr/keras-retinanet</p>
<pre><code class="language-python">def __create_pyramid_features(C3, C4, C5, feature_size=256):
  
    # upsample C5 to get P5 from the FPN paper
    P5           = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C5_reduced')(C5)
    P5_upsampled = layers.UpsampleLike(name='P5_upsampled')([P5, C4])
    P5           = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P5')(P5)

    # add P5 elementwise to C4
    P4           = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C4_reduced')(C4)
    P4           = keras.layers.Add(name='P4_merged')([P5_upsampled, P4])
    P4_upsampled = layers.UpsampleLike(name='P4_upsampled')([P4, C3])
    P4           = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P4')(P4)

    # add P4 elementwise to C3
    P3 = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C3_reduced')(C3)
    P3 = keras.layers.Add(name='P3_merged')([P4_upsampled, P3])
    P3 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P3')(P3)

    # &quot;P6 is obtained via a 3x3 stride-2 conv on C5&quot;
    P6 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P6')(C5)

    # &quot;P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6&quot;
    P7 = keras.layers.Activation('relu', name='C6_relu')(P6)
    P7 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P7')(P7)

    return [P3, P4, P5, P6, P7]
</code></pre>
<h2 id="loss">loss</h2>
<p>代码：https://github.com/fizyr/keras-retinanet</p>
<p>anchor_state:一个向量，也可以单独传入，-1表示不属于正负样本，是被忽略的，0表示负样本，1表示正样本。</p>
<pre><code class="language-python">
def focal(alpha=0.25, gamma=2.0):
    &quot;&quot;&quot; Create a functor for computing the focal loss.

    Args
        alpha: Scale the focal weight with alpha.
        gamma: Take the power of the focal weight with gamma.

    Returns
        A functor that computes the focal loss using the alpha and gamma.
    &quot;&quot;&quot;
    def _focal(y_true, y_pred):
        &quot;&quot;&quot; Compute the focal loss given the target tensor and the predicted tensor.

        As defined in https://arxiv.org/abs/1708.02002

        Args
            y_true: Tensor of target data from the generator with shape (B, N, num_classes).
            y_pred: Tensor of predicted data from the network with shape (B, N, num_classes).

        Returns
            The focal loss of y_pred w.r.t. y_true.
        &quot;&quot;&quot;
        labels         = y_true[:, :, :-1]
        anchor_state   = y_true[:, :, -1]  # -1 for ignore, 0 for background, 1 for object
        classification = y_pred

        # filter out &quot;ignore&quot; anchors
        indices        = backend.where(keras.backend.not_equal(anchor_state, -1))
        labels         = backend.gather_nd(labels, indices)
        classification = backend.gather_nd(classification, indices)

        # compute the focal loss
        alpha_factor = keras.backend.ones_like(labels) * alpha
        alpha_factor = backend.where(keras.backend.equal(labels, 1), alpha_factor, 1 - alpha_factor)
        focal_weight = backend.where(keras.backend.equal(labels, 1), 1 - classification, classification)
        focal_weight = alpha_factor * focal_weight ** gamma

        cls_loss = focal_weight * keras.backend.binary_crossentropy(labels, classification)

        # compute the normalizer: the number of positive anchors
        normalizer = backend.where(keras.backend.equal(anchor_state, 1))
        normalizer = keras.backend.cast(keras.backend.shape(normalizer)[0], keras.backend.floatx())
        normalizer = keras.backend.maximum(keras.backend.cast_to_floatx(1.0), normalizer)

        return keras.backend.sum(cls_loss) / normalizer

    return _focal


def smooth_l1(sigma=3.0):
    &quot;&quot;&quot; Create a smooth L1 loss functor.

    Args
        sigma: This argument defines the point where the loss changes from L2 to L1.

    Returns
        A functor for computing the smooth L1 loss given target data and predicted data.
    &quot;&quot;&quot;
    sigma_squared = sigma ** 2

    def _smooth_l1(y_true, y_pred):
        &quot;&quot;&quot; Compute the smooth L1 loss of y_pred w.r.t. y_true.

        Args
            y_true: Tensor from the generator of shape (B, N, 5). The last value for each box is the state of the anchor (ignore, negative, positive).
            y_pred: Tensor from the network of shape (B, N, 4).

        Returns
            The smooth L1 loss of y_pred w.r.t. y_true.
        &quot;&quot;&quot;
        # separate target and state
        regression        = y_pred
        regression_target = y_true[:, :, :-1]
        anchor_state      = y_true[:, :, -1]

        # filter out &quot;ignore&quot; anchors
        indices           = backend.where(keras.backend.equal(anchor_state, 1))
        regression        = backend.gather_nd(regression, indices)
        regression_target = backend.gather_nd(regression_target, indices)

        # compute smooth L1 loss
        # f(x) = 0.5 * (sigma * x)^2          if |x| &lt; 1 / sigma / sigma
        #        |x| - 0.5 / sigma / sigma    otherwise
        regression_diff = regression - regression_target
        regression_diff = keras.backend.abs(regression_diff)
        regression_loss = backend.where(
            keras.backend.less(regression_diff, 1.0 / sigma_squared),
            0.5 * sigma_squared * keras.backend.pow(regression_diff, 2),
            regression_diff - 0.5 / sigma_squared
        )

        # compute the normalizer: the number of positive anchors
        normalizer = keras.backend.maximum(1, keras.backend.shape(indices)[0])
        normalizer = keras.backend.cast(normalizer, dtype=keras.backend.floatx())
        return keras.backend.sum(regression_loss) / normalizer

    return _smooth_l1
</code></pre>
<h2 id="引用">引用</h2>
<ul>
<li>代码：https://github.com/fizyr/keras-retinanet</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[目标检测--RetinaNet做数据部分]]></title>
        <id>https://jzq6520.github.io/post/mu-biao-jian-ce-retinanet-zuo-shu-ju-bu-fen</id>
        <link href="https://jzq6520.github.io/post/mu-biao-jian-ce-retinanet-zuo-shu-ju-bu-fen">
        </link>
        <updated>2019-11-18T06:40:35.000Z</updated>
        <content type="html"><![CDATA[<p>制作label，并计算loss。</p>
<p>首先理一下网络输出的shape，和数据label的shape。再看一下计算loss的时候应该怎么把数据转换成容易计算的形式。</p>
<h2 id="1-shape">1 shape</h2>
<h3 id="11-网络输出的shape">1.1 网络输出的shape</h3>
<p>下图表示分类分支预测输出的feature map。 格子表示featuremap上的像素。可以发现其实featuremap的<strong>每一层是预测种类别的信息</strong>，我们可以假设有四类别的信息，即x,y,w,h，当然实际预测的是x,y,w,h变换后的值。直接可以看出，每个4个像素对应一个anchor，且位置与原图上的位置相对应。<br>
<img src="https://jzq6520.github.io/post-images/1574061437288.png" alt=""></p>
<h3 id="12计算loss时候的shape">1.2计算loss时候的shape</h3>
<p>因为我们需要计算fpn不同level层的损失（loss），由于不同level层的下采样次数不一样 ，所以featuremap的大小也不一样，这时候我们如果想要简单的将不同层的预测输出放在一个矩阵中，然后和label计算损失是不行的。因为像素宽度不一样，所以不能直接concat，所以我们需要reshape一下，再计算损失。reshape以后矩阵的每一行表示一个anchor。<br>
<img src="https://jzq6520.github.io/post-images/1574062241745.png" alt=""></p>
<h2 id="2-得到监督数据label或者说target">2 得到监督数据（label）或者说target</h2>
<p>其实说起来很简单，就是<strong>对于每个anchor只分配一个与其iou最大的目标</strong>，注：这里不管目标是相同类别的还是不同类别的，都是取最大。</p>
<p>但是实现起来就比较复杂。首先考虑到有两种情况，同种类别击中一个anchor，或不同类别击中一个anchor。</p>
<p>看了别人代码(https://github.com/yhenon/pytorch-retinanet)以后才发现实现代码的时候一个代码就可以把这两种情况考虑进来。看起来没多少代码，但是感觉还是非常考验逻辑的。</p>
<h3 id="21-高维度的广播机制矩阵广播矩阵">2.1 高维度的广播机制，矩阵广播矩阵</h3>
<p>首先用到了高纬度广播的机制。平时用的都是说把一个向量或是常量广播到矩阵或向量。<br>
但是这里却把矩阵和矩阵进行广播。</p>
<p><strong>简单的写一下广播的时候shape的变换</strong>：</p>
<pre><code>（10, 4) + (2, 4) --对被加的矩阵扩展维度--&gt; 
（10, 4, 1) + (2, 4) --&gt; 
（10, 4, 1) + (2, 4) = (10, 4, 2)
</code></pre>
<p>其实可以看做把(2, 4)形状的矩阵的每一行拿出来和（10, 4)形状的矩阵相加，然后把两个结果concat起来。</p>
<h3 id="22-数值索引和布尔索引的区别">2.2 数值索引和布尔索引的区别</h3>
<ul>
<li>False/true索引是只把true的索引出来。布尔索引要求shape要一致。</li>
<li>数值索引是把数值对应的数据拿出来然后堆叠到一起。数值索引不需要一致，且可以大于被索引的长度。</li>
</ul>
<pre><code class="language-python">## 数值索引
In [58]: a = torch.randn(10,2)
Out[58]: 
tensor([[-0.6997,  0.6535],
        [-0.3947,  0.3171],
        [ 0.3076, -1.2978],
        [-0.4225, -0.3197],
        [-1.5714, -1.7430],
        [-0.2521,  0.6664],
        [ 0.6825, -1.2606],
        [-1.1892,  0.3615],
        [-0.3309, -0.0463],
        [ 0.2456, -0.0654]])

In [59]: IoU_max, IoU_argmax = torch.max(a, dim=1)

In [60]: IoU_max
Out[60]: 
tensor([ 0.6535,  0.3171,  0.3076, -0.3197, -1.5714,  0.6664,  0.6825,  0.3615,
        -0.0463,  0.2456])

In [61]: IoU_argmax
Out[61]: tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0])

In [76]: bbox_annotation = np.array([[ 1,1,1,1],[2,2,2,2]])

In [77]: bbox_annotation[iou_argmax]
Out[77]: 
array([[2, 2, 2, 2],
       [2, 2, 2, 2],
       [1, 1, 1, 1],
       [2, 2, 2, 2],
       [1, 1, 1, 1],
       [2, 2, 2, 2],
       [1, 1, 1, 1],
       [2, 2, 2, 2],
       [2, 2, 2, 2],
       [1, 1, 1, 1]])
</code></pre>
<h2 id="3-坐标回归">3 坐标回归</h2>
<p>坐标回归的是相对anchor的一个偏移量（归一化偏移量）。</p>
<ul>
<li>中心点回归： 相对于anchor中心点的偏移，再除以anchor的宽度/高度。这里除以宽度相当于是做一个归一化，<strong>因为同样的偏移量但对于大目标和小目标的意义是不一样的</strong>。</li>
<li>宽/高回归：回归一个log（gt/anchor)；<strong>因为同样的宽高但对于大目标和小目标的意义是不一样的</strong>。大目标的宽高如果误差一点点我们希望提供的loss比较小，而小目标宽高如果误差一点点希望提供的loss比大目标的大一些。所以用log函数，因为log曲线的后面部分的值其实是比较小的，值的增长没那么快。</li>
</ul>
<h2 id="引用">引用</h2>
<ul>
<li>https://github.com/yhenon/pytorch-retinanet</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深度学习--机器学习术语表]]></title>
        <id>https://jzq6520.github.io/post/shen-du-xue-xi-ji-qi-xue-xi-zhu-yu-biao</id>
        <link href="https://jzq6520.github.io/post/shen-du-xue-xi-ji-qi-xue-xi-zhu-yu-biao">
        </link>
        <updated>2019-11-12T09:44:49.000Z</updated>
        <content type="html"><![CDATA[<p>---- from《Deep Learning with PyTorch》</p>
<ul>
<li>
<p><strong>Sample or input or data point</strong>: These mean particular instances of training a set. In our image classification problem seen in the last chapter, each image can be referred to as a sample, input, or data point.</p>
</li>
<li>
<p><strong>Prediction or output</strong>: The value our algorithm generates as an output. For example, in our previous example our algorithm predicted a particular image as 0, which is the label given to cat, so the number 0 is our prediction or output.</p>
</li>
<li>
<p><strong>Target or label</strong>: The actual tagged label for an image.</p>
</li>
<li>
<p><strong>Loss value or prediction error</strong>: Some measure of distance between the predicted value and actual value. The smaller the value, the better the accuracy.</p>
</li>
<li>
<p><strong>Classes</strong>: Possible set of values or labels for a given dataset. In the example in our previous chapter, we had two classes—cats and dogs.</p>
</li>
<li>
<p><strong>Binary classification</strong>: A classification task where each input example should be classified as either one of the two exclusive categories.</p>
</li>
<li>
<p><strong>Multi-class classification</strong>: A classification task where each input example can be classified into of more than two different categories.</p>
</li>
<li>
<p><strong>Multi-label classification</strong>: An input example can be tagged with multiple labels—for example, tagging a restaurant with different types of food it serves such as Italian, Mexican, and Indian. Another commonly-used example is object detection in an image, where the algorithm identifies different objects in the image.</p>
</li>
<li>
<p><strong>Scalar regression</strong>: Each input data point will be associated with one scalar quality, which is a number. Some examples could be predicting house prices, stock prices, and cricket scores.</p>
</li>
<li>
<p><strong>Vector regression</strong>: Where the algorithm needs to predict more than one scalar quantity. One good example is when you try to identify the bounding box that contains the location of a fish in an image. In order to predict the bounding box, your algorithm needs to predict four scalar quantities denoting the edges of a square.</p>
</li>
<li>
<p><strong>Batch</strong>: For most cases, we train our algorithm on a bunch of input samples referred to as the batch. The batch size varies generally from 2 to 256, depending on the GPU's memory. The weights are also updated for each batch, so the algorithms tend to learn faster than when trained on a single example.</p>
</li>
<li>
<p><strong>Epoch</strong>: Running the algorithm through a complete dataset is called an epoch. It is common to train (update the weights) for several epochs.</p>
</li>
</ul>
<h2 id="中文">中文：</h2>
<p>——-- 摘自《PyTorch深度学习》</p>
<ul>
<li><strong>样本或输入或数据点</strong>:这些是指训练一个集合的特定实例。在我们上一章看到的图像分类问题中，每个图像都可以被称为样本、输入或数据点。</li>
<li><strong>预测或输出</strong>:我们的算法生成的输出值。例如，在前面的例子中，我们的算法将特定的图像预测为0，这是给cat的标签，所以数字0是我们的预测或输出。</li>
<li><strong>目标或标签</strong>:图像的实际标签。</li>
<li><strong>损失值或预测误差</strong>:预测值与实际值之间的距离的某种度量。数值越小，精度越好。</li>
<li><strong>类</strong>:给定数据集的可能的值或标签集。在前一章的例子中，我们有两个类——猫和狗。</li>
<li><strong>二进制分类</strong>:一个分类任务，其中每个输入示例应该被分类为两个排他的类别之一。</li>
<li><strong>多类分类</strong>:一个分类任务，其中每个输入示例可以分为两个以上不同的类别。</li>
<li><strong>多标签分类</strong>:输入示例可以使用多个标签进行标记—例如，用不同类型的食物(如意大利、墨西哥和印度)标记餐馆。另一个常用的例子是图像中的对象检测，该算法识别图像中的不同对象。</li>
<li><strong>标量回归</strong>:每个输入数据点都与一个标量质量相关联，它是一个数字。一些例子可以用来预测房价、股票价格和板球比分。</li>
<li><strong>向量回归</strong>:算法需要预测一个以上的标量。一个很好的例子是当你试图识别一个包含一条鱼在图像中的位置的边界框。为了预测边界框，您的算法需要预测表示正方形边缘的四个标量。</li>
<li><strong>Batch</strong>:对于大多数情况，我们在一堆输入样本上训练我们的算法。批处理大小通常在2到256之间变化，这取决于GPU的内存。每个批处理的权重也会更新，因此算法的学习速度往往比在单个示例上训练的速度要快。</li>
<li><strong>Epoch</strong>:在完整数据集上运行算法称为Epoch。训练(更新重量)几个时代是很常见的。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[pytorch--自定义loss]]></title>
        <id>https://jzq6520.github.io/post/pytorch-zi-ding-yi-loss</id>
        <link href="https://jzq6520.github.io/post/pytorch-zi-ding-yi-loss">
        </link>
        <updated>2019-11-12T06:04:25.000Z</updated>
        <content type="html"><![CDATA[<ol>
<li>负log loss；</li>
<li>binary crossentropy；</li>
<li>focal loss；</li>
</ol>
<p>网上找到的loss写的都普遍复杂，我自己稍微写的逻辑简单一点。</p>
<pre><code class="language-python">if inputs.is_cuda and not self.alpha.is_cuda:
            self.alpha = self.alpha.cuda()
</code></pre>
<h2 id="focal-loss">focal loss</h2>
<p>focal loss仔细实践起来可以分为两种情况，一种是二分类（<strong>sigmoid激活</strong>）的时候，还有一种情况就是多分类（<strong>softmax激活</strong>）的时候。</p>
<h3 id="二分类focal-loss">二分类focal loss</h3>
<figure data-type="image" tabindex="1"><img src="https://jzq6520.github.io/post-images/1573545890626.png" alt=""></figure>
<pre><code class="language-python">class FocalLoss(nn.Module):
    &quot;&quot;&quot; -[alpha*y*(1-p)^gamma*log(p)+(1-alpha)(1-y)*p^gamma*log(1-p)] loss&quot;&quot;&quot;

    def __init__(self, gamma, alpha=None , onehot=False):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.onehot = onehot


    def forward(self, inputs, targets):
        &quot;&quot;&quot;

        :param input: onehot
        :param target: 默认是onehot以后
        :return:
        &quot;&quot;&quot;
        N = inputs.size(0)
        C = inputs.size(1)
        inputs = torch.clamp(inputs, min=0.001, max=1.0)  ##将一个张量中的数值限制在一个范围内，如限制在[0.1,1.0]范围内，可以避免一些运算错误，如预测结果q中元素可能为0
        if inputs.is_cuda and not self.alpha.is_cuda:
            self.alpha = self.alpha.cuda()    

         if not self.onehot:
            class_mask = inputs.data.new(N, C).fill_(0)
            class_mask = Variable(class_mask)
            ids = targets.view(-1, 1)
            class_mask.scatter_(1, ids.data, 1.)
            targets = class_mask

        pos_sample_loss_matrix = -targets * (torch.pow((1 - inputs), self.gamma)) * inputs.log()  ## 正样本的loss
        # mean_pos_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()

        neg_sample_loss_matrix = -(targets == 0).float() * (torch.pow((inputs), self.gamma)) * (1 - inputs).log()  ## 负样本的loss
        # mean_neg_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()
        if self.alpha:
            return (self.alpha * pos_sample_loss_matrix + (1 - self.alpha) * neg_sample_loss_matrix).sum() / (N * C)
        else:
            return (pos_sample_loss_matrix + neg_sample_loss_matrix).sum() / (N * C)


</code></pre>
<h2 id="多分类focal-loss">多分类focal loss</h2>
<figure data-type="image" tabindex="2"><img src="https://jzq6520.github.io/post-images/1573546010700.png" alt=""></figure>
<pre><code class="language-python">class FocalLoss(nn.Module):
    &quot;&quot;&quot; -[y*(1-p)^gamma*log(p) loss
        softmax激活输入的foacl loss。
    &quot;&quot;&quot;

    def __init__(self, gamma, onehot=False):
        super(FocalLoss, self).__init__()

        self.gamma = gamma
        self.onehot = onehot


    def forward(self, inputs, targets):
        &quot;&quot;&quot;

        :param input: onehot
        :param target: 默认是onehot以后
        :return:
        &quot;&quot;&quot;

        inputs = torch.clamp(inputs, min=0.001, max=1.0)  ##将一个张量中的数值限制在一个范围内，如限制在[0.1,1.0]范围内，可以避免一些运算错误，如预测结果q中元素可能为0

         if not self.onehot:
            N = inputs.size(0)
            C = inputs.size(1)
            class_mask = inputs.data.new(N, C).fill_(0)
            class_mask = Variable(class_mask)
            ids = targets.view(-1, 1)
            class_mask.scatter_(1, ids.data, 1.)
            targets = class_mask

        pos_sample_loss_matrix = -targets * (torch.pow((1 - inputs), self.gamma)) * inputs.log()  ## 正样本的loss
        # mean_pos_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()

        ## 默认输出均值
        ## 这里不能直接求mean，
        # 因为整个矩阵还是原来的输入大小的，
        # 求loss应该是除以label中有目标的总数。
        
        return pos_sample_loss_matrix / targets.sum()
        


</code></pre>
<h2 id="代码">代码</h2>
<ul>
<li>NegtiveLogLoss</li>
<li>BinaryCrossEntropy</li>
</ul>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch.autograd import Variable

class NegtiveLogLoss(nn.Module):
   &quot;&quot;&quot; -log(p) loss&quot;&quot;&quot;

   def __init__(self, onehot=False):
       super(NegtiveLogLoss, self).__init__()
       self.onehot = onehot

   def forward(self, inputs, targets):
       &quot;&quot;&quot;

       :param input: onehot
       :param target: 默认是onehot以后
       :return:
       &quot;&quot;&quot;

       inputs = torch.clamp(inputs, min=0.001, max=1.0)  ## 将一个张量中的数值限制在一个范围内，如限制在[0.1,1.0]范围内，可以避免一些运算错误，如预测结果q中元素可能为0

       if not self.onehot:
           N = inputs.size(0)
           C = inputs.size(1)
           class_mask = inputs.data.new(N, C).fill_(0)
           class_mask = Variable(class_mask)
           ids = targets.view(-1, 1)
           class_mask.scatter_(1, ids.data, 1.)
           targets = class_mask

       loss_matrix = -targets * inputs.log()  ## 对预测的矩阵里面的每个元素做log，
       ## 然后乘以one hot的label，也就是说获得1位置的值了。
       ## 这时候还是个矩阵，还没有计算均值
       ## 默认输出均值
       return loss_matrix.sum() / targets.sum()  ## 这里不能直接求mean，
       # 因为整个矩阵还是原来的输入大小的，
       # 求loss应该是除以label中有目标的总数。



class BinaryCrossEntropy(nn.Module):
   &quot;&quot;&quot; -(ylog(p)+(1-y)log(1-p) loss&quot;&quot;&quot;

   def __init__(self, alpha=None, onehot=False):
       super(BinaryCrossEntropy, self).__init__()
       self.alpha = alpha
       self.onehot = onehot

   def forward(self, inputs, targets):
       &quot;&quot;&quot;

       :param input: onehot
       :param target: 默认是onehot以后
       :return:
       &quot;&quot;&quot;
       N = inputs.size(0)
       C = inputs.size(1)
       inputs = torch.clamp(inputs, min=0.001, max=1.0)  ##将一个张量中的数值限制在一个范围内，如限制在[0.1,1.0]范围内，可以避免一些运算错误，如预测结果q中元素可能为0
       
       if not self.onehot:

           class_mask = inputs.data.new(N, C).fill_(0)
           class_mask = Variable(class_mask)
           ids = targets.view(-1, 1)
           class_mask.scatter_(1, ids.data, 1.)
           targets = class_mask

       pos_sample_loss_matrix = -targets * inputs.log()  ## 正样本的loss
       # mean_pos_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()
      
       neg_sample_loss_matrix = -(targets == 0).float() * (1 - inputs).log()  ## 负样本的loss
       # mean_neg_sample_loss = pos_sample_loss_matrix.sum() / targets.sum()

       if self.alpha:
           return (self.alpha*pos_sample_loss_matrix + (1-self.alpha)*neg_sample_loss_matrix).sum() / (N * C)
       else:
           return (pos_sample_loss_matrix + neg_sample_loss_matrix).sum() / (N * C)  



</code></pre>
<h2 id="引用">引用</h2>
<ul>
<li>http://kodgv.xyz/2019/04/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/FocalLoss%E9%92%88%E5%AF%B9%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE/</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深度学习--新的激活函数Mish]]></title>
        <id>https://jzq6520.github.io/post/shen-du-xue-xi-xin-de-ji-huo-han-shu-mish</id>
        <link href="https://jzq6520.github.io/post/shen-du-xue-xi-xin-de-ji-huo-han-shu-mish">
        </link>
        <updated>2019-11-11T09:42:23.000Z</updated>
        <content type="html"><![CDATA[<pre><code>import torch
import torch.nn.functional as F
from torch import nn

class Mish(nn.Module):
    '''
    Applies the mish function element-wise:
    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))
    Shape:
        - Input: (N, *) where * means, any number of additional
          dimensions
        - Output: (N, *), same shape as the input
    Examples:
        &gt;&gt;&gt; m = Mish()
        &gt;&gt;&gt; input = torch.randn(2)
        &gt;&gt;&gt; output = m(input)
    '''
    def __init__(self):
        '''
        Init method.
        '''
        super(Mish, self).__init__()

    def forward(self, input):
        '''
        Forward pass of the function.
        '''
        return input * torch.tanh(F.softplus(input))

model = models.resnet50(pretrained=True, progress=True)
print(&quot;acitve&quot;, model.relu)
model.relu = Mish()  ## 在pytorch的resnet50里面这样替换一下就可以了
print(&quot;acitve&quot;, model.relu)

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[pytorch--Deep Learning with PyTorch 简单记录]]></title>
        <id>https://jzq6520.github.io/post/pytorch-deep-learning-with-pytorch-jian-dan-ji-lu</id>
        <link href="https://jzq6520.github.io/post/pytorch-deep-learning-with-pytorch-jian-dan-ji-lu">
        </link>
        <updated>2019-11-11T06:18:36.000Z</updated>
        <content type="html"><![CDATA[<h2 id="底层细节">底层细节</h2>
<ul>
<li>A tensor containing only one element is called a scalar.</li>
<li>一维tensor：</li>
</ul>
<pre><code>temp = torch.FloatTensor([23,24,24.5,26,27.2,23.0])
</code></pre>
<ul>
<li>tensor 数据类型：<code>torch.FloatTensor</code>等</li>
<li>数据类型转换, Tensor后加long(), int(), double(),float(),byte()等函数就能将Tensor进行类型转换；</li>
<li>Torch provides a utility function called from_numpy(), which converts a numpy array into a torch tensor.</li>
</ul>
<pre><code>boston_tensor = torch.from_numpy(boston.data)
</code></pre>
<ul>
<li>tensor to numpy,and slice</li>
</ul>
<pre><code>plt.imshow(panda_tensor[:,:,0].numpy())
</code></pre>
<ul>
<li>4d tensor,先用numpy reshape</li>
</ul>
<pre><code>cat_imgs = np.array([np.array(Image.open(cat).resize((224,224))) for cat in cats[:64]]) 
cat_imgs = cat_imgs.reshape(-1,224,224,3) 
cat_tensors = torch.from_numpy(cat_imgs)
</code></pre>
<ul>
<li>算术运算，addition, subtraction, multiplication, dot product, and matrix multiplication.All of these operations can be either performed on the** CPU or the GPU**.</li>
</ul>
<pre><code>加法：d = torch.add(a,b) ， c = a+b
数值乘法：a.mul(b) ， a*b
矩阵相乘：a.matmul(b)
</code></pre>
<ul>
<li>把tensor从cpu拷贝到gpu，PyTorch provides a simple function called cuda() to copy a tensor on the CPU to the GPU.</li>
</ul>
<pre><code>a = torch.rand(10000,10000) b = torch.rand(10000,10000)
a.matmul(b)
Time taken: 3.23 s

#Move the tensors to GPU a = a.cuda() b = b.cuda()
a.matmul(b)
Time taken: 11.2 µs
</code></pre>
<ul>
<li>require_grad参数表示是否是可学习的，也就是是否梯度下降。</li>
<li>在loss上调用<code>backward</code>函数计算梯度。calculate the gradients by calling the <code>backward</code> function on the final loss variable。</li>
<li></li>
</ul>
<pre><code class="language-python">  def loss_fn(y,y_pred): 
    loss = (y_pred-y).pow(2).sum() 
    for param in [w,b]: ##这里是训练w和b，
        if not param.grad is None: ## 如果参数的梯度是None，即不计算梯度的，那么就梯度设置为0.
            param.grad.data.zero_()  ## remove any previously calculated gradients by calling the grad.data.zero_() operation.
    loss.backward()  ## 调用了这句以后就会自动计算梯度，梯度计算出来就存在param.grad.data里面。
    return loss.data[0]

def optimize(learning_rate): 
    w.data -= learning_rate * w.grad.data 
    b.data -= learning_rate * b.grad.data
</code></pre>
<ul>
<li>Dataset是一个数据结构,只需要继承并实例化三个方法</li>
</ul>
<pre><code>from torch.utils.data import Dataset 
class DogsAndCatsDataset(Dataset): 
    def __init__(self,):
        pass 
    def __len__(self):
        pass 
    def __getitem__(self,idx): 
        pass
</code></pre>
<ul>
<li>DataLoader其实是一个generator，但是可以进行<strong>多线程读取</strong>，可以指定batchsize。</li>
</ul>
<pre><code>dataloader = DataLoader(dogsdset,batch_size=32,num_workers=2) 
for imgs , labels in dataloader: 
	#Apply your DL on the dataset. 
	pass
</code></pre>
<h2 id="layer">layer</h2>
<p>higher-level functionalities are called <strong>layers</strong> across the deep learning frameworks。</p>
<p><img src="https://jzq6520.github.io/post-images/1573460839202.png" alt=""><br>
Summarizing the previous diagram, any deep learning training involves getting data, building an architecture that in general is getting a bunch of layers together, evaluating the accuracy of the model using a loss function, and then optimizing the algorithm by optimizing the weights of our network.</p>
<ul>
<li>全连接层<pre><code class="language-python">myLayer = Linear(in_features=10,out_features=5,bias=True)
</code></pre>
<ul>
<li>使用：<code>myLayer(input)</code></li>
<li>访问参数：
<ul>
<li>weight: <code>myLayer.weight</code></li>
<li>bias: <code>myLayer.bias</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="loss">loss</h2>
<p>在一些比较复杂的情况下，如果先用的pytorch中提供的loss不是好用，最好还是自己实现loss。<br>
<img src="https://jzq6520.github.io/post-images/1573550147656.png" alt=""></p>
<h2 id="优化器">优化器</h2>
<p>For the sake of simplicity, let's see these optimizers as black boxes that take loss functions and all the learnable parameters and move them slightly to improve our performances.</p>
<pre><code class="language-python">
for input, target in dataset: 
	optimizer.zero_grad() ## 将每个变量的梯度初始化为0
	output = model(input) 
	loss = loss_fn(output, target) 
	loss.backward() ## 计算梯度calculates the gradients
	optimizer.step() ## 对变量进行迭代优化makes the actual changes to our learnable parameter
</code></pre>
<h2 id="迁移学习">迁移学习</h2>
<pre><code class="language-python">model_ft = models.resnet18(pretrained=True) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, 2)

if is_cuda: 
    model_ft = model_ft.cuda()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cython(2)--编译和运行]]></title>
        <id>https://jzq6520.github.io/post/cython2-bian-yi-he-yun-xing</id>
        <link href="https://jzq6520.github.io/post/cython2-bian-yi-he-yun-xing">
        </link>
        <updated>2019-11-06T03:34:57.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>和c、c++一样，Cython代码运行前也是需要编译的；</li>
<li>Cython是python的<strong>超集</strong>，所以python编译器是不能直接import和运行Cython的；</li>
<li>这个编译步骤可以是<strong>显式</strong>的，也可以是<strong>隐式</strong>的，即可以<strong>自动运行</strong>，不需要用户参与(让Cython感觉很像Python)，也可以由终端用户在需要更多控制时临时运行，两种都有它的作用；</li>
<li><strong>自动编译</strong>Cython的一个很好的特性是，它使使用Cython感觉像是在使用纯Python工作；</li>
<li>几种操作（<strong>但是没必要知道所有的编译方法，所有按需学习</strong>）：
<ul>
<li>Cython代码可以从IPython解释器编译并交互式地运行。</li>
<li>可以在导入时自动编译。</li>
<li>它可以通过Python的distutils等构建工具单独编译。</li>
<li>它可以集成到标准构建系统中，如make、CMake或SCons。</li>
</ul>
</li>
</ul>
<h2 id="1-cython编译流程pipeline">1 Cython编译流程（pipeline）</h2>
<p>该管道的工作是将Cython代码转换为Python<strong>扩展模块</strong>（extension module），Python解释器可以导入并使用该模块。</p>
<p><strong>该管道包括两个阶段</strong>：</p>
<ol>
<li>第一阶段由cython编译器处理，它将cython源代码转换为优化的、与<strong>平台无关</strong>的C或c++。</li>
<li>第二阶段使用标准的C或c++编译器将生成的C或c++源代码<strong>编译成共享库</strong>（shared library）。生成的<strong>共享库</strong>是<strong>平台相关</strong>的。共享库在unix系统上是<code>.so</code>后缀。</li>
</ol>
<p>我们将这个<strong>编译后的模块称为扩展模块（extension module）</strong>，它可以<strong>像用纯Python编写一样导入和使用</strong>。</p>
<p>cython编译器是一个源代码到源代码的编译器，生成代码经过了高度优化。由Cython生成的C代码比典型的手工编写的C代码要快。<strong>所以我们可以看到生成的c文件里面的代码我们都是看不太懂的，而且很长很复杂</strong>。Cython生成的C代码也具有高度可移植性，可以同时支持所有常见的C编译器和许多Python版本。</p>
<p>编译的时候确保两个工具安装了：</p>
<ol>
<li>c或c++编译器； gcc</li>
<li>cython编译器。anaconda自带或 <code>pip install cython -i https://pypi.tuna.tsinghua.edu.cn/simple/</code>安装。</li>
</ol>
<h3 id="安装cython">安装cython</h3>
<ul>
<li>anaconda自带；</li>
<li><code>pip install cython -i https://pypi.tuna.tsinghua.edu.cn/simple/</code>安装。</li>
</ul>
<h2 id="2-编译方式1-using-distutils-with-cythonize标准方式">2 编译方式1 Using distutils with cythonize（标准方式）</h2>
<p>distutils包是用于构建、打包和分发Python项目的，python再带。</p>
<h3 id="21-一个pyx文件内">2.1 一个pyx文件内</h3>
<p>步骤：</p>
<ol>
<li><code>cythonize</code>将Cython编译成c/c++语言；</li>
<li>然后，<code>distutils</code>（distributing utils）将c/c++语言编译成可以执行代码os,即扩展模块。</li>
</ol>
<p>代码：</p>
<pre><code class="language-python"># setup.py
# cythonize在.pyx源文件上调用cython编译器，setup将生成的C或c++代码编译成Python扩展模块。
from distutils.core import setup, Extension
from Cython.Build import cythonize

# setup(ext_modules=cythonize('fib.pyx')) #这样写的话其实没有显示知道编译以后扩展包的名字
setup(ext_modules = cythonize(Extension(name='warp_fib', sources=[&quot;fib.pyx&quot;])) #指定扩展包名
</code></pre>
<p>然后命令行执行编译即可：</p>
<pre><code class="language-c">// build_ext参数是一个命令，指示distutils构建扩展对象或cythonize调用创建的对象。可选的——inplace标志指示distutils将每个扩展模块放在其各自的Cython.pyx源文件旁边。
$ python setup.py build_ext --inplace
</code></pre>
<h4 id="编译生成的文件">编译生成的文件</h4>
<p><img src="https://jzq6520.github.io/post-images/1573020753272.png" alt=""><br>
这里面 <code>.c .o .so</code>这几个文件都是编译生成的文件。</p>
<h3 id="22-包含c语言文件和pyx文件">2.2 包含c语言文件和pyx文件</h3>
<pre><code class="language-python">from distutils.core import setup, Extension from Cython.Build import cythonize

# First create an Extension object with the appropriate name and sources. 
ext = Extension(name=&quot;wrap_fib&quot;, sources=[&quot;cfib.c&quot;, &quot;wrap_fib.pyx&quot;])

# Use cythonize on the extension object. 
setup(ext_modules=cythonize(ext))
</code></pre>
<h3 id="23-包含预先编译好的动态库">2.3 包含预先编译好的动态库</h3>
<pre><code class="language-python">from distutils.core import setup, Extension 
from Cython.Build import cythonize

ext = Extension(name=&quot;wrap_fib&quot;, sources=[&quot;wrap_fib.pyx&quot;], library_dirs=[&quot;/path/to/libfib.so&quot;], libraries=[&quot;fib&quot;])

setup(ext_modules=cythonize(ext))
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cython(1)--基本概念]]></title>
        <id>https://jzq6520.github.io/post/cython1</id>
        <link href="https://jzq6520.github.io/post/cython1">
        </link>
        <updated>2019-11-06T02:19:26.000Z</updated>
        <content type="html"><![CDATA[<p>以下全来自于对《Cython：A Guide for Python Programmers》一书的学习。<br>
cython最大的作用就是用来加速python代码。</p>
<p>cython是什么：</p>
<ul>
<li>Cython是融合python和c/c++的一种编程语言；</li>
<li>cython编译器可以将Cython代码编程成c/c++。然后编译后可以被python当成扩展包进行调用。</li>
</ul>
<p>Cython的美妙之处在于:它将Python的表达性和动态性与C的裸机性能（bare-metal performance）结合在一起，<strong>同时仍然感觉像Python</strong>。</p>
<h2 id="1-特点">1. 特点</h2>
<ul>
<li>Cython代码里面可以写python代码，也就是说Cython可以理解python代码。</li>
<li>需要用cdef声明变量</li>
</ul>
<h2 id="2-速度">2. 速度</h2>
<p>总结：</p>
<ol>
<li>能用：循环密集、算术密集</li>
<li>不能用：I/O密集</li>
</ol>
<p>然后看分析：</p>
<p>速度快，可以看一下对比数据。<br>
<img src="https://jzq6520.github.io/post-images/1573008098399.png" alt=""></p>
<ul>
<li>函数调用：<code>fib(0)</code>可以是看做函数调用的时间对比，<code>fib(0)</code>运行时主要消耗在<strong>调用相应语言中的函数</strong>所需的时间上;<strong>运行函数体的时间相对较短</strong>。从表1-1中可以看到，Cython生成的代码比调用Python函数快一个数量级，比手工编写的快两倍多.</li>
<li>循环：python中的循环相对c语言来说是非常慢的。加速循环Python代码的一个可靠的方法是找到方法将Python for和while循环移动到<strong>已编译的代码中</strong>，可以通过<strong>调用内置函数</strong>，也可以使用类似<strong>Cython</strong>的东西来完成转换。</li>
<li>数学操作：python在做算术运算的时候是不知道数据类型的，所以还需要去查找，但是c和cyhton是显示声明数据类型的。</li>
<li>堆栈与堆分配：在C级，动态Python对象完全是堆分配的。Python煞费苦心地智能地管理内存，使用内存池并内化常用的整数和字符串。但事实仍然是，创建和销毁对象——任何对象，甚至标量——都会增加处理动态分配内存和Python内存子系统的开销。因为Python浮动对象是不可变的，所以使用Python浮动的操作涉及到创建和销毁堆分配的对象。Cython版本的fib声明所有变量都是堆栈分配的C double。通常，堆栈分配比堆分配快得多。此外，C浮点数是可变的，这意味着for循环体在分配和内存使用方面更有效。</li>
</ul>
<p>但是，值得注意的是，<strong>并不是所有Python代码在使用Cython编译时都能看到巨大的性能改进</strong>。前面的fib示例是有意对CPU进行限制的，这意味着所有的运行时都是在CPU寄存器内操作几个变量，几乎不需要移动数据。相反,如果这个函数内存约束(例如,添加两个大数组的元素),I / O绑定(例如,从磁盘读取大型文件),或网络绑定(例如,从一个FTP服务器下载文件),Python之间的性能差异,C, Cython可能显著降低(内存受限操作)或完全消失(<strong>I/O密集型</strong>或network密集型操作)。</p>
<p>当提高Python的性能是我们的目标时，帕累托原则就对我们有利:我们可以预期，<strong>一个程序大约80%的运行时间是由20%的代码造成的</strong>。这个原则的一个推论是，如果不进行分析，很难找到那20%。但是没有理由不分析Python代码，因为它的内置分析工具非常简单。<strong>在我们使用Cython改善性能之前，获取分析数据是第一步</strong>。</p>
<ul>
<li>也就是说，<strong>如果我们通过分析确定程序中的瓶颈是由于I/O或网络限制造成的，那么我们就不能指望Cython在性能上有显著的改进</strong>。</li>
</ul>
<h2 id="用cython包装wrappingc语言">用cython包装（wrapping）c语言</h2>
<p>python调用c语言的方法。<br>
下面给出的只是其中一种方式，是吧c语言分来开存放，还有一种方式，是写在一个文件里。</p>
<p>c代码：</p>
<pre><code class="language-c">// cfib.c
double cfib(int n) { 
    int i; 
    double a=0.0, b=1.0, tmp; 
    for (i=0; i&lt;n; ++i) {
            tmp = a; a = a + b; b = tmp; 
        } 
    return a; 
}
</code></pre>
<pre><code class="language-c">// cfib.h
double cfib(int n);
</code></pre>
<p>cython代码：</p>
<pre><code class="language-python"># wrap_fib.pyx
cdef extern from &quot;cfib.h&quot;: 
    double cfib(int n)

def fib(n): 
&quot;&quot;&quot;Returns the nth Fibonacci number.&quot;&quot;&quot; 
    return cfib(n)
</code></pre>
<p>最后经过编译就可以在python里面调用了<code>from wrap_fib import fib</code>。cython代码里面需要用定义一个包装（wrapping）函数来返回c定义的函数。</p>
<p>并且，因为Cython语言理解Python，并且可以访问Python的标准库，所以我们可以利用Python的所有强大功能和灵活性。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-RetinaNet和Focal Loss]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-retinanet-he-focal-loss</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-retinanet-he-focal-loss">
        </link>
        <updated>2019-10-31T07:34:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-focal-loss">1 Focal Loss</h2>
<p>文章的题目是Focal Loss for Dense Object Detection，密集目标检测。主要还是解决那些和目标没有交集的anchor的分类，这些anchor是最简单的负样本，但是他们数量巨多。</p>
<ul>
<li><strong>样本不平衡：对于RPN来说大部分anchor都是负样本，对于yolo来说大部分格子都是负样本</strong>。</li>
<li>两阶段目标检测训练的最后的分类部分通常都是稀疏的，什么意思呢，<em>ﬁrst stage generates a sparse set of candidate object locations, and the second stage classiﬁes each candidate location as one of the foreground classes or as background using a convolutional neural net- work</em>.（第一阶级会产生一些建议区域,然后第二阶段对这些区域进行分类，这样对第二阶段来说需要分类的区域已经大大减少了，<strong>所以说是稀疏的</strong>)，见 Fast R-CNN。</li>
<li>而单阶段的目标检测训练通常是密集的，什么意思呢，就是需要对所有可能区域进行分类，见YOLO。举个例子：以输入为800X800,五层金字塔尺度分别为100X100,50X50,25X25,12X12,6X6,<strong>算一下anchor数量，119745</strong>， <strong>10几万anchor覆盖在整张图像中，密密麻麻</strong>，让你知道什么是密集检测，哈哈，插句话，<strong>10几万anchor中绝大多数都是负样本</strong>，无用的计算太多，这也是现在anchor free的一个出发点。【2】</li>
<li>单阶段检测（高密度检测）训练遇到的主要问题是<strong>样本不平衡</strong>，因为通常正样本总是小于负样本的，例如yolo是根据<strong>目标中心</strong>是不是落在格子里，落在格子里那么这个格子就是正样本，否则这个格子就是负样本，<strong>那么大部分格子肯定是没有目标的，也就是说正负样本是非常不平衡的</strong>。而Fast R-CNN也是，region proposal很多，但是里面<strong>大部分区域（anchor）应该是负样本，少部分（anchor）是正样本</strong>，所以Fast R-CNN训练分类的时候是通过(稀疏)采样。因为anchor是要分类的，所以大部分anchor是负样本。</li>
<li>在这篇文章之前一般都是双阶段的效果比较好（proposal-driven mechanism）。</li>
<li>所以有没有办法让单阶段的效果变好，毕竟单阶段的速度快。</li>
<li><strong>文章就抓住了训练时候正负样本既不平衡的这个主要矛盾</strong>来提高模型精度。最后达到了和FPN， mask R-CNN这样的精度。</li>
<li>最好的模型是ResNet-101- FPN(当然是文章提出的时候，现在backbone都有很多优秀的了）， 5fps。</li>
</ul>
<h3 id="11-之前目标检测中解决样本不平衡的方法">1.1 之前目标检测中解决样本不平衡的方法</h3>
<p>利用第一阶段<strong>过滤到大部分假阳（背景）</strong>，然后第二阶段再进行下面的工作：</p>
<ol>
<li>对需要分类的区域进行采样，见Fast R-CNN。</li>
<li>在线难例挖掘 online hard example mining (OHEM)</li>
</ol>
<p>相比之下，一个单级目标检测必须处理一组更大的候选对象位置，这些对象位置是通过图像定期采样的。实际上，这常常相当于枚举约100k个位置，这些位置密集地覆盖了空间位置、尺度和纵横比。虽然也可以采用类似的抽样启发法，但它们的效率很低，因为训练过程仍然由容易分类的背景示例所主导。这种低效率是对象检测中的一个经典问题，通常通过引导或硬示例挖掘等技术来解决。</p>
<p>这种不平衡导致了两个问题:(1)训练效率低，因为大多数区域都是<strong>简单的负样本</strong>，这种简单的负样本对训练作用不大;(2)整体而言，很容易被这些<strong>容易的但是很多</strong>的负样本影响，<strong>导致模型退化</strong>。</p>
<h3 id="12-focal-loss">1.2 Focal loss</h3>
<ul>
<li><strong>Huber损失（类似smooth L1）</strong>，该函数通过降低具有较大错误的示例(硬示例)的损失的权重来<strong>减少异常值的贡献</strong>，因为在误差大的时候损失的梯度是不变的，不变很重要 这样就不容易被误差值所影响，其实也就是差值一个是不进行平方，一个进行平方计算，平方计算就会增长很快，而在误差小的时候梯度慢慢减少。</li>
<li>相比之下，<strong>Focal loss</strong>的重点并不是处理异常值，而是通过减权<strong>简单的例子</strong>来解决阶级不平衡，这样即使简单的例子的数量很大，它们<strong>对总损失的贡献也很小</strong>。<strong>这点非常重要，是减少简单例子的影响，所以选择这个loss的适合一定要思考你的负样本是不是所谓的简单的负样本</strong>。</li>
</ul>
<p>下面来看一下cross entropy的损失函数曲线（其实就是-log(p)）：<br>
<img src="https://jzq6520.github.io/post-images/1572512564723.png" alt=""><br>
cross entropy可以看着只计算这个神经元负责的当前正样本的loss，假设有两个神经元分别预测目标和背景，那么对于两个神经元来说这两个都是正样本（因为第一个负责预测目标，第二负责预测背景呀，所以第一个神经元的正样本就是目标，第二个神经元的正样本就是背景，都是要预测概率为1的）因为是<strong>onehot</strong>。</p>
<p>这里有个假设，假设有个正样本，我们预测他为正样本的概率是<code>0.6~1</code>之间，我们就算这个样本是简单的，<strong>也就说能预测对的就算是简单的</strong>。</p>
<p><strong>所以我们观察上图可以发现，在我们定义的简单样本的区间内，其实普通的cross entropy的loss还是继续下降的（也就是说还是有一点的loss的），因为cross entropy总是希望概率可以很大很确信</strong>。所以可以看出focal loss在这个区间内的loss是很小的，而预测不对的话损失是很大的。<strong>这就是Focal loss核心思想</strong>。划重点。</p>
<p>因为我们希望在目标检测中那么多的<strong>负样本不要共享很多loss，预测的差不多得了，能大于0.6就好了，知道你是就行了，超过0.6，loss就少贡献一点</strong>，但是因为正负都是同等对待的，所以正样本预测的概率数值也会小一点。</p>
<h4 id="121-普通的加权方式">1.2.1 普通的加权方式</h4>
<figure data-type="image" tabindex="1"><img src="https://jzq6520.github.io/post-images/1572514035161.png" alt=""></figure>
<ul>
<li>虽然alpha参数可以平衡正负样本，但是还无法区分简单和困难的例子，我们希望减少简单例子的权重而关注（Focus）那些难的样本。</li>
<li>α ∈ [0, 1] for class 1（正样本） and 1−α for class −1（负样本）.</li>
<li>因为RetinaNet里面是直接预测k个类，而不是先预测是不是目标。所以这里的加权应该是直接在是目标的类别加权α，然后负样本加权1−α。</li>
<li>下图可以看出<strong>只加α</strong>和<strong>加了focal与α</strong>的α的值是不一样的，与我们直观理解一样只加α的时候正样本的权重应该要高一点，而加了focal以后反而正样本的权重要小一点好：<br>
<img src="https://jzq6520.github.io/post-images/1572601464394.png" alt=""></li>
</ul>
<h4 id="122-focal-loss">1.2.2 focal loss</h4>
<p>所以focal loss增加了一个调制的因子<code>(1 − pt )^γ</code>，当γ大于0时就是focal loss，等于0的适合是普通的交叉熵loss。<br>
<img src="https://jzq6520.github.io/post-images/1572577100834.png" alt=""></p>
<p>有两点：</p>
<ol>
<li>可以发现当pt趋向于0的时候，<code>(1 − pt )^γ</code>因子就接近1，loss的值是非常大的，当pt趋向于1的时候（或者说是子啊0.6到1的时候）<code>(1 − pt )^γ</code>这个因子就会解决与0，那么loss贡献就会非常的小，这个和γ参数也有关，γ越大那么预测概率解决于0的时候的loss越小。</li>
<li><em>we found γ = 2 to work best in our experiments</em>，作者实验下来 γ = 2的时候最好。</li>
</ol>
<p>实际使用中作者还加了alpha权重：<br>
<img src="https://jzq6520.github.io/post-images/1572577872330.png" alt=""></p>
<h2 id="2-retinanet">2 RetinaNet</h2>
<p><strong>单阶段（one-stage）的目标检测，使用了Focal Loss解决分类的正负样本不平衡问题</strong>。</p>
<p>为什么叫这个名字，因为是一个密集目标检测（像视网膜一样密集），这里的密集意思是一次性在所有区域里面检测模型，而不是像两阶段一样从建议区域里面检测。</p>
<p>retinanet只是使用了Focal Loss的一个网络，其他和这个loss无关，采样的都是以前的一些结构，然后很好的组合成一块。</p>
<p>总的结构是这样的：<br>
<img src="https://jzq6520.github.io/post-images/1572589354355.png" alt=""></p>
<p>分别说一下RetinaNet里面用的这4个东西的设置：</p>
<ol>
<li>FPN</li>
<li>Anchor</li>
<li>打label</li>
<li>分类分支</li>
<li>回归分支</li>
</ol>
<h3 id="21-fpn">2.1 FPN</h3>
<ul>
<li>右边特征金字塔层（level）<strong>从p3到p7</strong>用来检测，FPN原文中只是用了p3到p5.<em>We construct a pyramid with levels P3 through P7 , where <code>l</code> indicates pyramid level (P <code>l</code> has resolution 2^<code>l</code>(2的l次方) lower than the input)</em>.</li>
<li>金字塔层channel为256。</li>
<li>P6是在p5上用stride为2的3x3卷积。P7也是这样，然后都有relu。, P 6 is obtained via a 3×3 stride-2 conv on C 5 , and P 7 is computed by applying ReLU followed by a 3×3 stride-2 conv on P 6 .</li>
</ul>
<h3 id="22-anchor">2.2 anchor</h3>
<ul>
<li>P3, P4, P5,P6,P7 的基础面积分别是32，64，128，256，512，然后实际面积是基础面积乘以三个倍数，得到三种面积，倍数分别是<code>2^0=1，2^(1/3)=1.26，2^(2/3)=1.58</code>；然后每种面积都有三种宽高比1：1， 1：2，2：1，所以每个金字塔层总共有9个anchor。</li>
</ul>
<h3 id="23-打label">2.3 打label</h3>
<p><strong>记住正负样本是对anchor打的</strong>。</p>
<p>这里有个特例就是<strong>iou=0的时候</strong>，iou等于0说明anchor的框是完全和目标没有交集的，这个时候就<strong>没有box相对坐标可以回归了</strong>。</p>
<p>这里打label的阈值和RPN中有点不同，这里是与目标框iou大于等于0.5的anchor算是正样本，小于0.4的anchor算是负样本，<code>[0.4，5）</code>iou之间不计算loss.</p>
<p><strong>这里来区分一下简单负样本，难负样本</strong>：</p>
<ul>
<li><strong>简单负样本</strong>：那些和目标框没有交集的anchor，也就是IOU为0的anchor。</li>
<li><strong>难负样本</strong>：和目标框有交集的，然后iou大于0小于0.4的，当然在这个范围内越接近0.4越难了，因为很接近目标啦，但还是负样本。</li>
</ul>
<h3 id="24-分类分支">2.4 分类分支</h3>
<ul>
<li>不同金字塔层的分类分支是参数共享的。</li>
<li>分类分支和回归分支没有共同的conv，这个和RPN不一样。</li>
<li>分支的conv很多，加上最后一个预测的有5个3x3conv，最后预测的时候也是3x3conv。</li>
<li>最后预测的适合是sigmoid激活，而不是softmax。这样好像对Focal loss的使用好，因为可以想象focalloss应用范围都是概率要0到1之间的，softmax是加起来才是1 那可能本身概率就会小一点。</li>
<li>每个conv层的channel都是256，最后输出channel是k个类别乘以A个anchor。</li>
</ul>
<h3 id="25-回归分支">2.5 回归分支</h3>
<ul>
<li>4乘A个channel。</li>
<li>回归离anchor最近的目标，<em>regressing the offset from each anchor box to a nearby ground-truth object</em>。<strong>一个目标可能分配给多个anchor，但是一个anchor只会分配一个目标</strong>。</li>
<li>其他设置和分类分支一样，channel，参数共享，conv什么的都一样</li>
</ul>
<p><em>那么iou为0的时候要回归坐标吗？负样本要回归坐标吗？</em>：</p>
<ol>
<li>regressing the offset from each anchor box to a nearby ground-truth object, <strong>if one exists</strong>.即如果存在的话要回归，<strong>具体要不要得看代码了</strong>。<strong>但是RPN里面是只计算正样本的回归损失，见RPN博客中的损失函数</strong>。</li>
</ol>
<h3 id="26-推理阶段-inference阶段-也就是预测阶段">2.6 推理阶段 Inference阶段 也就是预测阶段</h3>
<p>在推理阶段，由于预测出来的款也很多，作者对金字塔每层特征图都使用0.05（为什么这么低？不是0.5吗？因为后面还进行排序然后选前1000）的置信度阈值进行筛选，然后取置信度前1000的候选框（不足1000更好） ，接下来收集所有层的候选框，进行NMS,阈值0.5。</p>
<h3 id="25-训练阶段">2.5 训练阶段</h3>
<ul>
<li>γ = 2这是最好，是通过【0.5~5】测出来的。</li>
<li>再次强调<strong>使用所有anchor训练</strong>的，<em>We emphasize that when training RetinaNet, the focal loss is applied to all <strong>∼ 100k anchors</strong> in each sampled image</em>。以输入为800X800,五层金字塔尺度分别为100X100,50X50,25X25,12X12,6X6,<strong>算一下anchor数量，119745个</strong>。</li>
<li><strong>所有的loss相加哦，然后除以分配的ground-truth box的anchor（有交集）数量</strong>，因为<strong>简单负样本</strong>贡献loss少，几乎为0，<strong>难负样本还是贡献loss的</strong>，所以加入计算，因为负样本也是和目标框有交集的。<em>The total focal loss of an image is computed as the sum of the focal loss over all ∼ 100k anchors, normalized by the number of anchors assigned to a ground-truth box.</em></li>
<li>因为去除掉iou为0的anchor以后，还剩下一部分有和目标交集的难负样本呀，所以这部分的负样本的数量还是大于正样本的，所以文章中还对loss加了个权重，来平衡这部分的loss。最后我们注意,α,重量分配到罕见的类。<em>Finally we note that α, the weight assigned to the rare class, also has a stable range.</em>(for γ = 2, α = 0.25 works best)，这两个参数需要相互结合调整。</li>
<li>α ∈ [0, 1] for class 1 and 1−α for class −1.</li>
</ul>
<h4 id="251-参数初始化">2.5.1 参数初始化</h4>
<p>所有新conv层RetinaNet子网中除了最后一个conv外都初始化为bias=0。最后conv层用下面的公式算出来的值初始化，文章中设置为π = .01：<br>
<img src="https://jzq6520.github.io/post-images/1572600783017.png" alt=""><br>
作用：<strong>这样初始化可以防止大量的背景锚点在训练的第一次迭代中产生较大的、不稳定的损失值</strong>。</p>
<h3 id="网络训练细节">网络训练细节</h3>
<p>RetinaNet is trained with stochastic gradient descent (SGD). We use synchronized <strong>SGD</strong> over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise speciﬁed, all models are <strong>trained for 90k iterations</strong> with an <strong>initial learning rate of 0.01</strong>, which is then <strong>divided by 10 at 60k and again at 80k iterations</strong>. We use horizontal image ﬂipping as the only form of data augmentation unless otherwise noted. <strong>Weight decay of 0.0001 and momentum of 0.9 are used</strong>. The training loss is the sum the focal loss and the standard smooth L 1 loss used for box regression [10]. <strong>Training time</strong> ranges between <strong>10 and 35 hours</strong> for the models in Table 1e.</p>
<h2 id="结果">结果</h2>
<p>信息量确实很大，文章很好，看不动了，好累，直接贴实验结果吧。<br>
<img src="https://jzq6520.github.io/post-images/1572601681760.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572601690019.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572601695560.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572601732469.png" alt=""><br>
<img src="https://jzq6520.github.io/post-images/1572601737614.png" alt=""></p>
<h2 id="引用">引用</h2>
<ul>
<li>各种loss：http://baijiahao.baidu.com/s?id=1603857666277651546&amp;wfr=spider&amp;for=pc</li>
<li>fpn anchor：https://www.cnblogs.com/wzyuan/p/10847478.html</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CV-笔记-重读特征金字塔网络 (FPN)]]></title>
        <id>https://jzq6520.github.io/post/cv-bi-ji-chong-du-te-zheng-jin-zi-ta-wang-luo-fpn</id>
        <link href="https://jzq6520.github.io/post/cv-bi-ji-chong-du-te-zheng-jin-zi-ta-wang-luo-fpn">
        </link>
        <updated>2019-10-31T02:39:07.000Z</updated>
        <content type="html"><![CDATA[<p>对于网络的卷积特征的几个重要理解：</p>
<ul>
<li>但由于深度不同，导致了不同层较大的<strong>语义差异</strong>。高分辨率图的低层特征损害了其对目标识别的表征能力。</li>
<li>语义差异：语义差异就说对目标的类别的识别的能力的差异，低层特征主要是一些边缘，形状等交基础的特征，对整个物体的类别识别帮助不大，而高层特征可能是更加具体的特征，已经很能反映整个是什么物体了，所以底层特征语义较差。</li>
<li>利用卷积神经网络特征层次结构的金字塔形状，同时构建一个在所有尺度上都具有<strong>强大语义</strong>的特征金字塔，如果不看skip连接的话FPN相当于是在网络后面接了另外的很多层卷积，所以属于高层特征。</li>
<li>所以有了另外一路<strong>上采样的卷积层</strong>以后，就具有了在<strong>每个尺度上都有高语义的特征图</strong>。即依赖于一个通过<strong>自顶向下（即上采样路径）路径和横向连接</strong>将<strong>低分辨率语义强的特征</strong>与<strong>高分辨率语义弱的特征</strong>相结合的架构。所以这也是最后特征图的channel都是一样的原因吧，这里更加注重语义特征。</li>
<li>FPN文章中是用FPN和Faster R-CNN结合。</li>
</ul>
<h2 id="结构实现细节">结构实现细节</h2>
<ul>
<li>上采样使用最近邻插值</li>
<li>只使用c2,c3,c4,c5尺度的特征图。</li>
<li>中间的skip连接是用1x1卷积把channel数降到256.</li>
<li>fpn是完全对称的<br>
<img src="https://jzq6520.github.io/post-images/1572492067196.png" alt=""></li>
<li><strong>使用同样的channel的原因</strong>：因为金字塔的所有层次都使用<strong>共享的分类器/回归器</strong>，就像传统的特征图金字塔一样，我们修正了所有特征图的特征维数。Because all levels of the pyramid use shared classiﬁers/regressors as in a traditional featurized image pyramid, we ﬁx the feature dimension (numbers of channels, denoted as d) in all the feature maps. We set d = 256 in this pa- per and thus all <strong>extra convolutional</strong> layers have 256-channel outputs.There are no non-linearities in these <strong>extra layers</strong>, which we have empirically found to have minor impacts.<strong>在这些额外的层中不存在非线性，我们根据经验发现它们的影响很小</strong>。</li>
</ul>
<h2 id="基于特征金字塔fpn的rpn">基于特征金字塔(FPN)的RPN</h2>
<p>虽然fpn和rpn很相似，但是这两个p意思不一样，一个是pyramid（金字塔），一个是proposal（建议）。</p>
<ul>
<li><strong>RPN参数共享</strong>：<strong>head</strong>：文章中把RPN的那个操作的模块叫做<strong>头部</strong>，即一个3x3的卷积，然后加两个1x1的卷积进行分类和回归。</li>
<li>FPN中的RPN head（头部）是<strong>参数共享</strong>的。</li>
<li><strong>每个特征图负责检查一种尺度</strong>：<strong>assign anchors of a single scale to each level</strong>.并且每层只有一个尺度的anchor，即一个层上没有多尺度，每层只负责一种大小的目标检测，这里说的大小是指物体的面积，用面积衡量大小，但是物体的宽高比是不一样的。因为RPN已经是在不同尺度的特征图上做了，所以不需要再在一个特征图上做不同尺度的anchor了。</li>
<li>anchor设置：对于在faster R-CNN上用的anchor，<strong>在高分辨率特征图上检测小目标，在低分辨率上检测大目标</strong>。面积设置，特征图<strong>从大到小</strong>p2,p3,p4,p5分别的anchor面积是32^2 , 64^2 , 128^2 , 256^2，且每个面积有三个宽高比：{1:2, 1:1, 2:1} 。</li>
<li>实验对比共享rpn头和不共享，共享的效果比较好。</li>
</ul>
<h2 id="打label">打label</h2>
<ul>
<li>正样本：iou大于0.7的所有anchor或和ground-truth iou最早的anchor（因为可能存在所以anchor都和ground-truth iou小于0.7，那么这个是后就取iou最大的anchor了），<em>positive label if it has the highest IoU for a given ground- truth box or an IoU over 0.7 with any ground-truth box</em>。</li>
<li>负样本：所有iou小于0.3的anchor，a negative label if it has IoU lower than 0.3 for all ground-truth boxes。</li>
<li>ground-truth没明确分配到哪个尺度的特征图，只要anchor分配好就可以了。原文：<strong>Note that scales of ground-truth boxes are not explicitly used to assign them to the levels of the pyramid; instead, ground-truth boxes are associated with anchors, which have been assigned to pyramid levels. As such, we introduce no extra rules in addition to those</strong>。因为iou是两个物体面积差不多大才是大的，一个大物体和一个小物体全部覆盖，那iou也是低的。所以物体会根据iou大小自动分配的。</li>
<li></li>
</ul>
<h2 id="fpn的roi-pooling">FPN的ROI pooling</h2>
<p>那么FPN的roi pooling怎么做呢，因为有这么多个的特征图。</p>
<p>首先我们要搞清楚一个概念，<strong>RPN的作用是确定出原图上的目标ROI区域</strong>（而非特征图上的区域），这时候我们再将原图上的ROI坐标映射到特征图上，然后把特征图上的roi区域拿过来进行分类和区域的回归矫正。</p>
<p>理解了这一点，那就清楚了，RPN确定的只是在原图上的roi区域，所以RPN做完以后，anchor就没用了，这时候就根据roi来确定我要在哪一层选择这个区域的特征来进行分类和回归。</p>
<p>虽然我们直观上理解是哪个特征图检测出了这个目标，就由哪个特征图所roi pooling，<strong>其实不是的</strong>，文章中是对roi进行重新分配了，<strong>所以RPN做完以后预测出的候选区域就和特征图没有半毛钱关系了，后面就等着再分配了</strong>。</p>
<p>那么分配规则是怎么样的呢，文章中是按照下面这个公式来确定分配给哪一层，那个类似于中括号的是<strong>取整</strong>，应该是向上取整，如果是5到4.1就是再第5层特征层做。。<br>
<img src="https://jzq6520.github.io/post-images/1572504129264.png" alt=""></p>
<p>首先，先通俗理解一下，前面最开始分配anchor的时候就知道了，深层特征图检测大目标，浅层的检测小目标，所以这里也是一样，<strong>大目标（即大ROI）分配给低分辨率（小特征图）的特征图即P5，小目标（小ROI）分配给到的高分辨率（大特征图）的特征图</strong>。</p>
<p>如果我们最小的特征图是P5，那么k0初始化为5，然后这里的224应该不要这样写好，应该写成输入图像的大小，如果输入是448那这里就是448了，意思就是如果roi是图像的一半，那么应该分配给P4特征图做roi pooling。以此类推。</p>
<p>其实这个公式算出来就是，缩小一半（2倍）就是到倒数第二层，缩小4倍就是到再下一层，缩小8倍就是再下一层。那么1到2倍就是再最后一层了。因为以2为底，log二分1就是-1，log四分之一就是-2。</p>
<p>然后做roi pooling都是7x7，最后堆叠到一个batchsize里面（这个和Fast R-CNN是一样的，见之前的博客）。然后预测分类和回归的适合连两个全连接。<em>1024-d fully-connected (fc) layers (each followed by ReLU) before the ﬁnal classiﬁcation and bounding box regression layers</em>.</p>
<h2 id="参数细节">参数细节</h2>
<h3 id="region-proposal-with-rpn">Region Proposal with RPN</h3>
<p>All architectures in Table 1 are trained end-to-end. The input image is resized such that its shorter side has 800 pixels. We adopt synchronized SGD training on 8 GPUs. A mini-batch involves 2 images per GPU and 256 anchors per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the ﬁrst 30k mini-batches and 0.002 for the next 10k. For all RPN experiments (including baselines), we include the anchor boxes that are outside the image for training, which is unlike [29] where these anchor boxes are ignored. Other implementation details are as in [29]. Training RPN with FPN on 8 GPUs takes about 8 hours on COCO.</p>
<h3 id="object-detection-with-fastfaster-r-cnn">Object Detection with Fast/Faster R-CNN</h3>
<p>The input image is resized such that its shorter side has 800 pixels. Synchronized SGD is used to train the model on 8 GPUs. Each mini-batch in- volves 2 image per GPU and 512 RoIs per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the ﬁrst 60k mini-batches and 0.002 for the next 20k. We use 2000 RoIs per image for training and 1000 for testing. Training Fast R-CNN with FPN takes about 10 hours on the COCO dataset.</p>
]]></content>
    </entry>
</feed>